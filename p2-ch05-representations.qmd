# Sequence Representation and Tokenization {#sec-representations}

## From Sequence to Model: The Representation Problem

Every genomic deep learning model must answer a fundamental question before learning can begin: how should DNA sequence be represented as numerical input? This question might seem purely technical, a preprocessing detail to be settled and forgotten. Yet the choice of representation profoundly shapes what a model can learn, how efficiently it trains, and what biological phenomena it can capture. A patient's genome contains roughly three billion nucleotides, but whether a clinical variant interpretation model sees that genome as three billion individual tokens, five hundred million overlapping hexamers, or some adaptive compression depends entirely on representation decisions made before a single parameter is learned.

The clinical stakes are immediate. A splice-disrupting variant in *BRCA1* might span just two nucleotides at an exon-intron boundary. If the tokenization scheme merges those nucleotides with surrounding sequence into a single coarse token, the model loses the resolution needed to distinguish pathogenic from benign changes. Conversely, a regulatory variant's effect might depend on enhancer sequences fifty kilobases away. If the representation limits context to a few hundred nucleotides, that distal information never reaches the model. Representation choices thus constrain not just computational efficiency but the biological questions a model can answer.

The challenge can be understood through an analogy to natural language processing. When training a language model on English text, researchers must decide how to segment the continuous stream of characters into discrete tokens. One could treat each character as a token, preserving maximum resolution but creating very long sequences. Alternatively, one could use words as tokens, compressing the sequence but potentially losing information about word structure. Or one could learn a vocabulary of subword units that balances these concerns. Each choice affects what patterns the model can discover and how efficiently it can process long documents.

DNA presents similar choices but with important differences. The genome has only four letters rather than dozens, no natural word boundaries, and biological structure that operates at multiple scales simultaneously. A transcription factor binding site might span 6 to 12 nucleotides, but the regulatory grammar linking multiple binding sites can extend over hundreds of base pairs. Coding sequences follow a strict three-nucleotide codon structure, while noncoding regions have no such constraint. Any representation scheme must navigate these biological realities while remaining computationally tractable.

This chapter examines the evolution of sequence representation strategies in genomic deep learning. We begin with one-hot encoding, the foundation of CNN-based models, then trace the progression through k-mer tokenization and byte pair encoding to modern approaches including single-nucleotide tokens and biologically-informed schemes. We then address two aspects of representation that prove critical for transformer architectures: the transformation from discrete tokens to dense learned embeddings, and the encoding of positional information that genomic applications demand. Understanding these choices clarifies design decisions in models throughout Parts III and IV, and illuminates why seemingly minor representation choices can dramatically affect model capabilities.

## One-Hot Encoding: The CNN Foundation

One-hot encoding represents the simplest possible approach to sequence representation: each nucleotide becomes a sparse binary vector with a single active element indicating its identity. Adenine is encoded as [1, 0, 0, 0], cytosine as [0, 1, 0, 0], guanine as [0, 0, 1, 0], and thymine as [0, 0, 0, 1]. A sequence of length $L$ thus becomes a matrix of dimensions $4 \times L$, interpretable as four channels analogous to the RGB channels of an image plus one additional channel.

This representation dominated the CNN era of genomic deep learning for good reason. One-hot encoding is lossless, preserving every nucleotide explicitly without any information compression. It maintains single-nucleotide resolution, enabling detection of effects from individual SNPs, which is critical for variant interpretation. The representation exhibits translation equivariance, meaning that convolutional filters learn position-invariant motifs that can be recognized anywhere in the sequence. And it requires no preprocessing, vocabulary construction, or tokenizer training, making implementation straightforward.

DeepSEA, ExPecto, and SpliceAI all employed one-hot encoding without modification (@sec-cnn). The convolutional layers in these models learned to detect sequence patterns directly from the binary representation, with first-layer filters discovering motifs corresponding to transcription factor binding sites and deeper layers capturing combinations and spatial arrangements. The representation worked because CNNs process sequences through local operations, with each convolutional filter examining only a small window of positions at a time. The sparse, orthogonal nature of one-hot vectors posed no obstacle to this local processing.

Yet for transformer architectures, one-hot encoding presents significant challenges. Transformers compute attention between all pairs of positions in a sequence, with computational cost scaling as $O(L^2)$ where $L$ is the sequence length. A 10 kb sequence requires 10,000 tokens, meaning 100 million pairwise attention computations per layer. This quickly becomes prohibitive for the long sequences that genomic applications require. Furthermore, transformers typically learn dense embeddings for each token, but with only four possible nucleotides, there is little opportunity for the model to discover rich representations through the embedding layer. The sparse one-hot vectors provide minimal information for the embedding to transform. Most critically, practical transformer context windows of 512 to 4,096 tokens translate to only 512 to 4,096 base pairs when using one-hot encoding, a tiny fraction of genes or regulatory regions and far less than the context that proved valuable for models like Enformer and SpliceAI.

These limitations motivated the search for alternative representations that could compress genomic sequences into fewer tokens while preserving the information needed for biological prediction.

## K-mer Tokenization: The DNABERT Approach

The computational constraints of one-hot encoding for transformers motivated exploration of sequence compression through k-mer tokenization. This approach treats overlapping subsequences of length $k$ as tokens, drawing an analogy between k-mers and words in natural language. Just as sentences are composed of words that carry meaning through their sequence and combination, genomic sequences might be understood as composed of k-mer "words" that encode biological function through their arrangement. DNABERT (2021) pioneered this approach for genomic transformers, using 6-mers as tokens and training a BERT-style masked language model on human reference sequences [@ji_dnabert_2021].

The k-mer vocabulary has a fixed size of $4^k$ possible tokens. For 6-mers, this yields 4,096 distinct tokens, comparable to the vocabulary sizes used in some natural language models. Each token represents six consecutive nucleotides, creating a direct correspondence between subsequence and token identity. The tokenization proceeds by sliding a window across the sequence and recording each k-mer encountered.

DNABERT used overlapping k-mers, meaning that for a sequence like ACGTACGT, the 6-mer tokens would share five nucleotides with their neighbors. The sequence position advances by one nucleotide at a time, generating one token per position (minus the k-1 positions at the end where a complete k-mer cannot be formed). This overlapping design preserves positional information and ensures that every nucleotide contributes to multiple tokens, potentially providing redundancy that helps the model learn robust representations.

The DNABERT approach provided valuable proof of concept. It demonstrated that self-supervised pretraining on raw DNA sequences could improve performance over training from scratch, that learned embeddings could capture biologically meaningful regularities even when trained only on the reference genome, and that BERT-style architectures could be reused across multiple downstream tasks. DNABERT achieved strong performance on prediction of promoters, splice sites, and transcription factor binding sites after fine-tuning with relatively small amounts of task-specific labeled data.

Subsequent analysis, however, revealed fundamental limitations of k-mer tokenization that stemmed from the overlapping design. DNABERT-2 (2024) articulated these problems clearly [@zhou_dnabert-2_2024]. Overlapping k-mers provide no sequence compression: the number of tokens equals the number of nucleotides (minus a small constant), so context window limitations persist unchanged. A 10 kb sequence still requires approximately 10,000 tokens, and the quadratic attention complexity remains prohibitive for long sequences.

Overlapping tokenization also creates ambiguity in how sequence positions map to tokens. A single nucleotide contributes to $k$ different tokens, complicating interpretation of which token is responsible for any given prediction. This ambiguity becomes particularly problematic for variant effect interpretation, where one wants to understand how changing a specific nucleotide alters model predictions. The effect of a single nucleotide substitution propagates through $k$ different tokens in ways that can be difficult to disentangle.

The overlapping design introduces sample inefficiency as well. The model must learn that overlapping tokens share nucleotides, a relationship that is obvious from the tokenization scheme but must be discovered through training. This redundancy consumes model capacity that could otherwise be devoted to learning more complex biological patterns. Finally, the fixed $4^k$ vocabulary does not adapt to corpus statistics. Frequent and rare k-mers receive equal representation capacity in the embedding table, even though their importance for prediction may differ substantially.

These limitations motivated exploration of alternative tokenization strategies that could achieve genuine sequence compression while preserving the information needed for biological prediction.

## Byte Pair Encoding: Learning the Vocabulary

Byte Pair Encoding offers a fundamentally different approach to tokenization. Rather than defining tokens through a fixed rule (every $k$ consecutive nucleotides), BPE constructs a vocabulary by learning which subsequences appear frequently in the training corpus. The algorithm, originally developed for data compression, iteratively merges the most frequent adjacent token pairs until reaching a desired vocabulary size.

The BPE algorithm begins by initializing the vocabulary with single nucleotides: {A, C, G, T}. It then scans the training corpus to count all adjacent token pairs and identifies the most frequent pair. This pair is merged into a new token, added to the vocabulary, and all instances in the corpus are replaced with the new token. The process repeats, counting pairs again (now including the newly created token) and merging the next most frequent pair. Through many iterations, BPE builds a vocabulary of variable-length tokens that capture frequently occurring sequence patterns.

The critical insight is that BPE produces genuine sequence compression. Unlike overlapping k-mers where each nucleotide generates its own token, BPE creates non-overlapping tokens that can span multiple nucleotides. A 10 kb sequence might compress to 2,000 or 3,000 tokens depending on its repetitive structure, enabling transformers to process much longer sequences within the same context window.

DNABERT-2 replaced 6-mer tokenization with BPE and demonstrated dramatic improvements [@zhou_dnabert-2_2024]. The new model achieved comparable performance to state-of-the-art approaches while using 21 times fewer parameters and requiring approximately 92 times less GPU time in pretraining. The efficiency gains stem directly from non-overlapping tokenization: actual sequence compression enables processing longer sequences with the same computational budget, and eliminating the redundancy of overlapping tokens allows the model to focus capacity on learning biological patterns rather than token relationships.

The BPE vocabulary learns corpus statistics through its construction process. Repetitive elements that appear frequently throughout the genome, such as Alu sequences or common regulatory motifs, receive dedicated tokens that span many nucleotides. These long tokens enable efficient representation of repetitive regions while preserving single-nucleotide resolution for unique sequences. Rare sequences that BPE never encountered during vocabulary construction are represented as concatenations of shorter subunits, maintaining the ability to encode any sequence while allocating more representation capacity to common patterns.

GROVER (Genome Rules Obtained Via Extracted Representations) extended this approach by training BPE specifically on the human genome and selecting vocabulary using a custom next-k-mer prediction task [@sanabria_grover_2024]. Analysis of the resulting token embeddings revealed that the learned vocabulary encodes biologically meaningful structure. Common tokens cluster separately from rare ones in embedding space. GC-rich tokens segregate from AT-rich tokens, reflecting the different properties of these sequence compositions. Token length correlates with specific embedding dimensions, allowing the model to represent both the content and extent of each token. Some tokens appear primarily in repetitive regions while others distribute broadly across the genome, and this localization pattern is captured in the learned representations.

BPE introduces its own complications. The variable-length tokens mean that variant positions fall at different locations relative to token boundaries depending on the local sequence context. A SNP might fall in the middle of a long token in one context but at a token boundary in another, potentially affecting how the model represents and processes the variant. This context-dependence can complicate variant effect interpretation, as the same nucleotide change may alter different numbers of tokens depending on surrounding sequence.

## Single-Nucleotide Tokenization: Maximum Resolution

While k-mer and BPE tokenization compress sequences to enable longer context windows, they sacrifice single-nucleotide resolution in doing so. This trade-off becomes problematic for variant effect prediction, where the precise position and identity of mutations is paramount. A single nucleotide polymorphism can completely alter protein function through mechanisms ranging from amino acid substitution to splice site disruption to regulatory element ablation. Multi-nucleotide tokens obscure exactly where variants fall and how they relate to the boundaries of biological features.

HyenaDNA (2023) took the opposite approach, using single-nucleotide tokens with no compression whatsoever [@nguyen_hyenadna_2023]. Each nucleotide (A, C, G, T) is a separate token, maintaining the maximum possible resolution. Every nucleotide is independently represented in the token sequence, SNP effects can be isolated to specific token positions without ambiguity, and there are no tokenization artifacts that depend on surrounding sequence context.

The challenge with single-nucleotide tokens is sequence length. A 1 Mb region requires 1 million tokens, far beyond the capacity of any standard transformer. The quadratic attention complexity would require a trillion pairwise computations per layer, rendering the approach computationally infeasible with conventional architectures.

HyenaDNA addressed this challenge through a fundamental architectural innovation rather than a tokenization compromise. The Hyena architecture replaces the attention mechanism with implicit convolutions that scale sub-quadratically with sequence length. Where attention computes explicit pairwise interactions between all positions, Hyena uses long convolutions parameterized by a small neural network, achieving similar representational power with $O(L \log L)$ complexity rather than $O(L^2)$. This enables processing of sequences hundreds of times longer than attention-based transformers within the same computational budget.

The result was a 500-fold increase in context length over dense attention models while maintaining single-nucleotide resolution. HyenaDNA could process 1 Mb sequences where DNABERT was limited to approximately 500 bp and the Nucleotide Transformer to approximately 6 kb. On the Nucleotide Transformer benchmarks, HyenaDNA reached state-of-the-art performance on 12 of 18 datasets with orders of magnitude fewer parameters and less pretraining data. On GenomicBenchmarks, it surpassed prior state-of-the-art on 7 of 8 datasets by an average of 10 accuracy points.

Perhaps most notably, HyenaDNA demonstrated the first use of in-context learning in genomics. The model could perform tasks based on examples provided in the context window without any fine-tuning, simply by conditioning on demonstration sequences. This capability, familiar from large language models, had not previously been shown for genomic sequences and suggests that very long context combined with high resolution enables qualitatively new forms of biological reasoning.

The development of sub-quadratic architectures like Hyena, Mamba, and state space models has thus fundamentally changed the tokenization calculus. When computational constraints no longer force a choice between resolution and context length, single-nucleotide tokenization becomes increasingly attractive, particularly for applications requiring precise variant interpretation.

## Biologically-Informed Tokenization

Standard tokenization schemes treat DNA as a homogeneous string of characters, ignoring the biological reality that different genomic regions serve fundamentally different functions and follow different structural rules. Coding sequences obey a strict codon structure where every three nucleotides encode an amino acid, while noncoding regions have no such constraint. Treating these regions identically wastes an opportunity to build biological knowledge directly into the representation.

For protein-coding regions, the natural unit of sequence is the codon, not the individual nucleotide. GenSLMs (2022) pioneered codon-level tokenization for genomic foundation models, treating each three-nucleotide codon as a single token and exploiting the fact that codons are the biologically meaningful units of protein-coding sequence [@zvyagin_genslms_2022]. The 64-codon vocabulary captures the complete space of possible genetic code words, with each token corresponding to either an amino acid or a stop signal. This alignment with translation semantics means that mutations affecting amino acid identity (nonsynonymous changes) alter the token sequence, while synonymous mutations within a codon alter the specific token used but maintain the broader codon-family structure.

Life-Code (2025) extended codon-aware tokenization to broader genomic contexts, encoding coding and noncoding regions in a way that respects reading frame and local biological function [@liu_life-code_2025]. Coding regions are tokenized by codons, aligning token boundaries with the fundamental unit of protein translation. Noncoding regions, lacking codon structure, are tokenized by learned patterns that capture regulatory motifs and other functional elements. This biologically-informed design enables Life-Code to learn protein structure through knowledge distillation from protein language models, capture interactions between coding and noncoding regions within a unified framework, and achieve state-of-the-art results across tasks involving DNA, RNA, and protein.

BioToken (2025) extends tokenization further to include explicit genomic structural annotations [@medvedev_biotoken_2025]. Rather than treating variants as implicit changes in the sequence string, BioToken creates tokens that explicitly represent SNPs, insertions, and deletions. Known regulatory elements receive dedicated tokens encoding their presence and type. Gene structure, chromatin state, and other functional annotations are integrated directly into the token representation. This approach treats tokens as rich entities that bundle nucleotides with positional, functional, or experimental context.

Such variant-aware representations are especially attractive for variant effect prediction and clinical interpretation, where the input is often "reference plus variant" rather than a generic sequence. By incorporating biological inductive biases directly into tokenization, BioToken's associated model achieves competitive or superior performance to specialized models like Enformer and SpliceAI with significantly fewer parameters. This efficiency suggests that appropriate representation can substitute for model scale, at least partially, by making the learning problem easier through informed structure.

The broader lesson is that tokenization can and perhaps should be informed by biological structure when that structure is known and relevant. BPE learns statistical patterns from the corpus, but those patterns need not correspond to biological units. Codon tokenization imposes biological semantics directly, at the cost of applicability to noncoding regions. Future approaches might combine these strategies, using codon-aware tokenization for coding regions and BPE or single-nucleotide tokens for noncoding sequence.

## From Tokens to Embeddings: Learning Representations

Once a tokenization scheme divides sequence into discrete units, these tokens must be transformed into numerical representations that neural networks can process. The embedding layer performs this transformation, mapping each token from a finite vocabulary to a dense vector in continuous space. This seemingly simple operation proves fundamental to what models can learn.

Consider the difference between one-hot encoding and learned embeddings. A one-hot representation treats each nucleotide as maximally distinct from every other: the dot product between any two different nucleotides is zero, providing no information about their relationships. Adenine and thymine are equally different from each other as adenine and guanine, despite the biological reality that purines (A, G) share structural properties distinct from pyrimidines (C, T), and that complementary base pairs (A-T, G-C) have special significance.

Learned embeddings allow the model to discover such relationships from data. The embedding layer maintains a matrix $E$ of dimensions $V \times d$, where $V$ is vocabulary size and $d$ is embedding dimension. Each token maps to a row of this matrix, and during training, backpropagation adjusts the embedding vectors to support downstream prediction. If distinguishing purines from pyrimidines helps the model predict regulatory function, the embedding space will organize to reflect this distinction. If complementary relationships matter, they will emerge in the geometry of the learned space.

The embedding dimension $d$ controls representational capacity. Small embeddings (32 to 64 dimensions) suffice for simple tokenization schemes like single nucleotides, where only four vectors must be distinguished. Larger vocabularies require larger embeddings: DNABERT-2's BPE tokens use 768-dimensional embeddings, comparable to natural language models. The choice involves a trade-off between expressiveness and efficiency, as larger embeddings increase both model capacity and computational cost.

For genomic sequences, the embedding layer can learn surprising structure. Analysis of trained DNA language models reveals that embedding spaces organize around biologically meaningful properties. GC content, often considered a nuisance variable in genomics, emerges as a major axis of variation in embedding space because it correlates with so many functional properties. Repetitive elements cluster together. Coding sequence embeddings differ systematically from noncoding embeddings, even when the tokenization scheme makes no explicit distinction.

This emergent organization has practical implications. The structure learned in the embedding layer propagates through all subsequent computations. If the embeddings fail to capture relevant distinctions, later layers must learn them from scratch. If embeddings encode spurious correlations, the model may exploit them inappropriately. Understanding what embeddings learn, and whether that learning aligns with biological reality, becomes an important diagnostic for model behavior.

The relationship between tokenization and embedding deserves emphasis. Coarse tokenization (large k-mers, aggressive BPE) creates more tokens, each with more room for rich embedding representations but requiring the model to learn more parameters. Fine tokenization (single nucleotides) creates fewer token types with simpler embeddings but forces the model to build complex representations through composition across layers. Neither approach is uniformly superior; the optimal choice depends on available training data, model scale, and task requirements.

## Position Encodings: Where Tokens Live

Transformers process tokens as sets rather than sequences. The attention mechanism computes interactions between all pairs of tokens regardless of their positions, treating a sequence as a bag of elements with no inherent order. For language, this creates a problem: "dog bites man" and "man bites dog" contain identical tokens but mean very different things. For genomics, the problem is more severe: a transcription factor binding site has entirely different effects depending on whether it appears in a promoter, an enhancer, or a gene body. Position must somehow be encoded.

The standard solution adds positional information to token embeddings before attention computation. The combined representation carries both content (what nucleotide or k-mer) and position (where in the sequence). Several strategies exist, each with distinct properties that matter for genomic applications.

**Absolute positional embeddings** assign a learnable vector to each position in the sequence. Position 1 receives embedding $p_1$, position 2 receives $p_2$, and so forth. These embeddings are added to the token embeddings, creating combined representations that carry both identity and location. BERT and early genomic transformers like DNABERT used this approach. The limitation is that the model can only handle sequences up to the maximum position seen during training. A model trained with 512-position embeddings cannot process position 513; there is no embedding for it. This fixed maximum context proves particularly restrictive for genomics, where biological phenomena span scales from individual binding sites to megabase regulatory domains.

**Sinusoidal positional encodings** address the fixed-length limitation by computing position embeddings from mathematical functions rather than learning them. The original Transformer paper used sinusoids of different frequencies:

$$PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d})$$
$$PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d})$$

Each position receives a unique pattern of sines and cosines, with lower-frequency components capturing coarse position and higher-frequency components capturing fine position. The mathematical structure means that any position, even one never seen during training, can receive a well-defined encoding. Sinusoidal encodings also have the property that relative positions are represented consistently: the relationship between positions 10 and 20 mirrors that between positions 110 and 120.

**Relative positional encodings** directly represent the distance between tokens rather than their absolute locations. When computing attention between positions $i$ and $j$, the model incorporates information about $(j - i)$, the relative offset. This approach recognizes that for many biological phenomena, relative positioning matters more than absolute coordinates. A transcription factor binding site 50 bases upstream of a transcription start site has similar effects whether the TSS is at genomic position 1,000 or 1,000,000. Relative encodings also generalize naturally to sequences longer than those seen during training, since the relative distances remain bounded even as absolute positions grow.

**Rotary position embeddings (RoPE)** encode position by rotating token embeddings in the complex plane. Rather than adding a position vector, RoPE multiplies embeddings by a rotation matrix whose angle depends on position. This approach preserves the relative distance information in the dot product used for attention: the attention score between two positions depends on their relative separation regardless of absolute location. RoPE has become popular in recent large language models and has been adopted by several genomic foundation models including variants of the Nucleotide Transformer.

**ALiBi (Attention with Linear Biases)** takes a different approach entirely, adding position-dependent biases directly to attention scores rather than modifying embeddings. The bias penalizes attention between distant positions, with the penalty increasing linearly with distance. ALiBi requires no learned position parameters and generalizes straightforwardly to longer sequences. The linear distance penalty may not perfectly capture biological relationships, where some regulatory interactions span consistent long distances while others operate locally, but the simplicity and extrapolation properties have proven valuable.

For genomic applications, the choice of position encoding has implications beyond mere sequence length. Biological coordinates matter: a variant at chr17:41,276,045 has specific meaning that should be preserved. Cross-strand relationships exist: the reverse complement of a sequence carries related but distinct information. Circular genomes like mitochondrial DNA and bacterial chromosomes have no beginning or end, creating wraparound relationships that linear position encodings cannot represent naturally.

Several recent models have explored genomic-specific position encoding strategies. Some incorporate absolute genomic coordinates, allowing models to learn position-specific patterns like centromeric sequences or telomeric regions. Others encode strand explicitly, representing Watson and Crick strands as distinct position modalities. Models for bacterial or viral genomes sometimes use circular position encodings that respect the topology of circular chromosomes. These adaptations illustrate that position encoding is not merely a technical detail but a design choice that shapes what biological patterns a model can capture.

## Special Considerations for Biological Sequences

Genomic sequences present several features that distinguish them from natural language text and require careful consideration in representation design.

**Strand awareness** poses a fundamental challenge. DNA is double-stranded, with each strand carrying complementary information. A sequence ACGT on the forward strand corresponds to ACGT (read 5' to 3') but also implies ACGT on the reverse strand read in the opposite direction (which is the reverse complement ACGT â†’ TGCA). Some biological features are strand-specific: a gene on the forward strand is transcribed from that strand only. Other features are strand-agnostic: many transcription factor binding sites function identically on either strand. Representation schemes must decide whether to treat strands as equivalent (by augmenting training data with reverse complements), as distinct (by encoding strand explicitly), or as related-but-different (through equivariant architectures that process both strands jointly).

The Nucleotide Transformer addressed strand by including both orientations during training, using data augmentation to ensure the model sees sequences from both directions. Caduceus (2024) introduced a more elegant solution: a bidirectional architecture that processes forward and reverse complement strands simultaneously through shared computation [@schiff_caduceus_2024]. The model outputs are equivariant to reverse complementation, meaning that reversing and complementing the input produces correspondingly transformed outputs. This inductive bias ensures consistent treatment of strand without requiring augmentation or doubling computational cost.

**Circular genomes** present another topological consideration. Bacterial chromosomes and plasmids, mitochondrial DNA, and many viral genomes are circular, with no natural start or end position. Linear position encodings impose arbitrary boundaries on these sequences. Some models address this through circular position encodings that wrap around at sequence boundaries, while others simply process circular genomes as linear sequences with the understanding that boundary effects may introduce artifacts.

**Genomic coordinates** carry information absent from raw sequence. The position chr17:41,276,045 refers to a specific location in the BRCA1 gene, and variants at this position have been extensively studied. Knowing the genomic coordinate enables lookup of prior knowledge: population frequencies from gnomAD, clinical interpretations from ClinVar, functional annotations from ENCODE. Some representation schemes incorporate coordinate information explicitly, enabling models to learn position-specific patterns and to integrate with external databases. Others deliberately exclude coordinates to force models to learn purely from sequence, trading prior knowledge for generalization to novel sequences.

**Multiple sequence inputs** arise frequently in genomic applications. Variant effect prediction requires comparing reference and alternate alleles. Comparative genomics involves aligned sequences from multiple species. Some regulatory predictions require input from multiple genomic regions (promoter plus enhancer, for instance). Representation schemes must accommodate these multi-sequence inputs, either through concatenation, paired encoding, or specialized architectures that process multiple sequences jointly.

These considerations illustrate that genomic sequence representation is not simply a matter of choosing tokens but of designing representations that respect the structure of biological data. The best choice depends on the specific application: single-nucleotide tokens for variant interpretation, strand-aware encoding for regulatory prediction, coordinate inclusion for clinical models that must integrate with knowledge bases.

## Trade-offs and Practical Guidance

The choice between tokenization strategies involves multiple competing considerations that depend on the intended application.

**Compression versus resolution** represents the fundamental tension. Higher compression enables longer context windows within fixed computational budgets but loses precision for identifying exactly where variants fall and how they relate to biological features. One-hot encoding and single-nucleotide tokenization provide no compression but maintain full resolution. Non-overlapping k-mers achieve approximately k-fold compression at the cost of k-nucleotide resolution. BPE provides variable compression depending on sequence repetitiveness, with corresponding variable resolution. For variant effect prediction, where single nucleotide changes can have dramatic phenotypic consequences, resolution is paramount and the computational costs of long single-nucleotide sequences are often justified.

**Vocabulary size** affects both model capacity and efficiency. Larger vocabularies require bigger embedding tables but may capture more complex patterns directly. Smaller vocabularies are parameter-efficient but require the model to learn compositional structure through multiple layers. The vocabulary size of one-hot encoding (4 tokens plus special tokens) minimizes embedding parameters but maximizes the compositional learning burden. K-mer vocabularies scale exponentially with $k$, reaching 4,096 for 6-mers. BPE vocabularies are tunable, typically ranging from 4,096 to 32,000 tokens for genomic applications.

**Computational efficiency** depends on both tokenization and architecture. For standard attention with $O(L^2)$ complexity, any compression directly reduces cost: non-overlapping k-mers reduce attention cost by a factor of $k^2$, and BPE with average compression $c$ reduces cost by $c^2$. Sub-quadratic architectures like Hyena change this calculus, making single-nucleotide tokenization computationally feasible at long contexts and eliminating the need to trade resolution for efficiency.

**Variant interpretation** has specific implications. Single-nucleotide tokens enable clean comparison of reference and alternate alleles at the same token position with no ambiguity about effect localization. K-mer tokens complicate matters because a single SNP changes $k$ overlapping tokens, requiring aggregation across affected tokens and introducing potential boundary effects. BPE tokens create context-dependent effects where the same variant may fall at different positions relative to token boundaries depending on surrounding sequence.

For practitioners, several heuristics have emerged. Use single-nucleotide tokens when variant-level reasoning or high-resolution interpretability is central. Use k-mers or BPE when context length is the primary bottleneck and tasks do not require base-level precision. Consider biologically-informed tokens when integrating multi-modal or annotation-rich data. Match position encoding to task requirements: relative encodings for tasks where absolute position is arbitrary, coordinate-aware encodings for clinical applications requiring integration with databases.

## Implications for Subsequent Chapters

The representation choices examined in this chapter set the stage for design decisions discussed throughout the rest of the book. In @sec-cnn, we see how convolutional filters operating on one-hot encoded sequence implicitly learn k-mer-like patterns, with first-layer filters resembling known transcription factor binding motifs. The CNN approach can be understood as learning the tokenization jointly with the prediction task rather than fixing it in advance.

In @sec-attention, transformer architectures bring position encoding to the foreground. The choice between absolute, relative, and rotary position encodings affects whether models can generalize to sequence lengths beyond training and whether they capture the relative spatial relationships central to regulatory grammar. The quadratic complexity of attention creates strong pressure for sequence compression, motivating the k-mer and BPE approaches discussed here.

In @sec-dna-lm, we revisit why many DNA transformers still use k-mers and how models like the Nucleotide Transformer balance context length with biological resolution. The pretraining objectives for DNA language models interact with tokenization choices: masked language modeling over k-mers versus bases produces different learned representations. In @sec-protein-lm, protein language models use amino acids as natural tokens, sidestepping many tokenization debates but raising new questions about how to represent insertions, deletions, and post-translational modifications.

In @sec-regulatory, long-range models like Enformer and Borzoi largely retained one-hot encoding or single-nucleotide input encodings for precision in variant effect prediction, leaning on architectural innovations (hybrid CNN-transformer designs) rather than token compression to scale context. The choice reflects the primacy of variant interpretation in regulatory genomics applications.

The representation problem remains an active area of research. As models grow larger and contexts extend further, new tokenization strategies may emerge that better balance compression, resolution, and biological structure. The field has moved from treating tokenization as a fixed preprocessing step to recognizing it as a fundamental design decision that shapes what models can learn and how they can be applied. Understanding sequence representation is therefore not a technical footnote but a core element of genomic foundation model design, with implications that ripple through every subsequent chapter of this book.