# Introduction {.unnumbered}

We can sequence a human genome for under three hundred dollars and deposit millions of genomes into continental biobanks. A typical whole-genome sequence reveals roughly three million single-nucleotide variants, yet clinicians can confidently act on fewer than a hundred of them. This asymmetry between our capacity to generate genomic data and our ability to interpret it defines the central challenge of modern genomics, and it is precisely the gap that genomic foundation models aim to close.

The interpretive bottleneck is not simply a matter of missing labels or insufficient training data. The human genome encodes regulatory logic distributed across millions of base pairs, with enhancers acting on genes tens of kilobases away, chromatin structure gating access to transcription machinery, and splicing decisions depending on sequence contexts that span entire introns. Classical approaches attacked these problems piecemeal: one model for splice sites, another for transcription factor binding, a third for variant pathogenicity. Each required hand-crafted features, curated training sets, and careful validation within a narrow domain. The result was a fragmented landscape where transferring insights from one task to another demanded starting nearly from scratch.

The foundation model paradigm offers a different approach. Rather than training specialized models for each downstream task, foundation models learn general-purpose representations from massive, heterogeneous datasets using self-supervised objectives such as masked-token prediction or next-token modeling [@bommasani_foundation_2022]. These representations can then be adapted to new problems through fine-tuning, probing, or zero-shot inference. In natural language processing and computer vision, this paradigm has proven transformative. In genomics, the question is whether similar approaches can bridge the gap between sequence and function, connecting raw nucleotides to molecular phenotypes, disease risk, and therapeutic targets [@guo_foundation_2025].

This book examines that intersection. **Genomic foundation models (GFMs)** are large, reusable models trained on genomic and related biological data that can be adapted to many downstream tasks. The term encompasses DNA language models trained on reference genomes and metagenomes, protein language models that learn evolutionary constraints from sequence databases, RNA structure predictors, and multi-omic architectures that integrate across data modalities. What unites them is the core insight that pretraining on broad biological data produces representations that transfer to specialized tasks more effectively than training narrow models from scratch.

The book does not offer a general introduction to genomics or machine learning. Readers seeking comprehensive coverage of molecular biology should consult *Molecular Biology of the Cell*; those wanting deep learning fundamentals will find better treatments in dedicated textbooks. The goal here is narrower and more specific: to provide a conceptual and practical map of how modern deep models for DNA, RNA, and proteins are built, what they actually learn, and how they can be used responsibly in research and clinical workflows. The chapters connect classical genomics pipelines, early deep regulatory models, sequence language models, and multi-omic GFMs into a single narrative arc that emphasizes both capabilities and limitations.


## Why Foundation Models for Genomics?

Traditional genomic modeling has been overwhelmingly task-specific. A variant caller is tuned to distinguish sequencing errors from true variants in a particular sequencing platform and sample type. A supervised convolutional network predicts a fixed set of chromatin marks for a specific cell line. A polygenic risk score is fit for one trait, in one ancestry group, using data from one biobank. These models can achieve excellent performance in the settings they were designed for, but they often transfer poorly to new assays, tissues, ancestries, or institutions. When the input distribution shifts, whether because of a new sequencing chemistry, a different population, or a novel cell type, performance degrades in ways that are difficult to anticipate.

Foundation models address this fragility through three interrelated strategies. First, they leverage scale: training on massive, heterogeneous datasets spanning multiple assays, tissues, species, and cohorts forces the model to learn representations that capture shared biological structure rather than dataset-specific artifacts. Second, they employ self-supervised objectives that do not require manual labels, allowing them to exploit the vast quantities of unlabeled sequence data, perturbation screens, and population variation that genomics generates. Third, they are designed for reusability: rather than training a new model for each task, practitioners probe, adapt, or fine-tune a shared backbone, amortizing the cost of representation learning across many downstream applications.

The extent to which this paradigm delivers on its promises in genomics remains an active research question. Some tasks benefit dramatically from pretrained representations; others show marginal improvement over strong classical baselines. Transfer across species, cell types, and assays works better in some settings than others. The computational costs of training and deploying large models create practical constraints that vary across research and clinical environments. This book does not assume that foundation models are the answer to every genomic problem. It aims instead to equip readers with the frameworks to evaluate when these approaches help, when simpler methods suffice, and how to design analyses that exploit the strengths of modern architectures while remaining alert to their limitations.


## Recurring Themes

Several threads run through the book, and individual chapters can be read as different perspectives on the same underlying questions.

The co-evolution of data and architecture is one such thread. Early variant effect predictors relied on hand-engineered features and shallow models trained on modest curated datasets. Convolutional networks enabled direct learning of regulatory motifs and local grammar from raw sequence, but their fixed receptive fields limited their reach. Transformers and other long-context architectures opened the door to capturing broader regulatory neighborhoods and chromatin structure. Foundation models push toward representations that span multiple assays, tissues, and organisms. At each stage, the question is not simply whether the model is more sophisticated, but how the available data constrain what the model can sensibly learn.

Context length and genomic geometry represent another recurring concern. Many genomic phenomena are intrinsically non-local: enhancers regulate genes across hundreds of kilobases, chromatin loops bring distal elements into contact, and polygenic effects distribute risk across thousands of variants genome-wide. The book returns repeatedly to how models represent these long-range dependencies, what architectural choices enable or constrain their reach, and what is gained or lost as context windows scale.

The distinction between prediction and design cuts across multiple chapters. Most current models are used as predictors: given a sequence and context, what molecular or phenotypic outcome is expected? The same models can also be embedded in design workflows, from variant prioritization and library construction to therapeutic sequence optimization. Foundation models change where the boundary lies between analysis and experimental planning, and they introduce new failure modes when generative or optimization objectives are misspecified.

Evaluation connects benchmark performance to real-world decisions. Benchmark scores are seductive and easy to compare, but biological and clinical decisions are messy, multi-objective, and constrained by data drift, confounding, and poorly specified endpoints. A recurring theme is the gap between state-of-the-art metrics on held-out test sets and actual impact in research or clinical deployment. Careful evaluation, confounder analysis, and calibration can narrow that gap, but only when practitioners understand what their metrics actually measure.

Interpretability and mechanism form a final thread. The book treats interpretability not as optional decoration but as a design constraint that shapes how models should be built and evaluated. Saliency maps, motif extraction, and mechanistic analyses can deepen understanding of what a model has learned, but they can also provide false comfort when applied to confounded or brittle representations. Distinguishing genuine biological insight from pattern-matching artifacts requires both technical tools and careful experimental design.


## How the Book Is Organized

The book is organized into six parts containing twenty-seven chapters, plus five appendices. Each part can be read on its own, but the parts are designed to build on one another.

**Part I: Foundations** lays the genomic and statistical groundwork that later models rest on. @sec-ngs introduces next-generation sequencing, alignment, and variant calling, highlighting sources of error and the evolution from hand-crafted pipelines to learned variant callers. @sec-data surveys the core data resources that underlie most modern work: reference genomes, population variation catalogs, clinical variant databases, and functional genomics consortia such as ENCODE and GTEx. @sec-pgs reviews genome-wide association studies, linkage disequilibrium, fine-mapping, and polygenic scores, emphasizing what these variant-to-trait associations do and do not tell us about mechanism. @sec-cadd covers conservation-based and machine-learning-based variant effect predictors such as CADD, including their feature sets, label construction, and issues of circularity and dataset bias. Together, these chapters answer a foundational question: what data and pre-deep-learning tools form the backdrop that any genomic foundation model must respect, integrate with, or improve upon?

**Part II: Core Principles** introduces the conceptual and technical foundations of modern sequence modeling. @sec-token examines how genomic and protein sequences are converted into model-compatible tokens, covering k-mers, byte-pair encodings, and learned embeddings, and showing how tokenization choices shape downstream representations. @sec-transformers provides a detailed treatment of attention mechanisms, position encodings, and transformer architectures, with emphasis on how these ideas translate from language to biological sequence. @sec-foundation develops a working definition and taxonomy of foundation models in genomics, distinguishing them from earlier supervised approaches and outlining the design dimensions that characterize the field. @sec-pretrain covers pretraining objectives, from masked language modeling and next-token prediction to contrastive and generative approaches, examining how self-supervision extracts structure from unlabeled biological data. @sec-transfer addresses transfer learning, domain adaptation, and few-shot learning, asking when and how pretrained representations generalize to new tasks, species, and data modalities.

**Part III: Model Architectures** surveys the major architectural families in genomic deep learning. @sec-cnn examines convolutional approaches that established the field, including DeepSEA, ExPecto, and SpliceAI, analyzing what they learn about motifs and regulatory grammar and where their fixed receptive fields impose limitations. @sec-dna covers DNA language models such as DNABERT, Nucleotide Transformer, and HyenaDNA, tracing their training corpora, objectives, evaluation suites, and current capabilities. @sec-rna extends beyond splicing to RNA structure prediction and RNA foundation models, examining how secondary structure and functional context inform representation learning. @sec-prot describes large protein language models trained on evolutionary sequence databases, their emergent structure and function representations, and applications to variant effect prediction and protein design. @sec-hybrid covers hybrid CNN-transformer and related architectures designed for long genomic contexts, such as Enformer and Borzoi, which predict regulatory readouts over tens to hundreds of kilobases.

**Part IV: Multi-Modal and Multi-Scale Modeling** examines how foundation model principles extend beyond sequence to embrace cellular and systems-level biology. @sec-epi covers foundation models for single-cell transcriptomics, epigenomics, and three-dimensional genome structure, showing how transformer architectures adapt to the unique characteristics of these data types. @sec-networks turns to graph neural networks and network-based approaches that represent genes, proteins, and their interactions as structured graphs rather than flat sequences. @sec-systems broadens the view to multi-omics integration and systems-level reasoning, exploring how models can jointly represent genomic, transcriptomic, proteomic, and clinical information to connect sequence variation to phenotype across multiple layers of biological organization.

**Part V: Evaluation and Interpretation** develops frameworks for assessing what models actually learn and how reliably they perform. @sec-benchmarks surveys existing benchmarks for genomic foundation models, analyzing their construction, coverage, and limitations. @sec-eval presents evaluation principles and proper methodology, covering data splitting, metric choice, and the link between benchmark performance and real-world utility. @sec-vep recasts variant effect prediction in the foundation model era, spanning protein-based and DNA-based approaches and discussing calibration, uncertainty, and integration into existing pipelines. @sec-confound details sources of confounding and data leakage, from batch effects and ancestry structure to label bias and covariate shift, offering practical strategies for detection and mitigation. @sec-interp explores interpretability tools from classical motif discovery and attribution methods to emerging mechanistic approaches, asking when these tools reveal genuine biological mechanisms and when they provide false comfort.

**Part VI: Translation** moves from methods to end-to-end workflows in research and clinical practice. @sec-clinical discusses clinical risk prediction that combines genomic features with electronic health records and environmental data, focusing on discrimination, calibration, fairness, and deployment in health systems. @sec-variants examines how foundation models fit into rare disease and cancer workflows, including variant prioritization pipelines, integration with family and tumor-normal data, and laboratory validation. @sec-drugs looks at how GFMs intersect with target discovery, functional genomics screens, and biomarker development in pharmaceutical and biotechnology settings. @sec-design covers generative applications, from protein design and therapeutic sequence optimization to synthetic biology and bioengineering workflows. @sec-future concludes with open problems, emerging directions, and considerations for responsible development and deployment of genomic AI systems.

**Five appendices** provide supporting material. @sec-apx-dl offers a compact introduction to neural networks, CNNs, transformers, training, and evaluation for readers who want enough machine learning background to engage with the main chapters without consulting external references. @sec-apx-deployment covers practical considerations for deploying genomic foundation models, including computational requirements, infrastructure, and operational concerns. @sec-apx-models provides a comprehensive reference table of models discussed throughout the book, with architecture summaries, training data, and key citations. @sec-apx-resources offers a curated collection of datasets, software tools, courses, and papers for deeper exploration. @sec-apx-glossary defines key terms spanning genomics, machine learning, and clinical applications.


## What This Book Aims to Provide

Genomic foundation models represent a moving target: architectures, datasets, and evaluation suites evolve rapidly. This book is not intended as a frozen survey of the current state of the art but as a framework for reasoning about new models as they appear.

Readers who work through the material should be able to place a new model in the landscape of data, architecture, objective, and application. They should be equipped to design analyses and experiments that use GFMs as components, whether as feature extractors, priors, or simulators, without overclaiming what the models can do. They should recognize common pitfalls in training, evaluation, and deployment, especially in clinical and translational settings where errors have real consequences. And they should be able to decide where foundation models are genuinely useful and where simpler methods or classical baselines remain sufficient.

The next chapter turns to the foundations: how raw reads become variants, how variants become the datasets and benchmarks on which all subsequent models depend, and where the errors in this upstream process create systematic challenges that propagate through everything that follows.