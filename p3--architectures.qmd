# Part III: Foundation Model Families {.unnumbered}

Each architecture embodies a different set of assumptions about biological sequence. Convolutional models assume that local motifs and their short-range combinations are the primary carriers of regulatory information; they learn to recognize transcription factor binding sites, splice signals, and chromatin accessibility patterns from the sequence grammar immediately surrounding each position. Protein language models treat amino acid sequences as structured compositions whose meaning emerges from evolutionary context; they learn what substitutions are tolerated by observing which sequences survived natural selection. DNA language models extend this paradigm to nucleotides, learning regulatory grammar through self-supervised objectives that predict masked or next tokens. Hybrid architectures attempt to reconcile local and global perspectives, using convolutions to extract features efficiently while deploying attention to model interactions spanning tens or hundreds of kilobases. Understanding these assumptions clarifies what each model family can capture and where each will fail.

This part surveys the major foundation model families in genomic deep learning. @sec-fm-principles establishes what defines a foundation model and develops a practical taxonomy for navigating the rapidly expanding ecosystem. @sec-dna-lm examines DNA language models, including DNABERT, Nucleotide Transformer, and HyenaDNA, that apply self-supervised pretraining to genomic sequence, learning representations that transfer across diverse downstream tasks. @sec-protein-lm turns to protein language models, where the foundation model paradigm achieved its earliest and most dramatic successes; ESM, ProtTrans, and their descendants emerged alongside AlphaFold2 in 2020, collectively demonstrating that deep learning could capture protein structure and function from sequence alone. AlphaFold2 revolutionized structure prediction through its Evoformer architecture, and AlphaMissense subsequently adapted that architecture for proteome-wide variant effect prediction. 

@sec-regulatory examines hybrid architectures, including Enformer, Borzoi, and AlphaGenome, that combine convolutional processing with transformer blocks to achieve context windows spanning hundreds of kilobases, enabling direct prediction of gene expression from sequence. @sec-vep-fm synthesizes these approaches in the context of variant effect prediction, showing how foundation model representations translate into pathogenicity scores across variant types and genomic contexts. By the end of this part, readers will understand not only how each model family works but when to deploy each and what limitations to anticipate.