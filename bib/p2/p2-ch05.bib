@article{ji_dnabert_2021,
	title = {{DNABERT}: pre-trained {Bidirectional} {Encoder} {Representations} from {Transformers} model for {DNA}-language in genome},
	volume = {37},
	issn = {1367-4803},
	shorttitle = {{DNABERT}},
	url = {https://doi.org/10.1093/bioinformatics/btab083},
	doi = {10.1093/bioinformatics/btab083},
	abstract = {Deciphering the language of non-coding DNA is one of the fundamental problems in genome research. Gene regulatory code is highly complex due to the existence of polysemy and distant semantic relationship, which previous informatics methods often fail to capture especially in data-scarce scenarios.To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, to capture global and transferrable understanding of genomic DNA sequences based on up and downstream nucleotide contexts. We compared DNABERT to the most widely used programs for genome-wide regulatory elements prediction and demonstrate its ease of use, accuracy and efficiency. We show that the single pre-trained transformers model can simultaneously achieve state-of-the-art performance on prediction of promoters, splice sites and transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, DNABERT enables direct visualization of nucleotide-level importance and semantic relationship within input sequences for better interpretability and accurate identification of conserved sequence motifs and functional genetic variant candidates. Finally, we demonstrate that pre-trained DNABERT with human genome can even be readily applied to other organisms with exceptional performance. We anticipate that the pre-trained DNABERT model can be fined tuned to many other sequence analyses tasks.The source code, pretrained and finetuned model for DNABERT are available at GitHub (https://github.com/jerryji1993/DNABERT).Supplementary data are available at Bioinformatics online.},
	number = {15},
	urldate = {2025-08-31},
	journal = {Bioinformatics},
	author = {Ji, Yanrong and Zhou, Zhihan and Liu, Han and Davuluri, Ramana V},
	month = aug,
	year = {2021},
	note = {802 citations (Crossref/DOI) [2025-10-22]},
	pages = {2112--2120},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/VKSTMFPQ/2021-08-09 - Ji et al. - DNABERT pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/UBBEDFL5/btab083.html:text/html},
}
@misc{liu_life-code_2025,
	title = {Life-{Code}: {Central} {Dogma} {Modeling} with {Multi}-{Omics} {Sequence} {Unification}},
	shorttitle = {Life-{Code}},
	url = {http://arxiv.org/abs/2502.07299},
	doi = {10.48550/arXiv.2502.07299},
	abstract = {The interactions between DNA, RNA, and proteins are fundamental to biological processes, as illustrated by the central dogma of molecular biology. Although modern biological pre-trained models have achieved great success in analyzing these macromolecules individually, their interconnected nature remains underexplored. This paper follows the guidance of the central dogma to redesign both the data and model pipeline and offers a comprehensive framework, Life-Code, that spans different biological functions. As for data flow, we propose a unified pipeline to integrate multi-omics data by reverse-transcribing RNA and reverse-translating amino acids into nucleotide-based sequences. As for the model, we design a codon tokenizer and a hybrid long-sequence architecture to encode the interactions between coding and non-coding regions through masked modeling pre-training. To model the translation and folding process with coding sequences, Life-Code learns protein structures of the corresponding amino acids by knowledge distillation from off-the-shelf protein language models. Such designs enable Life-Code to capture complex interactions within genetic sequences, providing a more comprehensive understanding of multi-omics with the central dogma. Extensive experiments show that Life-Code achieves state-of-the-art results on various tasks across three omics, highlighting its potential for advancing multi-omics analysis and interpretation.},
	urldate = {2025-08-07},
	publisher = {arXiv},
	author = {Liu, Zicheng and Li, Siyuan and Chen, Zhiyuan and Wu, Fang and Yu, Chang and Yang, Qirong and Guo, Yucheng and Yang, Yujie and Zhang, Xiaoming and Li, Stan Z.},
	month = jun,
	year = {2025},
	note = {arXiv:2502.07299 [cs]},
	keywords = {Podcast, Printed},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/LFXSCF3K/2025-06-15 - Liu et al. - Life-Code Central Dogma Modeling with Multi-Omics Sequence Unification.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/P626N5L8/2502.html:text/html},
}
@misc{medvedev_biotoken_2025,
	title = {{BioToken} and {BioFM} – {Biologically}-{Informed} {Tokenization} {Enables} {Accurate} and {Efficient} {Genomic} {Foundation} {Models}},
	copyright = {© 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2025.03.27.645711v1},
	doi = {10.1101/2025.03.27.645711},
	abstract = {Genomic variation underlies human phenotypic diversity, disease susceptibility, and evolutionary adaptation. Although large-scale genomic sequencing has transformed our ability to map genetic variation, accurately modeling and interpreting this data remains challenging due to fundamental limitations in existing genomic foundation models (GFMs). Current genomic models typically treat DNA simplistically as linear textual sequences, overlooking critical biological context, such as genomic structural annotations, regulatory elements, and functional contexts central to genomic interpretation. As a result, these models are prone to positional memorization of common sequences, severely limiting their generalization to biologically meaningful tasks. Here, we introduce BioToken, a modular and extendable tokenization framework designed to encode genomic variants and biologically relevant structural annotations directly into genomic representations. By utilizing intrinsic inductive biases, BioToken facilitates meaningful representation learning and generalization across diverse molecular phenotypes, such as gene expression, alternative splicing, and variant pathogenicity prediction. Built on BioToken, our genomic foundation model, BioFM, achieves competitive or superior results relative to specialized models (e.g., Enformer, SpliceTransformer) across a comprehensive suite of genomic benchmarks, including noncoding pathogenicity, expression modulation, sQTL prediction, and long-range genomic interactions. Notably, BioFM achieves state-of-the-art performance with significantly fewer parameters (265M), substantially reducing training costs and computational requirements. Our findings high-light the substantial advantages of integrating biologically-informed inductive biases into genomic foundation modeling, providing a robust and accessible path forward in genomics. We provide our code and model checkpoints to support further research in this direction.},
	language = {en},
	urldate = {2025-05-27},
	publisher = {bioRxiv},
	author = {Medvedev, Aleksandr and Viswanathan, Karthik and Kanithi, Praveenkumar and Vishniakov, Kirill and Munjal, Prateek and Christophe, Clément and Pimentel, Marco AF and Rajan, Ronnie and Khan, Shadab},
	month = apr,
	year = {2025},
	note = {1 citations (Crossref/DOI) [2025-10-22]
Pages: 2025.03.27.645711
Section: New Results},
	keywords = {Podcast, Printed},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/XZBQLGZE/2025-04-01 - Medvedev et al. - BioToken and BioFM – Biologically-Informed Tokenization Enables Accurate and Efficient Genomic Found.pdf:application/pdf},
}
@misc{nguyen_hyenadna_2023,
	title = {{HyenaDNA}: {Long}-{Range} {Genomic} {Sequence} {Modeling} at {Single} {Nucleotide} {Resolution}},
	shorttitle = {{HyenaDNA}},
	url = {http://arxiv.org/abs/2306.15794},
	doi = {10.48550/arXiv.2306.15794},
	abstract = {Genomic (DNA) sequences encode an enormous amount of information for gene regulation and protein synthesis. Similar to natural language models, researchers have proposed foundation models in genomics to learn generalizable features from unlabeled genome data that can then be fine-tuned for downstream tasks such as identifying regulatory elements. Due to the quadratic scaling of attention, previous Transformer-based genomic models have used 512 to 4k tokens as context ({\textless}0.001\% of the human genome), significantly limiting the modeling of long-range interactions in DNA. In addition, these methods rely on tokenizers or fixed k-mers to aggregate meaningful DNA units, losing single nucleotide resolution where subtle genetic variations can completely alter protein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a large language model based on implicit convolutions was shown to match attention in quality while allowing longer context lengths and lower time complexity. Leveraging Hyena's new long-range capabilities, we present HyenaDNA, a genomic foundation model pretrained on the human reference genome with context lengths of up to 1 million tokens at the single nucleotide-level - an up to 500x increase over previous dense attention-based models. HyenaDNA scales sub-quadratically in sequence length (training up to 160x faster than Transformer), uses single nucleotide tokens, and has full global context at each layer. We explore what longer context enables - including the first use of in-context learning in genomics. On fine-tuned benchmarks from the Nucleotide Transformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 18 datasets using a model with orders of magnitude less parameters and pretraining data. On the GenomicBenchmarks, HyenaDNA surpasses SotA on 7 of 8 datasets on average by +10 accuracy points. Code at https://github.com/HazyResearch/hyena-dna.},
	urldate = {2025-07-23},
	publisher = {arXiv},
	author = {Nguyen, Eric and Poli, Michael and Faizi, Marjan and Thomas, Armin and Birch-Sykes, Callum and Wornow, Michael and Patel, Aman and Rabideau, Clayton and Massaroli, Stefano and Bengio, Yoshua and Ermon, Stefano and Baccus, Stephen A. and Ré, Chris},
	month = nov,
	year = {2023},
	note = {arXiv:2306.15794 [cs]},
	keywords = {Important, Podcast, Printed, Read},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/JVEF4BBI/2023-11-14 - Nguyen et al. - HyenaDNA Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/G59U5Z77/2306.html:text/html},
}
@article{sanabria_grover_2024,
	title = {[{GROVER}] {DNA} language model {GROVER} learns sequence context in the human genome},
	volume = {6},
	copyright = {2024 The Author(s)},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-024-00872-0},
	doi = {10.1038/s42256-024-00872-0},
	abstract = {Deep-learning models that learn a sense of language on DNA have achieved a high level of performance on genome biological tasks. Genome sequences follow rules similar to natural language but are distinct in the absence of a concept of words. We established byte-pair encoding on the human genome and trained a foundation language model called GROVER (Genome Rules Obtained Via Extracted Representations) with the vocabulary selected via a custom task, next-k-mer prediction. The defined dictionary of tokens in the human genome carries best the information content for GROVER. Analysing learned representations, we observed that trained token embeddings primarily encode information related to frequency, sequence content and length. Some tokens are primarily localized in repeats, whereas the majority widely distribute over the genome. GROVER also learns context and lexical ambiguity. Average trained embeddings of genomic regions relate to functional genomics annotation and thus indicate learning of these structures purely from the contextual relationships of tokens. This highlights the extent of information content encoded by the sequence that can be grasped by GROVER. On fine-tuning tasks addressing genome biology with questions of genome element identification and protein–DNA binding, GROVER exceeds other models’ performance. GROVER learns sequence context, a sense for structure and language rules. Extracting this knowledge can be used to compose a grammar book for the code of life.},
	language = {en},
	number = {8},
	urldate = {2025-07-25},
	journal = {Nature Machine Intelligence},
	author = {Sanabria, Melissa and Hirsch, Jonas and Joubert, Pierre M. and Poetsch, Anna R.},
	month = jul,
	year = {2024},
	note = {47 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Printed},
	pages = {911--923},
	file = {2024-07-23 - Sanabria et al. - [Preprint][GROVER] DNA language model GROVER learns sequence context in the human genome:/Users/meehl.joshua/Zotero/storage/WLAJANB6/2024-07-23 - Sanabria et al. - [Preprint][GROVER] DNA language model GROVER learns sequence context in the human genome.pdf:application/pdf;Full Text PDF:/Users/meehl.joshua/Zotero/storage/SNYYQVLT/2024-07-23 - Sanabria et al. - [GROVER] DNA language model GROVER learns sequence context in the human genome.pdf:application/pdf},
}
@misc{schiff_caduceus_2024,
	title = {Caduceus: {Bi}-{Directional} {Equivariant} {Long}-{Range} {DNA} {Sequence} {Modeling}},
	shorttitle = {Caduceus},
	url = {http://arxiv.org/abs/2403.03234},
	doi = {10.48550/arXiv.2403.03234},
	abstract = {Large-scale sequence modeling has sparked rapid advances that now extend into biology and genomics. However, modeling genomic sequences introduces challenges such as the need to model long-range token interactions, the effects of upstream and downstream regions of the genome, and the reverse complementarity (RC) of DNA. Here, we propose an architecture motivated by these challenges that builds off the long-range Mamba block, and extends it to a BiMamba component that supports bi-directionality, and to a MambaDNA block that additionally supports RC equivariance. We use MambaDNA as the basis of Caduceus, the first family of RC equivariant bi-directional long-range DNA language models, and we introduce pre-training and fine-tuning strategies that yield Caduceus DNA foundation models. Caduceus outperforms previous long-range models on downstream benchmarks; on a challenging long-range variant effect prediction task, Caduceus exceeds the performance of 10x larger models that do not leverage bi-directionality or equivariance.},
	urldate = {2025-07-23},
	publisher = {arXiv},
	author = {Schiff, Yair and Kao, Chia-Hsiang and Gokaslan, Aaron and Dao, Tri and Gu, Albert and Kuleshov, Volodymyr},
	month = jun,
	year = {2024},
	note = {arXiv:2403.03234 [q-bio]},
	keywords = {Podcast, Printed, Read},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/PN2AQVMU/2024-06-05 - Schiff et al. - Caduceus Bi-Directional Equivariant Long-Range DNA Sequence Modeling.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/YIPW7H9E/2403.html:text/html},
}
@misc{zhou_dnabert-2_2024,
	title = {{DNABERT}-2: {Efficient} {Foundation} {Model} and {Benchmark} {For} {Multi}-{Species} {Genome}},
	shorttitle = {{DNABERT}-2},
	url = {http://arxiv.org/abs/2306.15006},
	doi = {10.48550/arXiv.2306.15006},
	abstract = {Decoding the linguistic intricacies of the genome is a crucial problem in biology, and pre-trained foundational models such as DNABERT and Nucleotide Transformer have made significant strides in this area. Existing works have largely hinged on k-mer, fixed-length permutations of A, T, C, and G, as the token of the genome language due to its simplicity. However, we argue that the computation and sample inefficiencies introduced by k-mer tokenization are primary obstacles in developing large genome foundational models. We provide conceptual and empirical insights into genome tokenization, building on which we propose to replace k-mer tokenization with Byte Pair Encoding (BPE), a statistics-based data compression algorithm that constructs tokens by iteratively merging the most frequent co-occurring genome segment in the corpus. We demonstrate that BPE not only overcomes the limitations of k-mer tokenization but also benefits from the computational efficiency of non-overlapping tokenization. Based on these insights, we introduce DNABERT-2, a refined genome foundation model that adapts an efficient tokenizer and employs multiple strategies to overcome input length constraints, reduce time and memory expenditure, and enhance model capability. Furthermore, we identify the absence of a comprehensive and standardized benchmark for genome understanding as another significant impediment to fair comparative analysis. In response, we propose the Genome Understanding Evaluation (GUE), a comprehensive multi-species genome classification dataset that amalgamates \$36\$ distinct datasets across \$9\$ tasks, with input lengths ranging from \$70\$ to \$10000\$. Through comprehensive experiments on the GUE benchmark, we demonstrate that DNABERT-2 achieves comparable performance to the state-of-the-art model with \$21 {\textbackslash}times\$ fewer parameters and approximately \$92 {\textbackslash}times\$ less GPU time in pre-training.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Zhou, Zhihan and Ji, Yanrong and Li, Weijian and Dutta, Pratik and Davuluri, Ramana and Liu, Han},
	month = mar,
	year = {2024},
	note = {arXiv:2306.15006 [q-bio]},
	keywords = {Important, Podcast, Printed, Read},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/2KZP5KPQ/2024-03-18 - Zhou et al. - DNABERT-2 Efficient Foundation Model and Benchmark For Multi-Species Genome.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/KUYT79IL/2306.html:text/html},
}
@misc{zvyagin_genslms_2022,
	title = {{GenSLMs}: {Genome}-scale language models reveal {SARS}-{CoV}-2 evolutionary dynamics},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	shorttitle = {{GenSLMs}},
	url = {https://www.biorxiv.org/content/10.1101/2022.10.10.511571v2},
	doi = {10.1101/2022.10.10.511571},
	abstract = {We seek to transform how new and emergent variants of pandemiccausing viruses, specifically SARS-CoV-2, are identified and classified. By adapting large language models (LLMs) for genomic data, we build genome-scale language models (GenSLMs) which can learn the evolutionary landscape of SARS-CoV-2 genomes. By pretraining on over 110 million prokaryotic gene sequences and finetuning a SARS-CoV-2-specific model on 1.5 million genomes, we show that GenSLMs can accurately and rapidly identify variants of concern. Thus, to our knowledge, GenSLMs represents one of the first whole genome scale foundation models which can generalize to other prediction tasks. We demonstrate scaling of GenSLMs on GPU-based supercomputers and AI-hardware accelerators utilizing 1.63 Zettaflops in training runs with a sustained performance of 121 PFLOPS in mixed precision and peak of 850 PFLOPS. We present initial scientific insights from examining GenSLMs in tracking evolutionary dynamics of SARS-CoV-2, paving the path to realizing this on large biological data.},
	language = {en},
	urldate = {2025-01-28},
	publisher = {bioRxiv},
	author = {Zvyagin, Maxim and Brace, Alexander and Hippe, Kyle and Deng, Yuntian and Zhang, Bin and Bohorquez, Cindy Orozco and Clyde, Austin and Kale, Bharat and Perez-Rivera, Danilo and Ma, Heng and Mann, Carla M. and Irvin, Michael and Pauloski, J. Gregory and Ward, Logan and Hayot-Sasson, Valerie and Emani, Murali and Foreman, Sam and Xie, Zhen and Lin, Diangen and Shukla, Maulik and Nie, Weili and Romero, Josh and Dallago, Christian and Vahdat, Arash and Xiao, Chaowei and Gibbs, Thomas and Foster, Ian and Davis, James J. and Papka, Michael E. and Brettin, Thomas and Stevens, Rick and Anandkumar, Anima and Vishwanath, Venkatram and Ramanathan, Arvind},
	month = nov,
	year = {2022},
	note = {24 citations (Crossref/DOI) [2025-10-22]
Pages: 2022.10.10.511571
Section: New Results},
	keywords = {Podcast, Printed, Read},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/2UK2TV2N/2022-11-23 - Zvyagin et al. - GenSLMs Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics.pdf:application/pdf},
}
