@article{avsec_enformer_2021,
	title = {[{Enformer}] {Effective} gene expression prediction from sequence by integrating long-range interactions},
	volume = {18},
	url = {https://consensus.app/papers/effective-gene-expression-prediction-from-sequence-by-avsec-agarwal/6afb944129f35bad916e6f4a889c07cb/},
	doi = {10.1038/s41592-021-01252-x},
	abstract = {The next phase of genome biology research requires understanding how DNA sequence encodes phenotypes, from the molecular to organismal levels. How noncoding DNA determines gene expression in different cell types is a major unsolved problem, and critical downstream applications in human genetics depend on improved solutions. Here, we report substantially improved gene expression prediction accuracy from DNA sequence through the use of a new deep learning architecture called Enformer that is able to integrate long-range interactions (up to 100 kb away) in the genome. This improvement yielded more accurate variant effect predictions on gene expression for both natural genetic variants and saturation mutagenesis measured by massively parallel reporter assays. Notably, Enformer outperformed the best team on the critical assessment of genome interpretation (CAGI5) challenge for noncoding variant interpretation with no additional training. Furthermore, Enformer learned to predict promoter-enhancer interactions directly from DNA sequence competitively with methods that take direct experimental data as input. We expect that these advances will enable more effective fine-mapping of growing human disease associations to cell-type-specific gene regulatory mechanisms and provide a framework to interpret cis-regulatory evolution. To foster these downstream applications, we have made the pre-trained Enformer model openly available, and provide pre-computed effect predictions for all common variants in the 1000 Genomes dataset. One-sentence summary Improved noncoding variant effect prediction and candidate enhancer prioritization from a more accurate sequence to expression model driven by extended long-range interaction modelling.},
	urldate = {2024-12-25},
	journal = {Nature Methods},
	author = {Avsec, Žiga and Agarwal, Vikram and Visentin, D. and Ledsam, J. and Grabska-Barwinska, A. and Taylor, Kyle R. and Assael, Yannis and Jumper, J. and Kohli, Pushmeet and Kelley, David R.},
	month = oct,
	year = {2021},
	note = {846 citations (Semantic Scholar/DOI) [2025-10-22]
875 citations (Crossref/DOI) [2025-10-22]},
	keywords = {Podcast, Printed, Read},
	pages = {1196--1203},
	file = {41592_2021_1252_MOESM1_ESM:/Users/meehl.joshua/Zotero/storage/7PUC6VKT/41592_2021_1252_MOESM1_ESM.pdf:application/pdf;Full Text:/Users/meehl.joshua/Zotero/storage/WTX3AUV5/2021-10-04 - Avsec et al. - [Enformer] Effective gene expression prediction from sequence by integrating long-range interactions.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/JWYN5S8L/6afb944129f35bad916e6f4a889c07cb.html:text/html},
}
@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2025-12-16},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/I6VVQR7P/2019-05-24 - Devlin et al. - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/4JZ4TPWS/1810.html:text/html},
}
@article{ferruz_protgpt2_2022,
	title = {{ProtGPT2} is a deep unsupervised language model for protein design},
	volume = {13},
	copyright = {2022 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-022-32007-7},
	doi = {10.1038/s41467-022-32007-7},
	abstract = {Protein design aims to build novel proteins customized for specific purposes, thereby holding the potential to tackle many environmental and biomedical problems. Recent progress in Transformer-based architectures has enabled the implementation of language models capable of generating text with human-like capabilities. Here, motivated by this success, we describe ProtGPT2, a language model trained on the protein space that generates de novo protein sequences following the principles of natural ones. The generated proteins display natural amino acid propensities, while disorder predictions indicate that 88\% of ProtGPT2-generated proteins are globular, in line with natural sequences. Sensitive sequence searches in protein databases show that ProtGPT2 sequences are distantly related to natural ones, and similarity networks further demonstrate that ProtGPT2 is sampling unexplored regions of protein space. AlphaFold prediction of ProtGPT2-sequences yields well-folded non-idealized structures with embodiments and large loops and reveals topologies not captured in current structure databases. ProtGPT2 generates sequences in a matter of seconds and is freely available.},
	language = {en},
	number = {1},
	urldate = {2025-12-16},
	journal = {Nature Communications},
	author = {Ferruz, Noelia and Schmidt, Steffen and Höcker, Birte},
	month = jul,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	pages = {4348},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/EQY4RAB2/2022-07-27 - Ferruz et al. - ProtGPT2 is a deep unsupervised language model for protein design.pdf:application/pdf},
}
@article{ji_dnabert_2021,
	title = {{DNABERT}: pre-trained {Bidirectional} {Encoder} {Representations} from {Transformers} model for {DNA}-language in genome},
	volume = {37},
	issn = {1367-4803},
	shorttitle = {{DNABERT}},
	url = {https://doi.org/10.1093/bioinformatics/btab083},
	doi = {10.1093/bioinformatics/btab083},
	abstract = {Deciphering the language of non-coding DNA is one of the fundamental problems in genome research. Gene regulatory code is highly complex due to the existence of polysemy and distant semantic relationship, which previous informatics methods often fail to capture especially in data-scarce scenarios.To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, to capture global and transferrable understanding of genomic DNA sequences based on up and downstream nucleotide contexts. We compared DNABERT to the most widely used programs for genome-wide regulatory elements prediction and demonstrate its ease of use, accuracy and efficiency. We show that the single pre-trained transformers model can simultaneously achieve state-of-the-art performance on prediction of promoters, splice sites and transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, DNABERT enables direct visualization of nucleotide-level importance and semantic relationship within input sequences for better interpretability and accurate identification of conserved sequence motifs and functional genetic variant candidates. Finally, we demonstrate that pre-trained DNABERT with human genome can even be readily applied to other organisms with exceptional performance. We anticipate that the pre-trained DNABERT model can be fined tuned to many other sequence analyses tasks.The source code, pretrained and finetuned model for DNABERT are available at GitHub (https://github.com/jerryji1993/DNABERT).Supplementary data are available at Bioinformatics online.},
	number = {15},
	urldate = {2025-08-31},
	journal = {Bioinformatics},
	author = {Ji, Yanrong and Zhou, Zhihan and Liu, Han and Davuluri, Ramana V},
	month = aug,
	year = {2021},
	note = {802 citations (Crossref/DOI) [2025-10-22]},
	pages = {2112--2120},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/VKSTMFPQ/2021-08-09 - Ji et al. - DNABERT pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/UBBEDFL5/btab083.html:text/html},
}
@article{kagda_data_2025,
	title = {Data navigation on the {ENCODE} portal},
	volume = {16},
	copyright = {2025 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-025-64343-9},
	doi = {10.1038/s41467-025-64343-9},
	abstract = {Spanning two decades, the collaborative ENCODE project aims to identify all the functional elements within human and mouse genomes. To best serve the scientific community, the comprehensive ENCODE data including results from 23,000+ functional genomics experiments, 800+ functional elements characterization experiments and 60,000+ results from integrative computational analyses are available on an open-access data-portal (https://www.encodeproject.org/). The final phase of the project includes data from several novel assays aimed at characterization and validation of genomic elements. In addition to developing and maintaining the data portal, the Data Coordination Center (DCC) implemented and utilised uniform processing pipelines to generate uniformly processed data. Here we report recent updates to the data portal including a redesigned home page, an improved search interface, new custom-designed pages highlighting biologically related datasets and an enhanced cart interface for data visualisation plus user-friendly data download options. A summary of data generated using uniform processing pipelines is also provided.},
	language = {en},
	number = {1},
	urldate = {2025-12-02},
	journal = {Nature Communications},
	author = {Kagda, Meenakshi S. and Lam, Bonita and Litton, Casey and Small, Corinn and Sloan, Cricket A. and Spragins, Emma and Tanaka, Forrest and Whaling, Ian and Gabdank, Idan and Youngworth, Ingrid and Strattan, J. Seth and Hilton, Jason and Jou, Jennifer and Au, Jessica and Lee, Jin-Wook and Andreeva, Kalina and Graham, Keenan and Lin, Khine and Simison, Matt and Jolanki, Otto and Sud, Paul and Assis, Pedro and Adenekan, Philip and Miyasato, Stuart and Zhong, Weiwei and Luo, Yunhai and Myers, Zachary and Cherry, J. Michael and Hitz, Benjamin C.},
	month = oct,
	year = {2025},
	note = {Publisher: Nature Publishing Group},
	pages = {9592},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/J7ECWE4N/2025-10-30 - Kagda et al. - Data navigation on the ENCODE portal.pdf:application/pdf},
}
@misc{kaplan_scaling_2020,
	title = {Scaling {Laws} for {Neural} {Language} {Models}},
	url = {http://arxiv.org/abs/2001.08361},
	doi = {10.48550/arXiv.2001.08361},
	abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
	urldate = {2025-12-23},
	publisher = {arXiv},
	author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
	month = jan,
	year = {2020},
	note = {arXiv:2001.08361 [cs]},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/49GWWPEQ/2020-01-23 - Kaplan et al. - Scaling Laws for Neural Language Models.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/4EH6ZD93/2001.html:text/html},
}
@article{karczewski_mutational_2020,
	title = {The mutational constraint spectrum quantified from variation in 141,456 humans},
	volume = {581},
	copyright = {2020 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-020-2308-7},
	doi = {10.1038/s41586-020-2308-7},
	abstract = {Genetic variants that inactivate protein-coding genes are a powerful source of information about the phenotypic consequences of gene disruption: genes that are crucial for the function of an organism will be depleted of such variants in natural populations, whereas non-essential genes will tolerate their accumulation. However, predicted loss-of-function variants are enriched for annotation errors, and tend to be found at extremely low frequencies, so their analysis requires careful variant annotation and very large sample sizes1. Here we describe the aggregation of 125,748 exomes and 15,708 genomes from human sequencing studies into the Genome Aggregation Database (gnomAD). We identify 443,769 high-confidence predicted loss-of-function variants in this cohort after filtering for artefacts caused by sequencing and annotation errors. Using an improved model of human mutation rates, we classify human protein-coding genes along a spectrum that represents tolerance to inactivation, validate this classification using data from model organisms and engineered human cells, and show that it can be used to improve the power of gene discovery for both common and rare diseases.},
	language = {en},
	number = {7809},
	urldate = {2025-09-10},
	journal = {Nature},
	author = {Karczewski, Konrad J. and Francioli, Laurent C. and Tiao, Grace and Cummings, Beryl B. and Alföldi, Jessica and Wang, Qingbo and Collins, Ryan L. and Laricchia, Kristen M. and Ganna, Andrea and Birnbaum, Daniel P. and Gauthier, Laura D. and Brand, Harrison and Solomonson, Matthew and Watts, Nicholas A. and Rhodes, Daniel and Singer-Berk, Moriel and England, Eleina M. and Seaby, Eleanor G. and Kosmicki, Jack A. and Walters, Raymond K. and Tashman, Katherine and Farjoun, Yossi and Banks, Eric and Poterba, Timothy and Wang, Arcturus and Seed, Cotton and Whiffin, Nicola and Chong, Jessica X. and Samocha, Kaitlin E. and Pierce-Hoffman, Emma and Zappala, Zachary and O’Donnell-Luria, Anne H. and Minikel, Eric Vallabh and Weisburd, Ben and Lek, Monkol and Ware, James S. and Vittal, Christopher and Armean, Irina M. and Bergelson, Louis and Cibulskis, Kristian and Connolly, Kristen M. and Covarrubias, Miguel and Donnelly, Stacey and Ferriera, Steven and Gabriel, Stacey and Gentry, Jeff and Gupta, Namrata and Jeandet, Thibault and Kaplan, Diane and Llanwarne, Christopher and Munshi, Ruchi and Novod, Sam and Petrillo, Nikelle and Roazen, David and Ruano-Rubio, Valentin and Saltzman, Andrea and Schleicher, Molly and Soto, Jose and Tibbetts, Kathleen and Tolonen, Charlotte and Wade, Gordon and Talkowski, Michael E. and Neale, Benjamin M. and Daly, Mark J. and MacArthur, Daniel G.},
	month = may,
	year = {2020},
	note = {8022 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	pages = {434--443},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/I266L595/Karczewski et al. - 2020 - The mutational constraint spectrum quantified from variation in 141,456 humans.pdf:application/pdf},
}
@article{landrum_clinvar_2018,
	title = {{ClinVar}: improving access to variant interpretations and supporting evidence},
	volume = {46},
	issn = {0305-1048},
	shorttitle = {{ClinVar}},
	url = {https://doi.org/10.1093/nar/gkx1153},
	doi = {10.1093/nar/gkx1153},
	abstract = {ClinVar (https://www.ncbi.nlm.nih.gov/clinvar/) is a freely available, public archive of human genetic variants and interpretations of their significance to disease, maintained at the National Institutes of Health. Interpretations of the clinical significance of variants are submitted by clinical testing laboratories, research laboratories, expert panels and other groups. ClinVar aggregates data by variant-disease pairs, and by variant (or set of variants). Data aggregated by variant are accessible on the website, in an improved set of variant call format files and as a new comprehensive XML report. ClinVar recently started accepting submissions that are focused primarily on providing phenotypic information for individuals who have had genetic testing. Submissions may come from clinical providers providing their own interpretation of the variant (‘provider interpretation’) or from groups such as patient registries that primarily provide phenotypic information from patients (‘phenotyping only’). ClinVar continues to make improvements to its search and retrieval functions. Several new fields are now indexed for more precise searching, and filters allow the user to narrow down a large set of search results.},
	number = {D1},
	urldate = {2025-12-06},
	journal = {Nucleic Acids Research},
	author = {Landrum, Melissa J and Lee, Jennifer M and Benson, Mark and Brown, Garth R and Chao, Chen and Chitipiralla, Shanmuga and Gu, Baoshan and Hart, Jennifer and Hoffman, Douglas and Jang, Wonhee and Karapetyan, Karen and Katz, Kenneth and Liu, Chunlei and Maddipatla, Zenith and Malheiro, Adriana and McDaniel, Kurt and Ovetsky, Michael and Riley, George and Zhou, George and Holmes, J Bradley and Kattman, Brandi L and Maglott, Donna R},
	month = jan,
	year = {2018},
	pages = {D1062--D1067},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/PBKZHYPQ/2018-01-04 - Landrum et al. - ClinVar improving access to variant interpretations and supporting evidence.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/F7C3YBZY/gkx1153.html:text/html},
}
@misc{lin_esm-2_2022,
	title = {[{ESM}-2] {Language} models of protein sequences at the scale of evolution enable accurate structure prediction},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1},
	doi = {10.1101/2022.07.20.500902},
	abstract = {Large language models have recently been shown to develop emergent capabilities with scale, going beyond simple pattern matching to perform higher level reasoning and generate lifelike images and text. While language models trained on protein sequences have been studied at a smaller scale, little is known about what they learn about biology as they are scaled up. In this work we train models up to 15 billion parameters, the largest language models of proteins to be evaluated to date. We find that as models are scaled they learn information enabling the prediction of the three-dimensional structure of a protein at the resolution of individual atoms. We present ESMFold for high accuracy end-to-end atomic level structure prediction directly from the individual sequence of a protein. ESMFold has similar accuracy to AlphaFold2 and RoseTTAFold for sequences with low perplexity that are well understood by the language model. ESMFold inference is an order of magnitude faster than AlphaFold2, enabling exploration of the structural space of metagenomic proteins in practical timescales.},
	language = {en},
	urldate = {2022-12-20},
	publisher = {bioRxiv},
	author = {Lin, Zeming and Akin, Halil and Rao, Roshan and Hie, Brian and Zhu, Zhongkai and Lu, Wenting and Costa, Allan dos Santos and Fazel-Zarandi, Maryam and Sercu, Tom and Candido, Sal and Rives, Alexander},
	month = jul,
	year = {2022},
	note = {273 citations (Crossref/DOI) [2025-10-22]
Pages: 2022.07.20.500902
Section: New Results},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/B28E6SXM/2022-07-21 - Lin et al. - [ESM-2] Language models of protein sequences at the scale of evolution enable accurate structure pre.pdf:application/pdf},
}
@article{linder_borzoi_2025,
	title = {[{Borzoi}] {Predicting} {RNA}-seq coverage from {DNA} sequence as a unifying model of gene regulation},
	volume = {57},
	copyright = {2025 The Author(s)},
	issn = {1546-1718},
	url = {https://www.nature.com/articles/s41588-024-02053-6},
	doi = {10.1038/s41588-024-02053-6},
	abstract = {Sequence-based machine-learning models trained on genomics data improve genetic variant interpretation by providing functional predictions describing their impact on the cis-regulatory code. However, current tools do not predict RNA-seq expression profiles because of modeling challenges. Here, we introduce Borzoi, a model that learns to predict cell-type-specific and tissue-specific RNA-seq coverage from DNA sequence. Using statistics derived from Borzoi’s predicted coverage, we isolate and accurately score DNA variant effects across multiple layers of regulation, including transcription, splicing and polyadenylation. Evaluated on quantitative trait loci, Borzoi is competitive with and often outperforms state-of-the-art models trained on individual regulatory functions. By applying attribution methods to the derived statistics, we extract cis-regulatory motifs driving RNA expression and post-transcriptional regulation in normal tissues. The wide availability of RNA-seq data across species, conditions and assays profiling specific aspects of regulation emphasizes the potential of this approach to decipher the mapping from DNA sequence to regulatory function.},
	language = {en},
	number = {4},
	urldate = {2025-05-24},
	journal = {Nature Genetics},
	author = {Linder, Johannes and Srivastava, Divyanshi and Yuan, Han and Agarwal, Vikram and Kelley, David R.},
	month = jan,
	year = {2025},
	note = {66 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Important, Podcast, Printed},
	pages = {949--961},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/CZUJB3B3/2025-01-08 - Linder et al. - [Borzoi] Predicting RNA-seq coverage from DNA sequence as a unifying model of gene regulation.pdf:application/pdf},
}
@misc{nguyen_hyenadna_2023,
	title = {{HyenaDNA}: {Long}-{Range} {Genomic} {Sequence} {Modeling} at {Single} {Nucleotide} {Resolution}},
	shorttitle = {{HyenaDNA}},
	url = {http://arxiv.org/abs/2306.15794},
	doi = {10.48550/arXiv.2306.15794},
	abstract = {Genomic (DNA) sequences encode an enormous amount of information for gene regulation and protein synthesis. Similar to natural language models, researchers have proposed foundation models in genomics to learn generalizable features from unlabeled genome data that can then be fine-tuned for downstream tasks such as identifying regulatory elements. Due to the quadratic scaling of attention, previous Transformer-based genomic models have used 512 to 4k tokens as context ({\textless}0.001\% of the human genome), significantly limiting the modeling of long-range interactions in DNA. In addition, these methods rely on tokenizers or fixed k-mers to aggregate meaningful DNA units, losing single nucleotide resolution where subtle genetic variations can completely alter protein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a large language model based on implicit convolutions was shown to match attention in quality while allowing longer context lengths and lower time complexity. Leveraging Hyena's new long-range capabilities, we present HyenaDNA, a genomic foundation model pretrained on the human reference genome with context lengths of up to 1 million tokens at the single nucleotide-level - an up to 500x increase over previous dense attention-based models. HyenaDNA scales sub-quadratically in sequence length (training up to 160x faster than Transformer), uses single nucleotide tokens, and has full global context at each layer. We explore what longer context enables - including the first use of in-context learning in genomics. On fine-tuned benchmarks from the Nucleotide Transformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 18 datasets using a model with orders of magnitude less parameters and pretraining data. On the GenomicBenchmarks, HyenaDNA surpasses SotA on 7 of 8 datasets on average by +10 accuracy points. Code at https://github.com/HazyResearch/hyena-dna.},
	urldate = {2025-07-23},
	publisher = {arXiv},
	author = {Nguyen, Eric and Poli, Michael and Faizi, Marjan and Thomas, Armin and Birch-Sykes, Callum and Wornow, Michael and Patel, Aman and Rabideau, Clayton and Massaroli, Stefano and Bengio, Yoshua and Ermon, Stefano and Baccus, Stephen A. and Ré, Chris},
	month = nov,
	year = {2023},
	note = {arXiv:2306.15794 [cs]},
	keywords = {Important, Podcast, Printed, Read},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/JVEF4BBI/2023-11-14 - Nguyen et al. - HyenaDNA Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/G59U5Z77/2306.html:text/html},
}
@article{nguyen_sequence_2024,
	title = {Sequence modeling and design from molecular to genome scale with {Evo}},
	volume = {386},
	url = {https://www.science.org/doi/full/10.1126/science.ado9336},
	doi = {10.1126/science.ado9336},
	abstract = {The genome is a sequence that encodes the DNA, RNA, and proteins that orchestrate an organism’s function. We present Evo, a long-context genomic foundation model with a frontier architecture trained on millions of prokaryotic and phage genomes, and report scaling laws on DNA to complement observations in language and vision. Evo generalizes across DNA, RNA, and proteins, enabling zero-shot function prediction competitive with domain-specific language models and the generation of functional CRISPR-Cas and transposon systems, representing the first examples of protein-RNA and protein-DNA codesign with a language model. Evo also learns how small mutations affect whole-organism fitness and generates megabase-scale sequences with plausible genomic architecture. These prediction and generation capabilities span molecular to genomic scales of complexity, advancing our understanding and control of biology.},
	number = {6723},
	urldate = {2025-12-23},
	journal = {Science},
	author = {Nguyen, Eric and Poli, Michael and Durrant, Matthew G. and Kang, Brian and Katrekar, Dhruva and Li, David B. and Bartie, Liam J. and Thomas, Armin W. and King, Samuel H. and Brixi, Garyk and Sullivan, Jeremy and Ng, Madelena Y. and Lewis, Ashley and Lou, Aaron and Ermon, Stefano and Baccus, Stephen A. and Hernandez-Boussard, Tina and Ré, Christopher and Hsu, Patrick D. and Hie, Brian L.},
	month = nov,
	year = {2024},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eado9336},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/5SZDCHFG/2024-11-15 - Nguyen et al. - Sequence modeling and design from molecular to genome scale with Evo.pdf:application/pdf},
}
@misc{oord_representation_2019,
	title = {Representation {Learning} with {Contrastive} {Predictive} {Coding}},
	url = {http://arxiv.org/abs/1807.03748},
	doi = {10.48550/arXiv.1807.03748},
	abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
	urldate = {2025-12-23},
	publisher = {arXiv},
	author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	month = jan,
	year = {2019},
	note = {arXiv:1807.03748 [cs]},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/Z9ZANY85/2019-01-22 - Oord et al. - Representation Learning with Contrastive Predictive Coding.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/LD78UV48/1807.html:text/html},
}
@misc{raffel_exploring_2023,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
	url = {http://arxiv.org/abs/1910.10683},
	doi = {10.48550/arXiv.1910.10683},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	urldate = {2025-12-16},
	publisher = {arXiv},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	month = sep,
	year = {2023},
	note = {arXiv:1910.10683 [cs]},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/LVIR98Q9/2023-09-19 - Raffel et al. - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/B5EP85VE/1910.html:text/html},
}
@article{suzek_uniref_2007,
	title = {{UniRef}: comprehensive and non-redundant {UniProt} reference clusters},
	volume = {23},
	issn = {1367-4803},
	shorttitle = {{UniRef}},
	url = {https://doi.org/10.1093/bioinformatics/btm098},
	doi = {10.1093/bioinformatics/btm098},
	abstract = {Motivation: Redundant protein sequences in biological databases hinder sequence similarity searches and make interpretation of search results difficult. Clustering of protein sequence space based on sequence similarity helps organize all sequences into manageable datasets and reduces sampling bias and overrepresentation of sequences.Results: The UniRef (UniProt Reference Clusters) provide clustered sets of sequences from the UniProt Knowledgebase (UniProtKB) and selected UniProt Archive records to obtain complete coverage of sequence space at several resolutions while hiding redundant sequences. Currently covering \&gt;4 million source sequences, the UniRef100 database combines identical sequences and subfragments from any source organism into a single UniRef entry. UniRef90 and UniRef50 are built by clustering UniRef100 sequences at the 90 or 50\% sequence identity levels. UniRef100, UniRef90 and UniRef50 yield a database size reduction of ∼10, 40 and 70\%, respectively, from the source sequence set. The reduced redundancy increases the speed of similarity searches and improves detection of distant relationships. UniRef entries contain summary cluster and membership information, including the sequence of a representative protein, member count and common taxonomy of the cluster, the accession numbers of all the merged entries and links to rich functional annotation in UniProtKB to facilitate biological discovery. UniRef has already been applied to broad research areas ranging from genome annotation to proteomics data analysis.Availability: UniRef is updated biweekly and is available for online search and retrieval at http://www.uniprot.org, as well as for download at ftp://ftp.uniprot.org/pub/databases/uniprot/unirefContact:  bes23@georgetown.eduSupplementary information: Supplementary data are available at Bioinformatics online.},
	number = {10},
	urldate = {2025-12-16},
	journal = {Bioinformatics},
	author = {Suzek, Baris E. and Huang, Hongzhan and McGarvey, Peter and Mazumder, Raja and Wu, Cathy H.},
	month = may,
	year = {2007},
	pages = {1282--1288},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/SBM7NX8T/2007-05-15 - Suzek et al. - UniRef comprehensive and non-redundant UniProt reference clusters.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/3VW858PD/btm098.html:text/html},
}
@misc{zhou_dnabert-2_2024,
	title = {{DNABERT}-2: {Efficient} {Foundation} {Model} and {Benchmark} {For} {Multi}-{Species} {Genome}},
	shorttitle = {{DNABERT}-2},
	url = {http://arxiv.org/abs/2306.15006},
	doi = {10.48550/arXiv.2306.15006},
	abstract = {Decoding the linguistic intricacies of the genome is a crucial problem in biology, and pre-trained foundational models such as DNABERT and Nucleotide Transformer have made significant strides in this area. Existing works have largely hinged on k-mer, fixed-length permutations of A, T, C, and G, as the token of the genome language due to its simplicity. However, we argue that the computation and sample inefficiencies introduced by k-mer tokenization are primary obstacles in developing large genome foundational models. We provide conceptual and empirical insights into genome tokenization, building on which we propose to replace k-mer tokenization with Byte Pair Encoding (BPE), a statistics-based data compression algorithm that constructs tokens by iteratively merging the most frequent co-occurring genome segment in the corpus. We demonstrate that BPE not only overcomes the limitations of k-mer tokenization but also benefits from the computational efficiency of non-overlapping tokenization. Based on these insights, we introduce DNABERT-2, a refined genome foundation model that adapts an efficient tokenizer and employs multiple strategies to overcome input length constraints, reduce time and memory expenditure, and enhance model capability. Furthermore, we identify the absence of a comprehensive and standardized benchmark for genome understanding as another significant impediment to fair comparative analysis. In response, we propose the Genome Understanding Evaluation (GUE), a comprehensive multi-species genome classification dataset that amalgamates \$36\$ distinct datasets across \$9\$ tasks, with input lengths ranging from \$70\$ to \$10000\$. Through comprehensive experiments on the GUE benchmark, we demonstrate that DNABERT-2 achieves comparable performance to the state-of-the-art model with \$21 {\textbackslash}times\$ fewer parameters and approximately \$92 {\textbackslash}times\$ less GPU time in pre-training.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Zhou, Zhihan and Ji, Yanrong and Li, Weijian and Dutta, Pratik and Davuluri, Ramana and Liu, Han},
	month = mar,
	year = {2024},
	note = {arXiv:2306.15006 [q-bio]},
	keywords = {Important, Podcast, Printed, Read},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/2KZP5KPQ/2024-03-18 - Zhou et al. - DNABERT-2 Efficient Foundation Model and Benchmark For Multi-Species Genome.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/KUYT79IL/2306.html:text/html},
}
