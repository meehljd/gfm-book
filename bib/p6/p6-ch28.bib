@article{madani_progen_2023,
	title = {Large language models generate functional protein sequences across diverse families},
	volume = {41},
	copyright = {2023 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-1696},
	url = {https://www.nature.com/articles/s41587-022-01618-2},
	doi = {10.1038/s41587-022-01618-2},
	abstract = {Deep-learning language models have shown promise in various biotechnological applications, including protein design and engineering. Here we describe ProGen, a language model that can generate protein sequences with a predictable function across large protein families, akin to generating grammatically and semantically correct natural language sentences on diverse topics. The model was trained on 280 million protein sequences from {\textgreater}19,000 families and is augmented with control tags specifying protein properties. ProGen can be further fine-tuned to curated sequences and tags to improve controllable generation performance of proteins from families with sufficient homologous samples. Artificial proteins fine-tuned to five distinct lysozyme families showed similar catalytic efficiencies as natural lysozymes, with sequence identity to natural proteins as low as 31.4\%. ProGen is readily adapted to diverse protein families, as we demonstrate with chorismate mutase and malate dehydrogenase.},
	language = {en},
	number = {8},
	urldate = {2025-12-16},
	journal = {Nature Biotechnology},
	author = {Madani, Ali and Krause, Ben and Greene, Eric R. and Subramanian, Subu and Mohr, Benjamin P. and Holton, James M. and Olmos, Jose Luis and Xiong, Caiming and Sun, Zachary Z. and Socher, Richard and Fraser, James S. and Naik, Nikhil},
	month = aug,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	pages = {1099--1106},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/7XMBUQVH/2023-08 - Madani et al. - Large language models generate functional protein sequences across diverse families.pdf:application/pdf},
}

@article{ferruz_protgpt2_2022,
	title = {{ProtGPT2} is a deep unsupervised language model for protein design},
	volume = {13},
	copyright = {2022 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-022-32007-7},
	doi = {10.1038/s41467-022-32007-7},
	abstract = {Protein design aims to build novel proteins customized for specific purposes, thereby holding the potential to tackle many environmental and biomedical problems. Recent progress in Transformer-based architectures has enabled the implementation of language models capable of generating text with human-like capabilities. Here, motivated by this success, we describe ProtGPT2, a language model trained on the protein space that generates de novo protein sequences following the principles of natural ones. The generated proteins display natural amino acid propensities, while disorder predictions indicate that 88\% of ProtGPT2-generated proteins are globular, in line with natural sequences. Sensitive sequence searches in protein databases show that ProtGPT2 sequences are distantly related to natural ones, and similarity networks further demonstrate that ProtGPT2 is sampling unexplored regions of protein space. AlphaFold prediction of ProtGPT2-sequences yields well-folded non-idealized structures with embodiments and large loops and reveals topologies not captured in current structure databases. ProtGPT2 generates sequences in a matter of seconds and is freely available.},
	language = {en},
	number = {1},
	urldate = {2025-12-16},
	journal = {Nature Communications},
	author = {Ferruz, Noelia and Schmidt, Steffen and Höcker, Birte},
	month = jul,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	pages = {4348},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/EQY4RAB2/2022-07-27 - Ferruz et al. - ProtGPT2 is a deep unsupervised language model for protein design.pdf:application/pdf},
}

@article{watson_rfdiffusion_2023,
	title = {De novo design of protein structure and function with {RFdiffusion}},
	volume = {620},
	copyright = {2023 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-023-06415-8},
	doi = {10.1038/s41586-023-06415-8},
	abstract = {There has been considerable recent progress in designing new proteins using deep-learning methods1–9. Despite this progress, a general deep-learning framework for protein design that enables solution of a wide range of design challenges, including de novo binder design and design of higher-order symmetric architectures, has yet to be described. Diffusion models10,11 have had considerable success in image and language generative modelling but limited success when applied to protein modelling, probably due to the complexity of protein backbone geometry and sequence–structure relationships. Here we show that by fine-tuning the RoseTTAFold structure prediction network on protein structure denoising tasks, we obtain a generative model of protein backbones that achieves outstanding performance on unconditional and topology-constrained protein monomer design, protein binder design, symmetric oligomer design, enzyme active site scaffolding and symmetric motif scaffolding for therapeutic and metal-binding protein design. We demonstrate the power and generality of the method, called RoseTTAFold diffusion (RFdiffusion), by experimentally characterizing the structures and functions of hundreds of designed symmetric assemblies, metal-binding proteins and protein binders. The accuracy of RFdiffusion is confirmed by the cryogenic electron microscopy structure of a designed binder in complex with influenza haemagglutinin that is nearly identical to the design model. In a manner analogous to networks that produce images from user-specified inputs, RFdiffusion enables the design of diverse functional proteins from simple molecular specifications.},
	language = {en},
	number = {7976},
	urldate = {2025-12-16},
	journal = {Nature},
	author = {Watson, Joseph L. and Juergens, David and Bennett, Nathaniel R. and Trippe, Brian L. and Yim, Jason and Eisenach, Helen E. and Ahern, Woody and Borst, Andrew J. and Ragotte, Robert J. and Milles, Lukas F. and Wicky, Basile I. M. and Hanikel, Nikita and Pellock, Samuel J. and Courbet, Alexis and Sheffler, William and Wang, Jue and Venkatesh, Preetham and Sappington, Isaac and Torres, Susana Vázquez and Lauko, Anna and De Bortoli, Valentin and Mathieu, Emile and Ovchinnikov, Sergey and Barzilay, Regina and Jaakkola, Tommi S. and DiMaio, Frank and Baek, Minkyung and Baker, David},
	month = aug,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	pages = {1089--1100},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/J88PZ35T/2023-08 - Watson et al. - De novo design of protein structure and function with RFdiffusion.pdf:application/pdf},
}

@article{dauparas_proteinmpnn_2022,
	title = {Robust deep learning–based protein sequence design using {ProteinMPNN}},
	volume = {378},
	url = {https://www.science.org/doi/10.1126/science.add2187},
	doi = {10.1126/science.add2187},
	abstract = {Although deep learning has revolutionized protein structure prediction, almost all experimentally characterized de novo protein designs have been generated using physically based approaches such as Rosetta. Here, we describe a deep learning–based protein sequence design method, ProteinMPNN, that has outstanding performance in both in silico and experimental tests. On native protein backbones, ProteinMPNN has a sequence recovery of 52.4\% compared with 32.9\% for Rosetta. The amino acid sequence at different positions can be coupled between single or multiple chains, enabling application to a wide range of current protein design challenges. We demonstrate the broad utility and high accuracy of ProteinMPNN using x-ray crystallography, cryo–electron microscopy, and functional studies by rescuing previously failed designs, which were made using Rosetta or AlphaFold, of protein monomers, cyclic homo-oligomers, tetrahedral nanoparticles, and target-binding proteins.},
	number = {6615},
	urldate = {2025-12-16},
	journal = {Science},
	author = {Dauparas, J. and Anishchenko, I. and Bennett, N. and Bai, H. and Ragotte, R. J. and Milles, L. F. and Wicky, B. I. M. and Courbet, A. and de Haas, R. J. and Bethel, N. and Leung, P. J. Y. and Huddy, T. F. and Pellock, S. and Tischer, D. and Chan, F. and Koepnick, B. and Nguyen, H. and Kang, A. and Sankaran, B. and Bera, A. K. and King, N. P. and Baker, D.},
	month = oct,
	year = {2022},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {49--56},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/P9Z3VXWS/2022-10-07 - Dauparas et al. - Robust deep learning–based protein sequence design using ProteinMPNN.pdf:application/pdf},
}

@misc{hsu_learning_2022,
	title = {MISSING BIB ENTRY: hsu_learning_2022},
	note = {Placeholder generated from chapter citations; replace with full BibTeX.},
	year = {2022},
}

@article{deboer_deciphering_2020,
	title = {Deciphering eukaryotic gene-regulatory logic with 100 million random promoters},
	volume = {38},
	copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-1696},
	url = {https://www.nature.com/articles/s41587-019-0315-8},
	doi = {10.1038/s41587-019-0315-8},
	abstract = {How transcription factors (TFs) interpret cis-regulatory DNA sequence to control gene expression remains unclear, largely because past studies using native and engineered sequences had insufficient scale. Here, we measure the expression output of {\textgreater}100 million synthetic yeast promoter sequences that are fully random. These sequences yield diverse, reproducible expression levels that can be explained by their chance inclusion of functional TF binding sites. We use machine learning to build interpretable models of transcriptional regulation that predict {\textasciitilde}94\% of the expression driven from independent test promoters and {\textasciitilde}89\% of the expression driven from native yeast promoter fragments. These models allow us to characterize each TF’s specificity, activity and interactions with chromatin. TF activity depends on binding-site strand, position, DNA helical face and chromatin context. Notably, expression level is influenced by weak regulatory interactions, which confound designed-sequence studies. Our analyses show that massive-throughput assays of fully random DNA can provide the big data necessary to develop complex, predictive models of gene regulation.},
	language = {en},
	number = {1},
	urldate = {2025-12-16},
	journal = {Nature Biotechnology},
	author = {de Boer, Carl G. and Vaishnav, Eeshit Dhaval and Sadeh, Ronen and Abeyta, Esteban Luis and Friedman, Nir and Regev, Aviv},
	month = jan,
	year = {2020},
	note = {Publisher: Nature Publishing Group},
	pages = {56--65},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/5LSPI9KB/2020-01 - de Boer et al. - Deciphering eukaryotic gene-regulatory logic with 100 million random promoters.pdf:application/pdf},
}
