@article{zhou_deepsea_2015,
	title = {[{DeepSEA}] {Predicting} effects of noncoding variants with deep learning–based sequence model},
	volume = {12},
	copyright = {2015 Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.3547},
	doi = {10.1038/nmeth.3547},
	abstract = {DeepSEA, a deep-learning algorithm trained on large-scale chromatin-profiling data, predicts chromatin effects from sequence alone, has single-nucleotide sensitivity and can predict effects of noncoding variants.},
	language = {en},
	number = {10},
	urldate = {2025-08-12},
	journal = {Nature Methods},
	author = {Zhou, Jian and Troyanskaya, Olga G.},
	month = aug,
	year = {2015},
	note = {2031 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	pages = {931--934},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/DS72YP6N/2015-10 - Zhou and Troyanskaya - [DeepSEA] Predicting effects of noncoding variants with deep learning–based sequence model.pdf:application/pdf},
}

@article{zhou_expecto_2018,
	title = {[{Expecto}] {Deep} learning sequence-based ab initio prediction of variant effects on expression and disease risk},
	volume = {50},
	copyright = {2018 The Author(s)},
	issn = {1546-1718},
	url = {https://www.nature.com/articles/s41588-018-0160-6},
	doi = {10.1038/s41588-018-0160-6},
	abstract = {Key challenges for human genetics, precision medicine and evolutionary biology include deciphering the regulatory code of gene expression and understanding the transcriptional effects of genome variation. However, this is extremely difficult because of the enormous scale of the noncoding mutation space. We developed a deep learning–based framework, ExPecto, that can accurately predict, ab initio from a DNA sequence, the tissue-specific transcriptional effects of mutations, including those that are rare or that have not been observed. We prioritized causal variants within disease- or trait-associated loci from all publicly available genome-wide association studies and experimentally validated predictions for four immune-related diseases. By exploiting the scalability of ExPecto, we characterized the regulatory mutation space for human RNA polymerase II–transcribed genes by in silico saturation mutagenesis and profiled {\textgreater} 140 million promoter-proximal mutations. This enables probing of evolutionary constraints on gene expression and ab initio prediction of mutation disease effects, making ExPecto an end-to-end computational framework for the in silico prediction of expression and disease risk.},
	language = {en},
	number = {8},
	urldate = {2025-07-25},
	journal = {Nature Genetics},
	author = {Zhou, Jian and Theesfeld, Chandra L. and Yao, Kevin and Chen, Kathleen M. and Wong, Aaron K. and Troyanskaya, Olga G.},
	month = jul,
	year = {2018},
	note = {420 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Printed, Read, Recall},
	pages = {1171--1179},
	file = {2018-07-16 - Zhou et al. - [DeepSEA Beluga] Deep learning sequence-based ab initio prediction of variant effects on expression:/Users/meehl.joshua/Zotero/storage/GDYBADGZ/2018-07-16 - Zhou et al. - [DeepSEA Beluga] Deep learning sequence-based ab initio prediction of variant effects on expression.pdf:application/pdf},
}

@article{jaganathan_spliceai_2019,
	title = {[{SpliceAI}] {Predicting} {Splicing} from {Primary} {Sequence} with {Deep} {Learning}},
	volume = {176},
	issn = {0092-8674},
	url = {https://www.sciencedirect.com/science/article/pii/S0092867418316295},
	doi = {10.1016/j.cell.2018.12.015},
	abstract = {The splicing of pre-mRNAs into mature transcripts is remarkable for its precision, but the mechanisms by which the cellular machinery achieves such specificity are incompletely understood. Here, we describe a deep neural network that accurately predicts splice junctions from an arbitrary pre-mRNA transcript sequence, enabling precise prediction of noncoding genetic variants that cause cryptic splicing. Synonymous and intronic mutations with predicted splice-altering consequence validate at a high rate on RNA-seq and are strongly deleterious in the human population. De novo mutations with predicted splice-altering consequence are significantly enriched in patients with autism and intellectual disability compared to healthy controls and validate against RNA-seq in 21 out of 28 of these patients. We estimate that 9\%–11\% of pathogenic mutations in patients with rare genetic disorders are caused by this previously underappreciated class of disease variation.},
	number = {3},
	urldate = {2025-07-11},
	journal = {Cell},
	author = {Jaganathan, Kishore and Kyriazopoulou Panagiotopoulou, Sofia and McRae, Jeremy F. and Darbandi, Siavash Fazel and Knowles, David and Li, Yang I. and Kosmicki, Jack A. and Arbelaez, Juan and Cui, Wenwu and Schwartz, Grace B. and Chow, Eric D. and Kanterakis, Efstathios and Gao, Hong and Kia, Amirali and Batzoglou, Serafim and Sanders, Stephan J. and Farh, Kyle Kai-How},
	month = jan,
	year = {2019},
	note = {2156 citations (Crossref/DOI) [2025-10-22]},
	keywords = {Printed, Important, Podcast, Read},
	pages = {535--548.e24},
	file = {Full Text:/Users/meehl.joshua/Zotero/storage/RNFB2GEK/2019-01-24 - Jaganathan et al. - [SpliceAI] Predicting Splicing from Primary Sequence with Deep Learning.pdf:application/pdf;ScienceDirect Snapshot:/Users/meehl.joshua/Zotero/storage/7K7VMGER/S0092867418316295.html:text/html},
}

@article{ji_dnabert_2021,
	title = {{DNABERT}: pre-trained {Bidirectional} {Encoder} {Representations} from {Transformers} model for {DNA}-language in genome},
	volume = {37},
	issn = {1367-4803},
	shorttitle = {{DNABERT}},
	url = {https://doi.org/10.1093/bioinformatics/btab083},
	doi = {10.1093/bioinformatics/btab083},
	abstract = {Deciphering the language of non-coding DNA is one of the fundamental problems in genome research. Gene regulatory code is highly complex due to the existence of polysemy and distant semantic relationship, which previous informatics methods often fail to capture especially in data-scarce scenarios.To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, to capture global and transferrable understanding of genomic DNA sequences based on up and downstream nucleotide contexts. We compared DNABERT to the most widely used programs for genome-wide regulatory elements prediction and demonstrate its ease of use, accuracy and efficiency. We show that the single pre-trained transformers model can simultaneously achieve state-of-the-art performance on prediction of promoters, splice sites and transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, DNABERT enables direct visualization of nucleotide-level importance and semantic relationship within input sequences for better interpretability and accurate identification of conserved sequence motifs and functional genetic variant candidates. Finally, we demonstrate that pre-trained DNABERT with human genome can even be readily applied to other organisms with exceptional performance. We anticipate that the pre-trained DNABERT model can be fined tuned to many other sequence analyses tasks.The source code, pretrained and finetuned model for DNABERT are available at GitHub (https://github.com/jerryji1993/DNABERT).Supplementary data are available at Bioinformatics online.},
	number = {15},
	urldate = {2025-08-31},
	journal = {Bioinformatics},
	author = {Ji, Yanrong and Zhou, Zhihan and Liu, Han and Davuluri, Ramana V},
	month = aug,
	year = {2021},
	note = {802 citations (Crossref/DOI) [2025-10-22]},
	pages = {2112--2120},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/VKSTMFPQ/2021-08-09 - Ji et al. - DNABERT pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/UBBEDFL5/btab083.html:text/html},
}

@article{avsec_enformer_2021,
	title = {[{Enformer}] {Effective} gene expression prediction from sequence by integrating long-range interactions},
	volume = {18},
	url = {https://consensus.app/papers/effective-gene-expression-prediction-from-sequence-by-avsec-agarwal/6afb944129f35bad916e6f4a889c07cb/},
	doi = {10.1038/s41592-021-01252-x},
	abstract = {The next phase of genome biology research requires understanding how DNA sequence encodes phenotypes, from the molecular to organismal levels. How noncoding DNA determines gene expression in different cell types is a major unsolved problem, and critical downstream applications in human genetics depend on improved solutions. Here, we report substantially improved gene expression prediction accuracy from DNA sequence through the use of a new deep learning architecture called Enformer that is able to integrate long-range interactions (up to 100 kb away) in the genome. This improvement yielded more accurate variant effect predictions on gene expression for both natural genetic variants and saturation mutagenesis measured by massively parallel reporter assays. Notably, Enformer outperformed the best team on the critical assessment of genome interpretation (CAGI5) challenge for noncoding variant interpretation with no additional training. Furthermore, Enformer learned to predict promoter-enhancer interactions directly from DNA sequence competitively with methods that take direct experimental data as input. We expect that these advances will enable more effective fine-mapping of growing human disease associations to cell-type-specific gene regulatory mechanisms and provide a framework to interpret cis-regulatory evolution. To foster these downstream applications, we have made the pre-trained Enformer model openly available, and provide pre-computed effect predictions for all common variants in the 1000 Genomes dataset. One-sentence summary Improved noncoding variant effect prediction and candidate enhancer prioritization from a more accurate sequence to expression model driven by extended long-range interaction modelling.},
	urldate = {2024-12-25},
	journal = {Nature Methods},
	author = {Avsec, Žiga and Agarwal, Vikram and Visentin, D. and Ledsam, J. and Grabska-Barwinska, A. and Taylor, Kyle R. and Assael, Yannis and Jumper, J. and Kohli, Pushmeet and Kelley, David R.},
	month = oct,
	year = {2021},
	note = {846 citations (Semantic Scholar/DOI) [2025-10-22]
875 citations (Crossref/DOI) [2025-10-22]},
	keywords = {Printed, Podcast, Read},
	pages = {1196--1203},
	file = {41592_2021_1252_MOESM1_ESM:/Users/meehl.joshua/Zotero/storage/7PUC6VKT/41592_2021_1252_MOESM1_ESM.pdf:application/pdf;Full Text:/Users/meehl.joshua/Zotero/storage/WTX3AUV5/2021-10-04 - Avsec et al. - [Enformer] Effective gene expression prediction from sequence by integrating long-range interactions.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/JWYN5S8L/6afb944129f35bad916e6f4a889c07cb.html:text/html},
}

@misc{nguyen_hyenadna_2023,
	title = {{HyenaDNA}: {Long}-{Range} {Genomic} {Sequence} {Modeling} at {Single} {Nucleotide} {Resolution}},
	shorttitle = {{HyenaDNA}},
	url = {http://arxiv.org/abs/2306.15794},
	doi = {10.48550/arXiv.2306.15794},
	abstract = {Genomic (DNA) sequences encode an enormous amount of information for gene regulation and protein synthesis. Similar to natural language models, researchers have proposed foundation models in genomics to learn generalizable features from unlabeled genome data that can then be fine-tuned for downstream tasks such as identifying regulatory elements. Due to the quadratic scaling of attention, previous Transformer-based genomic models have used 512 to 4k tokens as context ({\textless}0.001\% of the human genome), significantly limiting the modeling of long-range interactions in DNA. In addition, these methods rely on tokenizers or fixed k-mers to aggregate meaningful DNA units, losing single nucleotide resolution where subtle genetic variations can completely alter protein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a large language model based on implicit convolutions was shown to match attention in quality while allowing longer context lengths and lower time complexity. Leveraging Hyena's new long-range capabilities, we present HyenaDNA, a genomic foundation model pretrained on the human reference genome with context lengths of up to 1 million tokens at the single nucleotide-level - an up to 500x increase over previous dense attention-based models. HyenaDNA scales sub-quadratically in sequence length (training up to 160x faster than Transformer), uses single nucleotide tokens, and has full global context at each layer. We explore what longer context enables - including the first use of in-context learning in genomics. On fine-tuned benchmarks from the Nucleotide Transformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 18 datasets using a model with orders of magnitude less parameters and pretraining data. On the GenomicBenchmarks, HyenaDNA surpasses SotA on 7 of 8 datasets on average by +10 accuracy points. Code at https://github.com/HazyResearch/hyena-dna.},
	urldate = {2025-07-23},
	publisher = {arXiv},
	author = {Nguyen, Eric and Poli, Michael and Faizi, Marjan and Thomas, Armin and Birch-Sykes, Callum and Wornow, Michael and Patel, Aman and Rabideau, Clayton and Massaroli, Stefano and Bengio, Yoshua and Ermon, Stefano and Baccus, Stephen A. and Ré, Chris},
	month = nov,
	year = {2023},
	note = {arXiv:2306.15794 [cs]},
	keywords = {Printed, Important, Podcast, Read},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/JVEF4BBI/2023-11-14 - Nguyen et al. - HyenaDNA Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/G59U5Z77/2306.html:text/html},
}
