@misc{chowdhery_palm_2022,
	title = {{PaLM}: {Scaling} {Language} {Modeling} with {Pathways}},
	shorttitle = {{PaLM}},
	url = {http://arxiv.org/abs/2204.02311},
	doi = {10.48550/arXiv.2204.02311},
	abstract = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.},
	urldate = {2025-12-26},
	publisher = {arXiv},
	author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
	month = oct,
	year = {2022},
	note = {arXiv:2204.02311 [cs]},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/2MLGMMR3/2022-10-05 - Chowdhery et al. - PaLM Scaling Language Modeling with Pathways.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/CGMF73IX/2204.html:text/html},
}

@article{davey_smith_mendelian_2003,
	title = {‘{Mendelian} randomization’: can genetic epidemiology contribute to understanding environmental determinants of disease?*},
	volume = {32},
	issn = {0300-5771},
	shorttitle = {‘{Mendelian} randomization’},
	url = {https://doi.org/10.1093/ije/dyg070},
	doi = {10.1093/ije/dyg070},
	abstract = {Associations between modifiable exposures and disease seen in observational epidemiology are sometimes confounded and thus misleading, despite our best efforts to improve the design and analysis of studies. Mendelian randomization—the random assortment of genes from parents to offspring that occurs during gamete formation and conception—provides one method for assessing the causal nature of some environmental exposures. The association between a disease and a polymorphism that mimics the biological link between a proposed exposure and disease is not generally susceptible to the reverse causation or confounding that may distort interpretations of conventional observational studies. Several examples where the phenotypic effects of polymorphisms are well documented provide encouraging evidence of the explanatory power of Mendelian randomization and are described. The limitations of the approach include confounding by polymorphisms in linkage disequilibrium with the polymorphism under study, that polymorphisms may have several phenotypic effects associated with disease, the lack of suitable polymorphisms for studying modifiable exposures of interest, and canalization—the buffering of the effects of genetic variation during development. Nevertheless, Mendelian randomization provides new opportunities to test causality and demonstrates how investment in the human genome project may contribute to understanding and preventing the adverse effects on human health of modifiable exposures.Genetic epidemiology—the theme of this issue of the International Journal of Epidemiology—is seen by many to be the only future for epidemiology, perhaps reflecting a growing awareness of the limitations of observational epidemiology1 (Box 1). Genetic epidemiology is concerned with understanding heritable aspects of disease risk, individual susceptibility to disease, and ultimately with contributing to a comprehensive molecular understanding of pathogenesis. The massive investment and expansion of human genetics, if it is to return value for the common good, must be integrated into public health functions. The human genome epidemiology network (HuGE Net—http://www.cdc.gov/genetics/huge.htm) has been established to promote the use of genetic knowledge—in terms of genetic tests and services—for disease prevention and health promotion.2,3 A broad taxonomy of human genome studies of public health relevance has been developed4 (Box 2). In this issue of the IJE, we publish a paper by Miguel Porta,5 who highlights the need for a more rational approach to genetic testing, given the likely low penetrance of many genes associated with cancers,6 likening the role of the genome to a jazz score that is interpreted and developed through experience and context—and is seldom predictable. Such insights may well temper enthusiasm for genetic testing in populations. However, in parallel to the approaches advocated by HuGE, genetic epidemiology can lead to a more robust understanding of environmental determinants of disease (e.g. dietary factors, occupational exposures, and health-related behaviours) relevant to whole populations (and not simply to genetically susceptible sub-populations).7–10 This approach has recently been referred to as ‘Mendelian randomization’.11–15 Here we begin by briefly reviewing reasons for current concerns about aetiological findings generated by conventional observational epidemiology and then we outline the potential contribution (and limitations) of Mendelian randomization.Box 1 ‘Epidemiology set to get fast-track treatment’‘A consortium of leading European research centres and pharmaceutical companies will this week announce a plan to transform epidemiology by combining it with the new techniques of high-throughput biology. They plan to create a new field of study—genomic epidemiology—by using screening technologies derived from the human genome project … We think it is important to expand classical epidemiology and genetic epidemiology to take it to this high-throughput mode, says Esper Boel, vice-president of biotechnology research at Novo Nordisk. We want to use post-genomic technologies to create a new clinical science, to turn functional genomics into real clinical chemistry.’From: Butler D. Epidemiology set to get fast-track treatment. Nature 2001;414:139. Reprinted with permission.Box 2 SurveillancePopulation frequency of gene variants predisposing to specific diseasesPopulation frequency of morbidity and mortality from such diseasesPopulation frequency and effects of environmental factors known to interact with gene variantsEconomic costs of genetic components of diseasesCoverage, access, and uptake of genetic tests and servicesAetiologyMagnitude of disease risk associated with gene variants in different populationsContribution of gene variants to the overall level of disease in different populationsMagnitude of disease risk associated with gene–gene and gene–environment interactions in different populationsHealth services researchClinical validity and utility of genetic tests in different populationsDeterminants and impact of using genetic tests and services in different populationsAdapted from Khoury MJ, Burke W, Thomson EJ (eds). Genetics and Public Health in the 21st Century. Oxford: Oxford University Press, 2000.},
	number = {1},
	urldate = {2025-12-24},
	journal = {International Journal of Epidemiology},
	author = {Davey Smith, George and Ebrahim, Shah},
	month = feb,
	year = {2003},
	pages = {1--22},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/64B4WVX3/2003-02-01 - Davey Smith and Ebrahim - ‘Mendelian randomization’ can genetic epidemiology contribute to understanding environmental determ.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/4FXXPEJB/dyg070.html:text/html},
}

@article{fedus_switch_2022,
	title = {Switch {Transformers}: {Scaling} to {Trillion} {Parameter} {Models} with {Simple} and {Efficient} {Sparsity}},
	volume = {23},
	issn = {1533-7928},
	shorttitle = {Switch {Transformers}},
	url = {http://jmlr.org/papers/v23/21-0998.html},
	abstract = {In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) models defy this and instead select different parameters for each incoming example. The result is a sparsely-activated model---with an outrageous number of parameters---but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs, and training instability. We address these with the introduction of the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques mitigate the instabilities, and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the "Colossal Clean Crawled Corpus", and achieve a 4x speedup over the T5-XXL model.},
	number = {120},
	urldate = {2025-12-26},
	journal = {Journal of Machine Learning Research},
	author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
	year = {2022},
	pages = {1--39},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/FFUW4RL8/2022 - Fedus et al. - Switch Transformers Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.pdf:application/pdf;Source Code:/Users/meehl.joshua/Zotero/storage/8ZAAQQK2/21-0998.html:text/html},
}

@inproceedings{gu_mamba_2024,
	title = {Mamba: {Linear}-{Time} {Sequence} {Modeling} with {Selective} {State} {Spaces}},
	shorttitle = {Mamba},
	url = {https://openreview.net/forum?id=tEYskw1VY2},
	abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5x higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
	language = {en},
	urldate = {2025-12-23},
	author = {Gu, Albert and Dao, Tri},
	month = aug,
	year = {2024},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/VTUGYKLT/20240826 - Gu and Dao - Mamba Linear-Time Sequence Modeling with Selective State Spaces.pdf:application/pdf},
}
