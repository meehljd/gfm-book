@article{ji_dnabert_2021,
	title = {{DNABERT}: pre-trained {Bidirectional} {Encoder} {Representations} from {Transformers} model for {DNA}-language in genome},
	volume = {37},
	issn = {1367-4803},
	shorttitle = {{DNABERT}},
	url = {https://doi.org/10.1093/bioinformatics/btab083},
	doi = {10.1093/bioinformatics/btab083},
	abstract = {Deciphering the language of non-coding DNA is one of the fundamental problems in genome research. Gene regulatory code is highly complex due to the existence of polysemy and distant semantic relationship, which previous informatics methods often fail to capture especially in data-scarce scenarios.To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, to capture global and transferrable understanding of genomic DNA sequences based on up and downstream nucleotide contexts. We compared DNABERT to the most widely used programs for genome-wide regulatory elements prediction and demonstrate its ease of use, accuracy and efficiency. We show that the single pre-trained transformers model can simultaneously achieve state-of-the-art performance on prediction of promoters, splice sites and transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, DNABERT enables direct visualization of nucleotide-level importance and semantic relationship within input sequences for better interpretability and accurate identification of conserved sequence motifs and functional genetic variant candidates. Finally, we demonstrate that pre-trained DNABERT with human genome can even be readily applied to other organisms with exceptional performance. We anticipate that the pre-trained DNABERT model can be fined tuned to many other sequence analyses tasks.The source code, pretrained and finetuned model for DNABERT are available at GitHub (https://github.com/jerryji1993/DNABERT).Supplementary data are available at Bioinformatics online.},
	number = {15},
	urldate = {2025-08-31},
	journal = {Bioinformatics},
	author = {Ji, Yanrong and Zhou, Zhihan and Liu, Han and Davuluri, Ramana V},
	month = aug,
	year = {2021},
	note = {802 citations (Crossref/DOI) [2025-10-22]},
	pages = {2112--2120},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/VKSTMFPQ/2021-08-09 - Ji et al. - DNABERT pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/UBBEDFL5/btab083.html:text/html},
}


@misc{zhou_dnabert-2_2024,
	title = {{DNABERT}-2: {Efficient} {Foundation} {Model} and {Benchmark} {For} {Multi}-{Species} {Genome}},
	shorttitle = {{DNABERT}-2},
	url = {http://arxiv.org/abs/2306.15006},
	doi = {10.48550/arXiv.2306.15006},
	abstract = {Decoding the linguistic intricacies of the genome is a crucial problem in biology, and pre-trained foundational models such as DNABERT and Nucleotide Transformer have made significant strides in this area. Existing works have largely hinged on k-mer, fixed-length permutations of A, T, C, and G, as the token of the genome language due to its simplicity. However, we argue that the computation and sample inefficiencies introduced by k-mer tokenization are primary obstacles in developing large genome foundational models. We provide conceptual and empirical insights into genome tokenization, building on which we propose to replace k-mer tokenization with Byte Pair Encoding (BPE), a statistics-based data compression algorithm that constructs tokens by iteratively merging the most frequent co-occurring genome segment in the corpus. We demonstrate that BPE not only overcomes the limitations of k-mer tokenization but also benefits from the computational efficiency of non-overlapping tokenization. Based on these insights, we introduce DNABERT-2, a refined genome foundation model that adapts an efficient tokenizer and employs multiple strategies to overcome input length constraints, reduce time and memory expenditure, and enhance model capability. Furthermore, we identify the absence of a comprehensive and standardized benchmark for genome understanding as another significant impediment to fair comparative analysis. In response, we propose the Genome Understanding Evaluation (GUE), a comprehensive multi-species genome classification dataset that amalgamates \$36\$ distinct datasets across \$9\$ tasks, with input lengths ranging from \$70\$ to \$10000\$. Through comprehensive experiments on the GUE benchmark, we demonstrate that DNABERT-2 achieves comparable performance to the state-of-the-art model with \$21 {\textbackslash}times\$ fewer parameters and approximately \$92 {\textbackslash}times\$ less GPU time in pre-training.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Zhou, Zhihan and Ji, Yanrong and Li, Weijian and Dutta, Pratik and Davuluri, Ramana and Liu, Han},
	month = mar,
	year = {2024},
	note = {arXiv:2306.15006 [q-bio]},
	keywords = {Printed, Important, Podcast, Read},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/2KZP5KPQ/2024-03-18 - Zhou et al. - DNABERT-2 Efficient Foundation Model and Benchmark For Multi-Species Genome.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/KUYT79IL/2306.html:text/html},
}


@article{dalla-torre_nucleotide_2023,
	title = {Nucleotide {Transformer}: building and evaluating robust foundation models for human genomics},
	volume = {22},
	copyright = {2024 The Author(s)},
	issn = {1548-7105},
	shorttitle = {Nucleotide {Transformer}},
	url = {https://www.nature.com/articles/s41592-024-02523-z},
	doi = {10.1038/s41592-024-02523-z},
	abstract = {The prediction of molecular phenotypes from DNA sequences remains a longstanding challenge in genomics, often driven by limited annotated data and the inability to transfer learnings between tasks. Here, we present an extensive study of foundation models pre-trained on DNA sequences, named Nucleotide Transformer, ranging from 50 million up to 2.5 billion parameters and integrating information from 3,202 human genomes and 850 genomes from diverse species. These transformer models yield context-specific representations of nucleotide sequences, which allow for accurate predictions even in low-data settings. We show that the developed models can be fine-tuned at low cost to solve a variety of genomics applications. Despite no supervision, the models learned to focus attention on key genomic elements and can be used to improve the prioritization of genetic variants. The training and application of foundational models in genomics provides a widely applicable approach for accurate molecular phenotype prediction from DNA sequence.},
	language = {en},
	number = {2},
	urldate = {2025-04-16},
	journal = {Nature Methods},
	author = {Dalla-Torre, Hugo and Gonzalez, Liam and Mendoza-Revilla, Javier and Lopez Carranza, Nicolas and Grzywaczewski, Adam Henryk and Oteri, Francesco and Dallago, Christian and Trop, Evan and de Almeida, Bernardo P. and Sirelkhatim, Hassan and Richard, Guillaume and Skwark, Marcin and Beguir, Karim and Lopez, Marie and Pierrot, Thomas},
	month = jan,
	year = {2023},
	note = {123 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Printed, Important, Podcast, Read},
	pages = {287--297},
	file = {[Preprint] Dalla-Torre et al. - 2023 - Nucleotide Transformer building and evaluating robust foundation models for human genomics:/Users/meehl.joshua/Zotero/storage/METWGZEF/2023-01-15 - Dalla-Torre et al. - Nucleotide Transformer building and evaluating robust foundation models for human genomics.pdf:application/pdf;Full Text PDF:/Users/meehl.joshua/Zotero/storage/K28P6JFS/2023-01-15 - Dalla-Torre et al. - Nucleotide Transformer building and evaluating robust foundation models for human genomics.pdf:application/pdf},
}


@article{benegas_gpn_2023,
	title = {[{GPN}] {DNA} language models are powerful predictors of genome-wide variant effects},
	volume = {120},
	url = {https://www.pnas.org/doi/10.1073/pnas.2311219120},
	doi = {10.1073/pnas.2311219120},
	abstract = {The expanding catalog of genome-wide association studies (GWAS) provides biological insights across a variety of species, but identifying the causal variants behind these associations remains a significant challenge. Experimental validation is both labor-intensive and costly, highlighting the need for accurate, scalable computational methods to predict the effects of genetic variants across the entire genome. Inspired by recent progress in natural language processing, unsupervised pretraining on large protein sequence databases has proven successful in extracting complex information related to proteins. These models showcase their ability to learn variant effects in coding regions using an unsupervised approach. Expanding on this idea, we here introduce the Genomic Pre-trained Network (GPN), a model designed to learn genome-wide variant effects through unsupervised pretraining on genomic DNA sequences. Our model also successfully learns gene structure and DNA motifs without any supervision. To demonstrate its utility, we train GPN on unaligned reference genomes of Arabidopsis thaliana and seven related species within the Brassicales order and evaluate its ability to predict the functional impact of genetic variants in A. thaliana by utilizing allele frequencies from the 1001 Genomes Project and a comprehensive database of GWAS. Notably, GPN outperforms predictors based on popular conservation scores such as phyloP and phastCons. Our predictions for A. thaliana can be visualized as sequence logos in the UCSC Genome Browser (https://genome.ucsc.edu/s/gbenegas/gpn-arabidopsis). We provide code (https://github.com/songlab-cal/gpn) to train GPN for any given species using its DNA sequence alone, enabling unsupervised prediction of variant effects across the entire genome.},
	number = {44},
	urldate = {2023-11-16},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Benegas, Gonzalo and Batra, Sanjit Singh and Song, Yun S.},
	month = oct,
	year = {2023},
	note = {81 citations (Crossref/DOI) [2025-10-22]
Publisher: Proceedings of the National Academy of Sciences},
	keywords = {Printed, Read},
	pages = {e2311219120},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/YLQDHAPZ/2023-10-31 - Benegas et al. - [GPN] DNA language models are powerful predictors of genome-wide variant effects.pdf:application/pdf},
}


@misc{nguyen_hyenadna_2023,
	title = {{HyenaDNA}: {Long}-{Range} {Genomic} {Sequence} {Modeling} at {Single} {Nucleotide} {Resolution}},
	shorttitle = {{HyenaDNA}},
	url = {http://arxiv.org/abs/2306.15794},
	doi = {10.48550/arXiv.2306.15794},
	abstract = {Genomic (DNA) sequences encode an enormous amount of information for gene regulation and protein synthesis. Similar to natural language models, researchers have proposed foundation models in genomics to learn generalizable features from unlabeled genome data that can then be fine-tuned for downstream tasks such as identifying regulatory elements. Due to the quadratic scaling of attention, previous Transformer-based genomic models have used 512 to 4k tokens as context ({\textless}0.001\% of the human genome), significantly limiting the modeling of long-range interactions in DNA. In addition, these methods rely on tokenizers or fixed k-mers to aggregate meaningful DNA units, losing single nucleotide resolution where subtle genetic variations can completely alter protein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a large language model based on implicit convolutions was shown to match attention in quality while allowing longer context lengths and lower time complexity. Leveraging Hyena's new long-range capabilities, we present HyenaDNA, a genomic foundation model pretrained on the human reference genome with context lengths of up to 1 million tokens at the single nucleotide-level - an up to 500x increase over previous dense attention-based models. HyenaDNA scales sub-quadratically in sequence length (training up to 160x faster than Transformer), uses single nucleotide tokens, and has full global context at each layer. We explore what longer context enables - including the first use of in-context learning in genomics. On fine-tuned benchmarks from the Nucleotide Transformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 18 datasets using a model with orders of magnitude less parameters and pretraining data. On the GenomicBenchmarks, HyenaDNA surpasses SotA on 7 of 8 datasets on average by +10 accuracy points. Code at https://github.com/HazyResearch/hyena-dna.},
	urldate = {2025-07-23},
	publisher = {arXiv},
	author = {Nguyen, Eric and Poli, Michael and Faizi, Marjan and Thomas, Armin and Birch-Sykes, Callum and Wornow, Michael and Patel, Aman and Rabideau, Clayton and Massaroli, Stefano and Bengio, Yoshua and Ermon, Stefano and Baccus, Stephen A. and Ré, Chris},
	month = nov,
	year = {2023},
	note = {arXiv:2306.15794 [cs]},
	keywords = {Printed, Important, Podcast, Read},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/JVEF4BBI/2023-11-14 - Nguyen et al. - HyenaDNA Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/G59U5Z77/2306.html:text/html},
}


@misc{schiff_caduceus_2024,
	title = {Caduceus: {Bi}-{Directional} {Equivariant} {Long}-{Range} {DNA} {Sequence} {Modeling}},
	shorttitle = {Caduceus},
	url = {http://arxiv.org/abs/2403.03234},
	doi = {10.48550/arXiv.2403.03234},
	abstract = {Large-scale sequence modeling has sparked rapid advances that now extend into biology and genomics. However, modeling genomic sequences introduces challenges such as the need to model long-range token interactions, the effects of upstream and downstream regions of the genome, and the reverse complementarity (RC) of DNA. Here, we propose an architecture motivated by these challenges that builds off the long-range Mamba block, and extends it to a BiMamba component that supports bi-directionality, and to a MambaDNA block that additionally supports RC equivariance. We use MambaDNA as the basis of Caduceus, the first family of RC equivariant bi-directional long-range DNA language models, and we introduce pre-training and fine-tuning strategies that yield Caduceus DNA foundation models. Caduceus outperforms previous long-range models on downstream benchmarks; on a challenging long-range variant effect prediction task, Caduceus exceeds the performance of 10x larger models that do not leverage bi-directionality or equivariance.},
	urldate = {2025-07-23},
	publisher = {arXiv},
	author = {Schiff, Yair and Kao, Chia-Hsiang and Gokaslan, Aaron and Dao, Tri and Gu, Albert and Kuleshov, Volodymyr},
	month = jun,
	year = {2024},
	note = {arXiv:2403.03234 [q-bio]},
	keywords = {Printed, Podcast, Read},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/PN2AQVMU/2024-06-05 - Schiff et al. - Caduceus Bi-Directional Equivariant Long-Range DNA Sequence Modeling.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/YIPW7H9E/2403.html:text/html},
}


@misc{brixi_evo_2025,
	title = {[{Evo} 2] {Genome} modeling and design across all domains of life with {Evo} 2},
	copyright = {© 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2025.02.18.638918v1},
	doi = {10.1101/2025.02.18.638918},
	abstract = {All of life encodes information with DNA. While tools for sequencing, synthesis, and editing of genomic code have transformed biological research, intelligently composing new biological systems would also require a deep understanding of the immense complexity encoded by genomes. We introduce Evo 2, a biological foundation model trained on 9.3 trillion DNA base pairs from a highly curated genomic atlas spanning all domains of life. We train Evo 2 with 7B and 40B parameters to have an unprecedented 1 million token context window with single-nucleotide resolution. Evo 2 learns from DNA sequence alone to accurately predict the functional impacts of genetic variation—from noncoding pathogenic mutations to clinically significant BRCA1 variants—without task-specific finetuning. Applying mechanistic interpretability analyses, we reveal that Evo 2 autonomously learns a breadth of biological features, including exon–intron boundaries, transcription factor binding sites, protein structural elements, and prophage genomic regions. Beyond its predictive capabilities, Evo 2 generates mitochondrial, prokaryotic, and eukaryotic sequences at genome scale with greater naturalness and coherence than previous methods. Guiding Evo 2 via inference-time search enables controllable generation of epigenomic structure, for which we demonstrate the first inference-time scaling results in biology. We make Evo 2 fully open, including model parameters, training code, inference code, and the OpenGenome2 dataset, to accelerate the exploration and design of biological complexity.},
	language = {en},
	urldate = {2025-02-26},
	publisher = {bioRxiv},
	author = {Brixi, Garyk and Durrant, Matthew G. and Ku, Jerome and Poli, Michael and Brockman, Greg and Chang, Daniel and Gonzalez, Gabriel A. and King, Samuel H. and Li, David B. and Merchant, Aditi T. and Naghipourfar, Mohsen and Nguyen, Eric and Ricci-Tam, Chiara and Romero, David W. and Sun, Gwanggyu and Taghibakshi, Ali and Vorontsov, Anton and Yang, Brandon and Deng, Myra and Gorton, Liv and Nguyen, Nam and Wang, Nicholas K. and Adams, Etowah and Baccus, Stephen A. and Dillmann, Steven and Ermon, Stefano and Guo, Daniel and Ilango, Rajesh and Janik, Ken and Lu, Amy X. and Mehta, Reshma and Mofrad, Mohammad R. K. and Ng, Madelena Y. and Pannu, Jaspreet and Ré, Christopher and Schmok, Jonathan C. and John, John St and Sullivan, Jeremy and Zhu, Kevin and Zynda, Greg and Balsam, Daniel and Collison, Patrick and Costa, Anthony B. and Hernandez-Boussard, Tina and Ho, Eric and Liu, Ming-Yu and McGrath, Thomas and Powell, Kimberly and Burke, Dave P. and Goodarzi, Hani and Hsu, Patrick D. and Hie, Brian L.},
	month = feb,
	year = {2025},
	note = {71 citations (Crossref/DOI) [2025-10-22]
Pages: 2025.02.18.638918
Section: New Results},
	keywords = {Printed, Important, Podcast, Read},
	file = {2025-02-21 - Brixi et al. - [Evo 2] Genome modeling and design across all domains of life with Evo 2:/Users/meehl.joshua/Zotero/storage/BSMA7JT7/2025-02-21 - Brixi et al. - [Evo 2] Genome modeling and design across all domains of life with Evo 2.pdf:application/pdf},
}


@misc{marin_bend_2024,
	title = {{BEND}: {Benchmarking} {DNA} {Language} {Models} on biologically meaningful tasks},
	shorttitle = {{BEND}},
	url = {http://arxiv.org/abs/2311.12570},
	doi = {10.48550/arXiv.2311.12570},
	abstract = {The genome sequence contains the blueprint for governing cellular processes. While the availability of genomes has vastly increased over the last decades, experimental annotation of the various functional, non-coding and regulatory elements encoded in the DNA sequence remains both expensive and challenging. This has sparked interest in unsupervised language modeling of genomic DNA, a paradigm that has seen great success for protein sequence data. Although various DNA language models have been proposed, evaluation tasks often differ between individual works, and might not fully recapitulate the fundamental challenges of genome annotation, including the length, scale and sparsity of the data. In this study, we introduce BEND, a Benchmark for DNA language models, featuring a collection of realistic and biologically meaningful downstream tasks defined on the human genome. We find that embeddings from current DNA LMs can approach performance of expert methods on some tasks, but only capture limited information about long-range features. BEND is available at https://github.com/frederikkemarin/BEND.},
	language = {en},
	urldate = {2025-06-30},
	publisher = {arXiv},
	author = {Marin, Frederikke Isa and Teufel, Felix and Horlacher, Marc and Madsen, Dennis and Pultz, Dennis and Winther, Ole and Boomsma, Wouter},
	month = apr,
	year = {2024},
	note = {arXiv:2311.12570 [q-bio]},
	keywords = {Printed, Read},
	file = {PDF:/Users/meehl.joshua/Zotero/storage/PAK9JYZT/2024-04-09 - Marin et al. - BEND Benchmarking DNA Language Models on biologically meaningful tasks.pdf:application/pdf},
}


@article{cheng_dnalongbench_2024,
	title = {{DNALONGBENCH}: {A} {Benchmark} {Suite} {For} {Long}-{Range} {DNA} {Prediction} {Tasks}},
	shorttitle = {{DNALONGBENCH}},
	url = {https://openreview.net/forum?id=opv67PpqLS},
	abstract = {Modeling long-range DNA dependencies is crucial for understanding genome structure and function across a wide range of biological contexts in health and disease. However, effectively capturing the extensive long-range dependencies between DNA sequences, spanning millions of base pairs as seen in tasks such as three-dimensional (3D) chromatin folding, remains a significant challenge. Additionally, a comprehensive benchmark suite for evaluating tasks reliant on long-range dependencies is notably absent. To address this gap, we introduce DNALONGBENCH, a benchmark dataset spanning five important genomics tasks that consider long-range dependencies up to 1 million base pairs: enhancer-target gene interaction, expression quantitative trait loci, 3D genome organization, regulatory sequence activity, and transcription initiation signal. To comprehensively assess DNALONGBENCH, we evaluate the performance of five baseline methods: a task-specific expert model, a convolutional neural network (CNN)-based model, and three fine-tuned DNA foundation models -- HyenaDNA, Caduceus-Ph and Caduceus-PS. We envision DNALONGBENCH having the potential to become a standardized resource that facilitates comprehensive comparisons and rigorous evaluations of emerging DNA sequence-based deep learning models that consider long-range dependencies.},
	language = {en},
	urldate = {2025-07-01},
	author = {Cheng, Wenduo and Song, Zhenqiao and Zhang, Yang and Wang, Shike and Wang, Danqing and Yang, Muyu and Li, Lei and Ma, Jian},
	month = oct,
	year = {2024},
	keywords = {Printed},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/FBTTSJEW/2024-10-04 - Cheng et al. - DNALONGBENCH A Benchmark Suite For Long-Range DNA Prediction Tasks.pdf:application/pdf},
}


@article{manzo_comparative_2025,
	title = {Comparative {Analysis} of {Deep} {Learning} {Models} for {Predicting} {Causative} {Regulatory} {Variants}},
	issn = {2692-8205},
	doi = {10.1101/2025.05.19.654920},
	abstract = {MOTIVATION: Genome-wide association studies (GWAS) have identified numerous noncoding variants associated with complex human diseases, disorders, and traits. However, resolving the uncertainty between GWAS association and causality remains a significant challenge. The small subset of noncoding GWAS variants with causative effects on gene regulatory elements can only be detected through accurate methods that assess the impact of DNA sequence variation on gene regulatory activity. Deep learning models, such as those based on Convolutional Neural Networks (CNNs) and transformers, have gained prominence in predicting the regulatory effects of genetic variants, particularly in enhancers, by learning patterns from genomic and epigenomic data. Despite their potential, selecting the most suitable model is hindered by the lack of standardized benchmarks, consistent training conditions, and performance evaluation criteria in existing reviews.
RESULTS: This study evaluates state-of-the-art deep learning models for predicting the effects of genetic variants on enhancer activity using nine datasets stemming from MPRA, raQTL, and eQTL experiments, profiling the regulatory impact of 54,859 SNPs across four human cell lines. The results reveal that CNN models, such as TREDNet and SEI, consistently outperform other architectures in predicting the regulatory impact of single-nucleotide polymorphisms (SNPs). However, hybrid CNN-transformer models, such as Borzoi, display superior performance in identifying causal SNPs within a linkage disequilibrium block. While fine-tuning enhances the performance of transformer-based models, it remains insufficient to surpass CNN and hybrid models when evaluated under optimized conditions.},
	language = {eng},
	journal = {bioRxiv: The Preprint Server for Biology},
	author = {Manzo, Gaetano and Borkowski, Kathryn and Ovcharenko, Ivan},
	month = jun,
	year = {2025},
	pmid = {40568119},
	pmcid = {PMC12190767},
	note = {0 citations (Crossref/DOI) [2025-10-22]},
	keywords = {Printed, Podcast, Recall},
	pages = {2025.05.19.654920},
	file = {2025-06-11 - Manzo et al. - Comparative Analysis of Deep Learning Models for Predicting Causative Regulatory Variants:/Users/meehl.joshua/Zotero/storage/83FB7EZC/2025-06-11 - Manzo et al. - Comparative Analysis of Deep Learning Models for Predicting Causative Regulatory Variants.pdf:application/pdf},
}


@misc{liu_life-code_2025,
	title = {Life-{Code}: {Central} {Dogma} {Modeling} with {Multi}-{Omics} {Sequence} {Unification}},
	shorttitle = {Life-{Code}},
	url = {http://arxiv.org/abs/2502.07299},
	doi = {10.48550/arXiv.2502.07299},
	abstract = {The interactions between DNA, RNA, and proteins are fundamental to biological processes, as illustrated by the central dogma of molecular biology. Although modern biological pre-trained models have achieved great success in analyzing these macromolecules individually, their interconnected nature remains underexplored. This paper follows the guidance of the central dogma to redesign both the data and model pipeline and offers a comprehensive framework, Life-Code, that spans different biological functions. As for data flow, we propose a unified pipeline to integrate multi-omics data by reverse-transcribing RNA and reverse-translating amino acids into nucleotide-based sequences. As for the model, we design a codon tokenizer and a hybrid long-sequence architecture to encode the interactions between coding and non-coding regions through masked modeling pre-training. To model the translation and folding process with coding sequences, Life-Code learns protein structures of the corresponding amino acids by knowledge distillation from off-the-shelf protein language models. Such designs enable Life-Code to capture complex interactions within genetic sequences, providing a more comprehensive understanding of multi-omics with the central dogma. Extensive experiments show that Life-Code achieves state-of-the-art results on various tasks across three omics, highlighting its potential for advancing multi-omics analysis and interpretation.},
	urldate = {2025-08-07},
	publisher = {arXiv},
	author = {Liu, Zicheng and Li, Siyuan and Chen, Zhiyuan and Wu, Fang and Yu, Chang and Yang, Qirong and Guo, Yucheng and Yang, Yujie and Zhang, Xiaoming and Li, Stan Z.},
	month = jun,
	year = {2025},
	note = {arXiv:2502.07299 [cs]},
	keywords = {Printed, Podcast},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/LFXSCF3K/2025-06-15 - Liu et al. - Life-Code Central Dogma Modeling with Multi-Omics Sequence Unification.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/P626N5L8/2502.html:text/html},
}


@misc{medvedev_biotoken_2025,
	title = {{BioToken} and {BioFM} – {Biologically}-{Informed} {Tokenization} {Enables} {Accurate} and {Efficient} {Genomic} {Foundation} {Models}},
	copyright = {© 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2025.03.27.645711v1},
	doi = {10.1101/2025.03.27.645711},
	abstract = {Genomic variation underlies human phenotypic diversity, disease susceptibility, and evolutionary adaptation. Although large-scale genomic sequencing has transformed our ability to map genetic variation, accurately modeling and interpreting this data remains challenging due to fundamental limitations in existing genomic foundation models (GFMs). Current genomic models typically treat DNA simplistically as linear textual sequences, overlooking critical biological context, such as genomic structural annotations, regulatory elements, and functional contexts central to genomic interpretation. As a result, these models are prone to positional memorization of common sequences, severely limiting their generalization to biologically meaningful tasks. Here, we introduce BioToken, a modular and extendable tokenization framework designed to encode genomic variants and biologically relevant structural annotations directly into genomic representations. By utilizing intrinsic inductive biases, BioToken facilitates meaningful representation learning and generalization across diverse molecular phenotypes, such as gene expression, alternative splicing, and variant pathogenicity prediction. Built on BioToken, our genomic foundation model, BioFM, achieves competitive or superior results relative to specialized models (e.g., Enformer, SpliceTransformer) across a comprehensive suite of genomic benchmarks, including noncoding pathogenicity, expression modulation, sQTL prediction, and long-range genomic interactions. Notably, BioFM achieves state-of-the-art performance with significantly fewer parameters (265M), substantially reducing training costs and computational requirements. Our findings high-light the substantial advantages of integrating biologically-informed inductive biases into genomic foundation modeling, providing a robust and accessible path forward in genomics. We provide our code and model checkpoints to support further research in this direction.},
	language = {en},
	urldate = {2025-05-27},
	publisher = {bioRxiv},
	author = {Medvedev, Aleksandr and Viswanathan, Karthik and Kanithi, Praveenkumar and Vishniakov, Kirill and Munjal, Prateek and Christophe, Clément and Pimentel, Marco AF and Rajan, Ronnie and Khan, Shadab},
	month = apr,
	year = {2025},
	note = {1 citations (Crossref/DOI) [2025-10-22]
Pages: 2025.03.27.645711
Section: New Results},
	keywords = {Printed, Podcast},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/XZBQLGZE/2025-04-01 - Medvedev et al. - BioToken and BioFM – Biologically-Informed Tokenization Enables Accurate and Efficient Genomic Found.pdf:application/pdf},
}
