@article{avsec_enformer_2021,
	title = {[{Enformer}] {Effective} gene expression prediction from sequence by integrating long-range interactions},
	volume = {18},
	url = {https://consensus.app/papers/effective-gene-expression-prediction-from-sequence-by-avsec-agarwal/6afb944129f35bad916e6f4a889c07cb/},
	doi = {10.1038/s41592-021-01252-x},
	abstract = {The next phase of genome biology research requires understanding how DNA sequence encodes phenotypes, from the molecular to organismal levels. How noncoding DNA determines gene expression in different cell types is a major unsolved problem, and critical downstream applications in human genetics depend on improved solutions. Here, we report substantially improved gene expression prediction accuracy from DNA sequence through the use of a new deep learning architecture called Enformer that is able to integrate long-range interactions (up to 100 kb away) in the genome. This improvement yielded more accurate variant effect predictions on gene expression for both natural genetic variants and saturation mutagenesis measured by massively parallel reporter assays. Notably, Enformer outperformed the best team on the critical assessment of genome interpretation (CAGI5) challenge for noncoding variant interpretation with no additional training. Furthermore, Enformer learned to predict promoter-enhancer interactions directly from DNA sequence competitively with methods that take direct experimental data as input. We expect that these advances will enable more effective fine-mapping of growing human disease associations to cell-type-specific gene regulatory mechanisms and provide a framework to interpret cis-regulatory evolution. To foster these downstream applications, we have made the pre-trained Enformer model openly available, and provide pre-computed effect predictions for all common variants in the 1000 Genomes dataset. One-sentence summary Improved noncoding variant effect prediction and candidate enhancer prioritization from a more accurate sequence to expression model driven by extended long-range interaction modelling.},
	urldate = {2024-12-25},
	journal = {Nature Methods},
	author = {Avsec, Žiga and Agarwal, Vikram and Visentin, D. and Ledsam, J. and Grabska-Barwinska, A. and Taylor, Kyle R. and Assael, Yannis and Jumper, J. and Kohli, Pushmeet and Kelley, David R.},
	month = oct,
	year = {2021},
	note = {846 citations (Semantic Scholar/DOI) [2025-10-22]
875 citations (Crossref/DOI) [2025-10-22]},
	keywords = {Podcast, Printed, Read},
	pages = {1196--1203},
	file = {41592_2021_1252_MOESM1_ESM:/Users/meehl.joshua/Zotero/storage/7PUC6VKT/41592_2021_1252_MOESM1_ESM.pdf:application/pdf;Full Text:/Users/meehl.joshua/Zotero/storage/WTX3AUV5/2021-10-04 - Avsec et al. - [Enformer] Effective gene expression prediction from sequence by integrating long-range interactions.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/JWYN5S8L/6afb944129f35bad916e6f4a889c07cb.html:text/html},
}

@article{ben-david_theory_2010,
	title = {A theory of learning from different domains},
	volume = {79},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-009-5152-4},
	doi = {10.1007/s10994-009-5152-4},
	abstract = {Discriminative learning methods for classification perform well when training and test data are drawn from the same distribution. Often, however, we have plentiful labeled training data from a source domain but wish to learn a classifier which performs well on a target domain with a different distribution and little or no labeled training data. In this work we investigate two questions. First, under what conditions can a classifier trained from source data be expected to perform well on target data? Second, given a small amount of labeled target data, how should we combine it during training with the large amount of labeled source data to achieve the lowest target error at test time?},
	language = {en},
	number = {1},
	urldate = {2025-12-23},
	journal = {Machine Learning},
	author = {Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
	month = may,
	year = {2010},
	pages = {151--175},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/U2NU9HM4/2010-05-01 - Ben-David et al. - A theory of learning from different domains.pdf:application/pdf},
}

@article{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html?utm_source=transaction&utm_medium=email&utm_campaign=linkedin_newsletter},
	language = {en},
	urldate = {2025-12-23},
	journal = {Advances in Neural Information Processing Systems},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D. and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year = {2020},
	pages = {1877--1901},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/85KZUPPV/2020 - Brown et al. - Language Models are Few-Shot Learners.pdf:application/pdf},
}

@article{dalla-torre_nucleotide_2023,
	title = {Nucleotide {Transformer}: building and evaluating robust foundation models for human genomics},
	volume = {22},
	copyright = {2024 The Author(s)},
	issn = {1548-7105},
	shorttitle = {Nucleotide {Transformer}},
	url = {https://www.nature.com/articles/s41592-024-02523-z},
	doi = {10.1038/s41592-024-02523-z},
	abstract = {The prediction of molecular phenotypes from DNA sequences remains a longstanding challenge in genomics, often driven by limited annotated data and the inability to transfer learnings between tasks. Here, we present an extensive study of foundation models pre-trained on DNA sequences, named Nucleotide Transformer, ranging from 50 million up to 2.5 billion parameters and integrating information from 3,202 human genomes and 850 genomes from diverse species. These transformer models yield context-specific representations of nucleotide sequences, which allow for accurate predictions even in low-data settings. We show that the developed models can be fine-tuned at low cost to solve a variety of genomics applications. Despite no supervision, the models learned to focus attention on key genomic elements and can be used to improve the prioritization of genetic variants. The training and application of foundational models in genomics provides a widely applicable approach for accurate molecular phenotype prediction from DNA sequence.},
	language = {en},
	number = {2},
	urldate = {2025-04-16},
	journal = {Nature Methods},
	author = {Dalla-Torre, Hugo and Gonzalez, Liam and Mendoza-Revilla, Javier and Lopez Carranza, Nicolas and Grzywaczewski, Adam Henryk and Oteri, Francesco and Dallago, Christian and Trop, Evan and de Almeida, Bernardo P. and Sirelkhatim, Hassan and Richard, Guillaume and Skwark, Marcin and Beguir, Karim and Lopez, Marie and Pierrot, Thomas},
	month = jan,
	year = {2023},
	note = {123 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Important, Podcast, Printed, Read},
	pages = {287--297},
	file = {[Preprint] Dalla-Torre et al. - 2023 - Nucleotide Transformer building and evaluating robust foundation models for human genomics:/Users/meehl.joshua/Zotero/storage/METWGZEF/2023-01-15 - Dalla-Torre et al. - Nucleotide Transformer building and evaluating robust foundation models for human genomics.pdf:application/pdf;Full Text PDF:/Users/meehl.joshua/Zotero/storage/K28P6JFS/2023-01-15 - Dalla-Torre et al. - Nucleotide Transformer building and evaluating robust foundation models for human genomics.pdf:application/pdf},
}

@inproceedings{finn_model-agnostic_2017,
	title = {Model-{Agnostic} {Meta}-{Learning} for {Fast} {Adaptation} of {Deep} {Networks}},
	url = {https://proceedings.mlr.press/v70/finn17a.html},
	abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
	language = {en},
	urldate = {2025-12-23},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {1126--1135},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/VD4VDWDQ/2017-07-17 - Finn et al. - Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.pdf:application/pdf},
}

@article{gaedigk_pharmacogene_2018,
	title = {The {Pharmacogene} {Variation} ({PharmVar}) {Consortium}: {Incorporation} of the {Human} {Cytochrome} {P450} ({CYP}) {Allele} {Nomenclature} {Database}},
	volume = {103},
	copyright = {© 2017 The Authors Clinical Pharmacology \& Therapeutics published by Wiley Periodicals, Inc. on behalf of American Society for Clinical Pharmacology and Therapeutics},
	issn = {1532-6535},
	shorttitle = {The {Pharmacogene} {Variation} ({PharmVar}) {Consortium}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpt.910},
	doi = {10.1002/cpt.910},
	abstract = {The Human Cytochrome P450 (CYP) Allele Nomenclature Database, a critical resource for the pharmacogenetics and genomics communities, has transitioned to the Pharmacogene Variation (PharmVar) Consortium. In this report we provide a summary of the current database, provide an overview of the PharmVar consortium, and highlight the PharmVar database which will serve as the new home for pharmacogene nomenclature.},
	language = {en},
	number = {3},
	urldate = {2025-12-23},
	journal = {Clinical Pharmacology \& Therapeutics},
	author = {Gaedigk, Andrea and Ingelman-Sundberg, Magnus and Miller, Neil A. and Leeder, J. Steven and Whirl-Carrillo, Michelle and Klein, Teri E. and Committee, the PharmVar Steering},
	year = {2018},
	note = {\_eprint: https://ascpt.onlinelibrary.wiley.com/doi/pdf/10.1002/cpt.910},
	pages = {399--401},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/4A4XCT3M/2018 - Gaedigk et al. - The Pharmacogene Variation (PharmVar) Consortium Incorporation of the Human Cytochrome P450 (CYP) A.pdf:application/pdf},
}

@article{ganin_domain-adversarial_2016,
	title = {Domain-{Adversarial} {Training} of {Neural} {Networks}},
	volume = {17},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v17/15-239.html},
	abstract = {We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains.

The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages.

We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.},
	number = {59},
	urldate = {2025-12-23},
	journal = {Journal of Machine Learning Research},
	author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, François and March, Mario and Lempitsky, Victor},
	year = {2016},
	pages = {1--35},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/KMNA85PE/2016 - Ganin et al. - Domain-Adversarial Training of Neural Networks.pdf:application/pdf},
}

@inproceedings{houlsby_parameter-efficient_2019,
	title = {Parameter-{Efficient} {Transfer} {Learning} for {NLP}},
	url = {https://proceedings.mlr.press/v97/houlsby19a.html},
	abstract = {Fine-tuning large pretrained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter’s effectiveness, we transfer the recently proposed BERT Transformer model to 262626 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.80.80.8\% of the performance of full fine-tuning, adding only 3.63.63.6\% parameters per task. By contrast, fine-tuning trains 100100100\% of the parameters per task.},
	language = {en},
	urldate = {2025-12-23},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and Laroussilhe, Quentin De and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {2790--2799},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/9FX68CY3/2019-05-24 - Houlsby et al. - Parameter-Efficient Transfer Learning for NLP.pdf:application/pdf;Supplementary PDF:/Users/meehl.joshua/Zotero/storage/KXBWDLHS/2019-05-24 - Houlsby et al. - Parameter-Efficient Transfer Learning for NLP.pdf:application/pdf},
}

@misc{howard_universal_2018,
	title = {Universal {Language} {Model} {Fine}-tuning for {Text} {Classification}},
	url = {http://arxiv.org/abs/1801.06146},
	doi = {10.48550/arXiv.1801.06146},
	abstract = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24\% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.},
	urldate = {2025-12-23},
	publisher = {arXiv},
	author = {Howard, Jeremy and Ruder, Sebastian},
	month = may,
	year = {2018},
	note = {arXiv:1801.06146 [cs]},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/AQ2ANKMV/2018-05-23 - Howard and Ruder - Universal Language Model Fine-tuning for Text Classification.pdf:application/pdf},
}

@misc{hu_lora_2021,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{LoRA}},
	url = {http://arxiv.org/abs/2106.09685},
	doi = {10.48550/arXiv.2106.09685},
	abstract = {The dominant paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, conventional fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example, deploying many independent instances of fine-tuned models, each with 175B parameters, is extremely expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. For GPT-3, LoRA can reduce the number of trainable parameters by 10,000 times and the computation hardware requirement by 3 times compared to full fine-tuning. LoRA performs on-par or better than fine-tuning in model quality on both GPT-3 and GPT-2, despite having fewer trainable parameters, a higher training throughput, and no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptations, which sheds light on the efficacy of LoRA. We release our implementation in GPT-2 at https://github.com/microsoft/LoRA .},
	urldate = {2025-12-23},
	publisher = {arXiv},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Chen, Weizhu},
	month = jun,
	year = {2021},
	note = {arXiv:2106.09685 [cs]
version: 1},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/IPQVFEH4/2021-06-17 - Hu et al. - LoRA Low-Rank Adaptation of Large Language Models.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/XIGFMVKZ/2106.html:text/html},
}

@inproceedings{jawahar_what_2019,
	address = {Florence, Italy},
	title = {What does {BERT} learn about the structure of language?},
	url = {https://inria.hal.science/hal-02131630},
	abstract = {BERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks. This result indicates the possibility that BERT networks capture structural information about language. In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT. We first show that BERT's phrasal representation captures phrase-level information in the lower layers. We also show that BERT's intermediate layers encode a rich hierarchy of linguistic information, with surface features at the bottom, syntactic features in the middle and semantic features at the top. BERT turns out to require deeper layers when long-distance dependency information is required, e.g.{\textasciitilde}to track subject-verb agreement. Finally, we show that BERT representations capture linguistic information in a compositional way that mimics classical, tree-like structures.},
	urldate = {2025-12-23},
	booktitle = {{ACL} 2019 - 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	author = {Jawahar, Ganesh and Sagot, Benoît and Seddah, Djamé},
	month = jul,
	year = {2019},
	file = {HAL PDF Full Text:/Users/meehl.joshua/Zotero/storage/IZTP3GVQ/2019-07 - Jawahar et al. - What does BERT learn about the structure of language.pdf:application/pdf},
}

@article{ji_dnabert_2021,
	title = {{DNABERT}: pre-trained {Bidirectional} {Encoder} {Representations} from {Transformers} model for {DNA}-language in genome},
	volume = {37},
	issn = {1367-4803},
	shorttitle = {{DNABERT}},
	url = {https://doi.org/10.1093/bioinformatics/btab083},
	doi = {10.1093/bioinformatics/btab083},
	abstract = {Deciphering the language of non-coding DNA is one of the fundamental problems in genome research. Gene regulatory code is highly complex due to the existence of polysemy and distant semantic relationship, which previous informatics methods often fail to capture especially in data-scarce scenarios.To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, to capture global and transferrable understanding of genomic DNA sequences based on up and downstream nucleotide contexts. We compared DNABERT to the most widely used programs for genome-wide regulatory elements prediction and demonstrate its ease of use, accuracy and efficiency. We show that the single pre-trained transformers model can simultaneously achieve state-of-the-art performance on prediction of promoters, splice sites and transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, DNABERT enables direct visualization of nucleotide-level importance and semantic relationship within input sequences for better interpretability and accurate identification of conserved sequence motifs and functional genetic variant candidates. Finally, we demonstrate that pre-trained DNABERT with human genome can even be readily applied to other organisms with exceptional performance. We anticipate that the pre-trained DNABERT model can be fined tuned to many other sequence analyses tasks.The source code, pretrained and finetuned model for DNABERT are available at GitHub (https://github.com/jerryji1993/DNABERT).Supplementary data are available at Bioinformatics online.},
	number = {15},
	urldate = {2025-08-31},
	journal = {Bioinformatics},
	author = {Ji, Yanrong and Zhou, Zhihan and Liu, Han and Davuluri, Ramana V},
	month = aug,
	year = {2021},
	note = {802 citations (Crossref/DOI) [2025-10-22]},
	pages = {2112--2120},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/VKSTMFPQ/2021-08-09 - Ji et al. - DNABERT pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/UBBEDFL5/btab083.html:text/html},
}

@article{kelley_cross-species_2020,
	title = {[{Basenji2}] {Cross}-species regulatory sequence activity prediction},
	volume = {16},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008050},
	doi = {10.1371/journal.pcbi.1008050},
	abstract = {Machine learning algorithms trained to predict the regulatory activity of nucleic acid sequences have revealed principles of gene regulation and guided genetic variation analysis. While the human genome has been extensively annotated and studied, model organisms have been less explored. Model organism genomes offer both additional training sequences and unique annotations describing tissue and cell states unavailable in humans. Here, we develop a strategy to train deep convolutional neural networks simultaneously on multiple genomes and apply it to learn sequence predictors for large compendia of human and mouse data. Training on both genomes improves gene expression prediction accuracy on held out and variant sequences. We further demonstrate a novel and powerful approach to apply mouse regulatory models to analyze human genetic variants associated with molecular phenotypes and disease. Together these techniques unleash thousands of non-human epigenetic and transcriptional profiles toward more effective investigation of how gene regulation affects human disease.},
	language = {en},
	number = {7},
	urldate = {2025-12-15},
	journal = {PLOS Computational Biology},
	author = {Kelley, David R.},
	month = jul,
	year = {2020},
	note = {Publisher: Public Library of Science},
	pages = {e1008050},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/PRY99MKS/Jul 20, 2020 - Kelley - Cross-species regulatory sequence activity prediction.pdf:application/pdf},
}

@article{kircher_general_2014,
	title = {A general framework for estimating the relative pathogenicity of human genetic variants},
	volume = {46},
	issn = {1546-1718},
	doi = {10.1038/ng.2892},
	abstract = {Current methods for annotating and interpreting human genetic variation tend to exploit a single information type (for example, conservation) and/or are restricted in scope (for example, to missense changes). Here we describe Combined Annotation-Dependent Depletion (CADD), a method for objectively integrating many diverse annotations into a single measure (C score) for each variant. We implement CADD as a support vector machine trained to differentiate 14.7 million high-frequency human-derived alleles from 14.7 million simulated variants. We precompute C scores for all 8.6 billion possible human single-nucleotide variants and enable scoring of short insertions-deletions. C scores correlate with allelic diversity, annotations of functionality, pathogenicity, disease severity, experimentally measured regulatory effects and complex trait associations, and they highly rank known pathogenic variants within individual genomes. The ability of CADD to prioritize functional, deleterious and pathogenic variants across many functional categories, effect sizes and genetic architectures is unmatched by any current single-annotation method.},
	language = {eng},
	number = {3},
	journal = {Nature Genetics},
	author = {Kircher, Martin and Witten, Daniela M. and Jain, Preti and O'Roak, Brian J. and Cooper, Gregory M. and Shendure, Jay},
	month = feb,
	year = {2014},
	pmid = {24487276},
	pmcid = {PMC3992975},
	note = {5494 citations (Crossref/DOI) [2025-10-22]},
	pages = {310--315},
}

@article{landrum_clinvar_2018,
	title = {{ClinVar}: improving access to variant interpretations and supporting evidence},
	volume = {46},
	issn = {0305-1048},
	shorttitle = {{ClinVar}},
	url = {https://doi.org/10.1093/nar/gkx1153},
	doi = {10.1093/nar/gkx1153},
	abstract = {ClinVar (https://www.ncbi.nlm.nih.gov/clinvar/) is a freely available, public archive of human genetic variants and interpretations of their significance to disease, maintained at the National Institutes of Health. Interpretations of the clinical significance of variants are submitted by clinical testing laboratories, research laboratories, expert panels and other groups. ClinVar aggregates data by variant-disease pairs, and by variant (or set of variants). Data aggregated by variant are accessible on the website, in an improved set of variant call format files and as a new comprehensive XML report. ClinVar recently started accepting submissions that are focused primarily on providing phenotypic information for individuals who have had genetic testing. Submissions may come from clinical providers providing their own interpretation of the variant (‘provider interpretation’) or from groups such as patient registries that primarily provide phenotypic information from patients (‘phenotyping only’). ClinVar continues to make improvements to its search and retrieval functions. Several new fields are now indexed for more precise searching, and filters allow the user to narrow down a large set of search results.},
	number = {D1},
	urldate = {2025-12-06},
	journal = {Nucleic Acids Research},
	author = {Landrum, Melissa J and Lee, Jennifer M and Benson, Mark and Brown, Garth R and Chao, Chen and Chitipiralla, Shanmuga and Gu, Baoshan and Hart, Jennifer and Hoffman, Douglas and Jang, Wonhee and Karapetyan, Karen and Katz, Kenneth and Liu, Chunlei and Maddipatla, Zenith and Malheiro, Adriana and McDaniel, Kurt and Ovetsky, Michael and Riley, George and Zhou, George and Holmes, J Bradley and Kattman, Brandi L and Maglott, Donna R},
	month = jan,
	year = {2018},
	pages = {D1062--D1067},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/PBKZHYPQ/2018-01-04 - Landrum et al. - ClinVar improving access to variant interpretations and supporting evidence.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/F7C3YBZY/gkx1153.html:text/html},
}

@misc{li_prefix-tuning_2021,
	title = {Prefix-{Tuning}: {Optimizing} {Continuous} {Prompts} for {Generation}},
	shorttitle = {Prefix-{Tuning}},
	url = {http://arxiv.org/abs/2101.00190},
	doi = {10.48550/arXiv.2101.00190},
	abstract = {Fine-tuning is the de facto way to leverage large pretrained language models to perform downstream tasks. However, it modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen, but optimizes a small continuous task-specific vector (called the prefix). Prefix-tuning draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were "virtual tokens". We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We find that by learning only 0.1{\textbackslash}\% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics unseen during training.},
	urldate = {2025-12-23},
	publisher = {arXiv},
	author = {Li, Xiang Lisa and Liang, Percy},
	month = jan,
	year = {2021},
	note = {arXiv:2101.00190 [cs]},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/7DXCRR8Q/2021-01-01 - Li and Liang - Prefix-Tuning Optimizing Continuous Prompts for Generation.pdf:application/pdf},
}

@incollection{mccloskey_catastrophic_1989,
	title = {Catastrophic {Interference} in {Connectionist} {Networks}: {The} {Sequential} {Learning} {Problem}},
	volume = {24},
	shorttitle = {Catastrophic {Interference} in {Connectionist} {Networks}},
	url = {https://www.sciencedirect.com:5037/science/chapter/bookseries/pii/S0079742108605368},
	abstract = {Connectionist networks in which information is stored in weights on connections among simple processing units have attracted considerable interest in …},
	language = {en-US},
	urldate = {2025-12-23},
	booktitle = {Psychology of {Learning} and {Motivation}},
	publisher = {Academic Press},
	author = {McCloskey, Michael and Cohen, Neal},
	month = jan,
	year = {1989},
	doi = {10.1016/S0079-7421(08)60536-8},
	note = {ISSN: 0079-7421},
	pages = {109--165},
	file = {PDF:/Users/meehl.joshua/Zotero/storage/HZD2R8K6/19890101 - McCloskey and Cohen - Catastrophic Interference in Connectionist Networks The Sequential Learning Problem.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/NKI5SMHV/S0079742108605368.html:text/html},
}

@misc{meier_esm-1v_2021,
	title = {[{ESM}-1v] {Language} models enable zero-shot prediction of the effects of mutations on protein function},
	copyright = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2021.07.09.450648v1},
	doi = {10.1101/2021.07.09.450648},
	abstract = {Modeling the effect of sequence variation on function is a fundamental problem for understanding and designing proteins. Since evolution encodes information about function into patterns in protein sequences, unsupervised models of variant effects can be learned from sequence data. The approach to date has been to fit a model to a family of related sequences. The conventional setting is limited, since a new model must be trained for each prediction task. We show that using only zero-shot inference, without any supervision from experimental data or additional training, protein language models capture the functional effects of sequence variation, performing at state-of-the-art.},
	language = {en},
	urldate = {2022-11-03},
	publisher = {bioRxiv},
	author = {Meier, Joshua and Rao, Roshan and Verkuil, Robert and Liu, Jason and Sercu, Tom and Rives, Alexander},
	month = jul,
	year = {2021},
	note = {316 citations (Crossref/DOI) [2025-10-22]
Pages: 2021.07.09.450648
Section: New Results},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/7I5WYWWC/2021-07-10 - Meier et al. - [ESM-1v] Language models enable zero-shot prediction of the effects of mutations on protein function.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/KGEPNITZ/2021.07.09.450648v1.html:text/html},
}

@misc{nguyen_hyenadna_2023,
	title = {{HyenaDNA}: {Long}-{Range} {Genomic} {Sequence} {Modeling} at {Single} {Nucleotide} {Resolution}},
	shorttitle = {{HyenaDNA}},
	url = {http://arxiv.org/abs/2306.15794},
	doi = {10.48550/arXiv.2306.15794},
	abstract = {Genomic (DNA) sequences encode an enormous amount of information for gene regulation and protein synthesis. Similar to natural language models, researchers have proposed foundation models in genomics to learn generalizable features from unlabeled genome data that can then be fine-tuned for downstream tasks such as identifying regulatory elements. Due to the quadratic scaling of attention, previous Transformer-based genomic models have used 512 to 4k tokens as context ({\textless}0.001\% of the human genome), significantly limiting the modeling of long-range interactions in DNA. In addition, these methods rely on tokenizers or fixed k-mers to aggregate meaningful DNA units, losing single nucleotide resolution where subtle genetic variations can completely alter protein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a large language model based on implicit convolutions was shown to match attention in quality while allowing longer context lengths and lower time complexity. Leveraging Hyena's new long-range capabilities, we present HyenaDNA, a genomic foundation model pretrained on the human reference genome with context lengths of up to 1 million tokens at the single nucleotide-level - an up to 500x increase over previous dense attention-based models. HyenaDNA scales sub-quadratically in sequence length (training up to 160x faster than Transformer), uses single nucleotide tokens, and has full global context at each layer. We explore what longer context enables - including the first use of in-context learning in genomics. On fine-tuned benchmarks from the Nucleotide Transformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 18 datasets using a model with orders of magnitude less parameters and pretraining data. On the GenomicBenchmarks, HyenaDNA surpasses SotA on 7 of 8 datasets on average by +10 accuracy points. Code at https://github.com/HazyResearch/hyena-dna.},
	urldate = {2025-07-23},
	publisher = {arXiv},
	author = {Nguyen, Eric and Poli, Michael and Faizi, Marjan and Thomas, Armin and Birch-Sykes, Callum and Wornow, Michael and Patel, Aman and Rabideau, Clayton and Massaroli, Stefano and Bengio, Yoshua and Ermon, Stefano and Baccus, Stephen A. and Ré, Chris},
	month = nov,
	year = {2023},
	note = {arXiv:2306.15794 [cs]},
	keywords = {Important, Podcast, Printed, Read},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/JVEF4BBI/2023-11-14 - Nguyen et al. - HyenaDNA Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/G59U5Z77/2306.html:text/html},
}

@article{rieke_future_2020,
	title = {The future of digital health with federated learning},
	volume = {3},
	copyright = {2020 The Author(s)},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-020-00323-1},
	doi = {10.1038/s41746-020-00323-1},
	abstract = {Data-driven machine learning (ML) has emerged as a promising approach for building accurate and robust statistical models from medical data, which is collected in huge volumes by modern healthcare systems. Existing medical data is not fully exploited by ML primarily because it sits in data silos and privacy concerns restrict access to this data. However, without access to sufficient data, ML will be prevented from reaching its full potential and, ultimately, from making the transition from research to clinical practice. This paper considers key factors contributing to this issue, explores how federated learning (FL) may provide a solution for the future of digital health and highlights the challenges and considerations that need to be addressed.},
	language = {en},
	number = {1},
	urldate = {2025-12-23},
	journal = {npj Digital Medicine},
	author = {Rieke, Nicola and Hancox, Jonny and Li, Wenqi and Milletarì, Fausto and Roth, Holger R. and Albarqouni, Shadi and Bakas, Spyridon and Galtier, Mathieu N. and Landman, Bennett A. and Maier-Hein, Klaus and Ourselin, Sébastien and Sheller, Micah and Summers, Ronald M. and Trask, Andrew and Xu, Daguang and Baust, Maximilian and Cardoso, M. Jorge},
	month = sep,
	year = {2020},
	note = {Publisher: Nature Publishing Group},
	pages = {119},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/STVR9U23/2020-09-14 - Rieke et al. - The future of digital health with federated learning.pdf:application/pdf},
}

@article{rives_biological_2021,
	title = {[{ESM}-1b] {Biological} structure and function emerge from scaling unsupervised learning to 250 million protein sequences},
	volume = {118},
	issn = {1091-6490},
	doi = {10.1073/pnas.2016239118},
	abstract = {In the field of artificial intelligence, a combination of scale in data and model capacity enabled by unsupervised learning has led to major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing promises unprecedented data on natural sequence diversity. Protein language modeling at the scale of evolution is a logical step toward predictive and generative artificial intelligence for biology. To this end, we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone. The learned representation space has a multiscale organization reflecting structure from the level of biochemical properties of amino acids to remote homology of proteins. Information about secondary and tertiary structure is encoded in the representations and can be identified by linear projections. Representation learning produces features that generalize across a range of applications, enabling state-of-the-art supervised prediction of mutational effect and secondary structure and improving state-of-the-art features for long-range contact prediction.},
	language = {eng},
	number = {15},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Rives, Alexander and Meier, Joshua and Sercu, Tom and Goyal, Siddharth and Lin, Zeming and Liu, Jason and Guo, Demi and Ott, Myle and Zitnick, C. Lawrence and Ma, Jerry and Fergus, Rob},
	month = apr,
	year = {2021},
	pmid = {33876751},
	pmcid = {PMC8053943},
	note = {2208 citations (Crossref/DOI) [2025-10-22]},
	pages = {e2016239118},
	file = {Rives et al. - 2021 - Biological structure and function emerge from scal-Supplement.pdf:/Users/meehl.joshua/Zotero/storage/V5K3TREP/2021-04-13 - Rives et al. - [ESM-1b] Biological structure and function emerge from scaling unsupervised learning to 250 million.pdf:application/pdf;Rives et al. - 2021 - Biological structure and function emerge from scal.pdf:/Users/meehl.joshua/Zotero/storage/45JE6CKI/2021-04-13 - Rives et al. - [ESM-1b] Biological structure and function emerge from scaling unsupervised learning to 250 million.pdf:application/pdf},
}

@inproceedings{sainz_nlp_2023,
	address = {Singapore},
	title = {{NLP} {Evaluation} in trouble: {On} the {Need} to {Measure} {LLM} {Data} {Contamination} for each {Benchmark}},
	shorttitle = {{NLP} {Evaluation} in trouble},
	url = {https://aclanthology.org/2023.findings-emnlp.722/},
	doi = {10.18653/v1/2023.findings-emnlp.722},
	abstract = {In this position paper we argue that the classical evaluation on Natural Language Processing (NLP) tasks using annotated benchmarks is in trouble. The worst kind of data contamination happens when a Large Language Model (LLM) is trained on the test split of a benchmark, and then evaluated in the same benchmark. The extent of the problem is unknown, as it is not straightforward to measure. Contamination causes an overestimation of the performance of a contaminated model in a target benchmark and associated task with respect to their non-contaminated counterparts. The consequences can be very harmful, with wrong scientific conclusions being published while other correct ones are discarded. This position paper defines different levels of data contamination and argues for a community effort, including the development of automatic and semi-automatic measures to detect when data from a benchmark was exposed to a model, and suggestions for flagging papers with conclusions that are compromised by data contamination.},
	urldate = {2025-12-23},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Sainz, Oscar and Campos, Jon and García-Ferrero, Iker and Etxaniz, Julen and de Lacalle, Oier Lopez and Agirre, Eneko},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {10776--10787},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/H3Y5WH24/2023-12 - Sainz et al. - NLP Evaluation in trouble On the Need to Measure LLM Data Contamination for each Benchmark.pdf:application/pdf},
}

@inproceedings{snell_prototypical_2017,
	title = {Prototypical {Networks} for {Few}-shot {Learning}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/cb8da6767461f2812ae4290eac7cbc42-Abstract.html},
	urldate = {2025-12-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Snell, Jake and Swersky, Kevin and Zemel, Richard},
	year = {2017},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/2DAMH7PR/2017 - Snell et al. - Prototypical Networks for Few-shot Learning.pdf:application/pdf},
}

@article{suzek_uniref_2007,
	title = {{UniRef}: comprehensive and non-redundant {UniProt} reference clusters},
	volume = {23},
	issn = {1367-4803},
	shorttitle = {{UniRef}},
	url = {https://doi.org/10.1093/bioinformatics/btm098},
	doi = {10.1093/bioinformatics/btm098},
	abstract = {Motivation: Redundant protein sequences in biological databases hinder sequence similarity searches and make interpretation of search results difficult. Clustering of protein sequence space based on sequence similarity helps organize all sequences into manageable datasets and reduces sampling bias and overrepresentation of sequences.Results: The UniRef (UniProt Reference Clusters) provide clustered sets of sequences from the UniProt Knowledgebase (UniProtKB) and selected UniProt Archive records to obtain complete coverage of sequence space at several resolutions while hiding redundant sequences. Currently covering \&gt;4 million source sequences, the UniRef100 database combines identical sequences and subfragments from any source organism into a single UniRef entry. UniRef90 and UniRef50 are built by clustering UniRef100 sequences at the 90 or 50\% sequence identity levels. UniRef100, UniRef90 and UniRef50 yield a database size reduction of ∼10, 40 and 70\%, respectively, from the source sequence set. The reduced redundancy increases the speed of similarity searches and improves detection of distant relationships. UniRef entries contain summary cluster and membership information, including the sequence of a representative protein, member count and common taxonomy of the cluster, the accession numbers of all the merged entries and links to rich functional annotation in UniProtKB to facilitate biological discovery. UniRef has already been applied to broad research areas ranging from genome annotation to proteomics data analysis.Availability: UniRef is updated biweekly and is available for online search and retrieval at http://www.uniprot.org, as well as for download at ftp://ftp.uniprot.org/pub/databases/uniprot/unirefContact:  bes23@georgetown.eduSupplementary information: Supplementary data are available at Bioinformatics online.},
	number = {10},
	urldate = {2025-12-16},
	journal = {Bioinformatics},
	author = {Suzek, Baris E. and Huang, Hongzhan and McGarvey, Peter and Mazumder, Raja and Wu, Cathy H.},
	month = may,
	year = {2007},
	pages = {1282--1288},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/SBM7NX8T/2007-05-15 - Suzek et al. - UniRef comprehensive and non-redundant UniProt reference clusters.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/3VW858PD/btm098.html:text/html},
}

@inproceedings{wang_characterizing_2019,
	title = {Characterizing and {Avoiding} {Negative} {Transfer}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Characterizing_and_Avoiding_Negative_Transfer_CVPR_2019_paper.html},
	urldate = {2025-12-23},
	author = {Wang, Zirui and Dai, Zihang and Poczos, Barnabas and Carbonell, Jaime},
	year = {2019},
	pages = {11293--11302},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/SBCUC6WG/2019 - Wang et al. - Characterizing and Avoiding Negative Transfer.pdf:application/pdf},
}

@misc{wang_tent_2021,
	title = {Tent: {Fully} {Test}-time {Adaptation} by {Entropy} {Minimization}},
	shorttitle = {Tent},
	url = {http://arxiv.org/abs/2006.10726},
	doi = {10.48550/arXiv.2006.10726},
	abstract = {A model must adapt itself to generalize to new and different data during testing. In this setting of fully test-time adaptation the model has only the test data and its own parameters. We propose to adapt by test entropy minimization (tent): we optimize the model for confidence as measured by the entropy of its predictions. Our method estimates normalization statistics and optimizes channel-wise affine transformations to update online on each batch. Tent reduces generalization error for image classification on corrupted ImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on ImageNet-C. Tent handles source-free domain adaptation on digit recognition from SVHN to MNIST/MNIST-M/USPS, on semantic segmentation from GTA to Cityscapes, and on the VisDA-C benchmark. These results are achieved in one epoch of test-time optimization without altering training.},
	urldate = {2025-12-23},
	publisher = {arXiv},
	author = {Wang, Dequan and Shelhamer, Evan and Liu, Shaoteng and Olshausen, Bruno and Darrell, Trevor},
	month = mar,
	year = {2021},
	note = {arXiv:2006.10726 [cs]},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/T4PK7HJL/2021-03-18 - Wang et al. - Tent Fully Test-time Adaptation by Entropy Minimization.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/HW7W4FXP/2006.html:text/html},
}

@inproceedings{zaken_bitfit_2022,
	address = {Dublin, Ireland},
	title = {{BitFit}: {Simple} {Parameter}-efficient {Fine}-tuning for {Transformer}-based {Masked} {Language}-models},
	shorttitle = {{BitFit}},
	url = {https://aclanthology.org/2022.acl-short.1/},
	doi = {10.18653/v1/2022.acl-short.1},
	abstract = {We introduce BitFit, a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified. We show that with small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse fine-tuning methods. Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.},
	urldate = {2025-12-23},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ben Zaken, Elad and Goldberg, Yoav and Ravfogel, Shauli},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {1--9},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/KVY4QCK4/2022-05 - Ben Zaken et al. - BitFit Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models.pdf:application/pdf},
}

@misc{zhou_dnabert-2_2024,
	title = {{DNABERT}-2: {Efficient} {Foundation} {Model} and {Benchmark} {For} {Multi}-{Species} {Genome}},
	shorttitle = {{DNABERT}-2},
	url = {http://arxiv.org/abs/2306.15006},
	doi = {10.48550/arXiv.2306.15006},
	abstract = {Decoding the linguistic intricacies of the genome is a crucial problem in biology, and pre-trained foundational models such as DNABERT and Nucleotide Transformer have made significant strides in this area. Existing works have largely hinged on k-mer, fixed-length permutations of A, T, C, and G, as the token of the genome language due to its simplicity. However, we argue that the computation and sample inefficiencies introduced by k-mer tokenization are primary obstacles in developing large genome foundational models. We provide conceptual and empirical insights into genome tokenization, building on which we propose to replace k-mer tokenization with Byte Pair Encoding (BPE), a statistics-based data compression algorithm that constructs tokens by iteratively merging the most frequent co-occurring genome segment in the corpus. We demonstrate that BPE not only overcomes the limitations of k-mer tokenization but also benefits from the computational efficiency of non-overlapping tokenization. Based on these insights, we introduce DNABERT-2, a refined genome foundation model that adapts an efficient tokenizer and employs multiple strategies to overcome input length constraints, reduce time and memory expenditure, and enhance model capability. Furthermore, we identify the absence of a comprehensive and standardized benchmark for genome understanding as another significant impediment to fair comparative analysis. In response, we propose the Genome Understanding Evaluation (GUE), a comprehensive multi-species genome classification dataset that amalgamates \$36\$ distinct datasets across \$9\$ tasks, with input lengths ranging from \$70\$ to \$10000\$. Through comprehensive experiments on the GUE benchmark, we demonstrate that DNABERT-2 achieves comparable performance to the state-of-the-art model with \$21 {\textbackslash}times\$ fewer parameters and approximately \$92 {\textbackslash}times\$ less GPU time in pre-training.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Zhou, Zhihan and Ji, Yanrong and Li, Weijian and Dutta, Pratik and Davuluri, Ramana and Liu, Han},
	month = mar,
	year = {2024},
	note = {arXiv:2306.15006 [q-bio]},
	keywords = {Important, Podcast, Printed, Read},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/2KZP5KPQ/2024-03-18 - Zhou et al. - DNABERT-2 Efficient Foundation Model and Benchmark For Multi-Species Genome.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/KUYT79IL/2306.html:text/html},
}
