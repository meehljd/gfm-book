@article{dalla-torre_nucleotide_2023,
	title = {Nucleotide {Transformer}: building and evaluating robust foundation models for human genomics},
	volume = {22},
	copyright = {2024 The Author(s)},
	issn = {1548-7105},
	shorttitle = {Nucleotide {Transformer}},
	url = {https://www.nature.com/articles/s41592-024-02523-z},
	doi = {10.1038/s41592-024-02523-z},
	abstract = {The prediction of molecular phenotypes from DNA sequences remains a longstanding challenge in genomics, often driven by limited annotated data and the inability to transfer learnings between tasks. Here, we present an extensive study of foundation models pre-trained on DNA sequences, named Nucleotide Transformer, ranging from 50 million up to 2.5 billion parameters and integrating information from 3,202 human genomes and 850 genomes from diverse species. These transformer models yield context-specific representations of nucleotide sequences, which allow for accurate predictions even in low-data settings. We show that the developed models can be fine-tuned at low cost to solve a variety of genomics applications. Despite no supervision, the models learned to focus attention on key genomic elements and can be used to improve the prioritization of genetic variants. The training and application of foundational models in genomics provides a widely applicable approach for accurate molecular phenotype prediction from DNA sequence.},
	language = {en},
	number = {2},
	urldate = {2025-04-16},
	journal = {Nature Methods},
	author = {Dalla-Torre, Hugo and Gonzalez, Liam and Mendoza-Revilla, Javier and Lopez Carranza, Nicolas and Grzywaczewski, Adam Henryk and Oteri, Francesco and Dallago, Christian and Trop, Evan and de Almeida, Bernardo P. and Sirelkhatim, Hassan and Richard, Guillaume and Skwark, Marcin and Beguir, Karim and Lopez, Marie and Pierrot, Thomas},
	month = jan,
	year = {2023},
	note = {123 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Important, Podcast, Printed, Read},
	pages = {287--297},
	file = {[Preprint] Dalla-Torre et al. - 2023 - Nucleotide Transformer building and evaluating robust foundation models for human genomics:/Users/meehl.joshua/Zotero/storage/METWGZEF/2023-01-15 - Dalla-Torre et al. - Nucleotide Transformer building and evaluating robust foundation models for human genomics.pdf:application/pdf;Full Text PDF:/Users/meehl.joshua/Zotero/storage/K28P6JFS/2023-01-15 - Dalla-Torre et al. - Nucleotide Transformer building and evaluating robust foundation models for human genomics.pdf:application/pdf},
}

@inproceedings{jawahar_what_2019,
	address = {Florence, Italy},
	title = {What does {BERT} learn about the structure of language?},
	url = {https://inria.hal.science/hal-02131630},
	abstract = {BERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks. This result indicates the possibility that BERT networks capture structural information about language. In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT. We first show that BERT's phrasal representation captures phrase-level information in the lower layers. We also show that BERT's intermediate layers encode a rich hierarchy of linguistic information, with surface features at the bottom, syntactic features in the middle and semantic features at the top. BERT turns out to require deeper layers when long-distance dependency information is required, e.g.{\textasciitilde}to track subject-verb agreement. Finally, we show that BERT representations capture linguistic information in a compositional way that mimics classical, tree-like structures.},
	urldate = {2025-12-23},
	booktitle = {{ACL} 2019 - 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	author = {Jawahar, Ganesh and Sagot, Benoît and Seddah, Djamé},
	month = jul,
	year = {2019},
	file = {HAL PDF Full Text:/Users/meehl.joshua/Zotero/storage/IZTP3GVQ/2019-07 - Jawahar et al. - What does BERT learn about the structure of language.pdf:application/pdf},
}

@article{ji_dnabert_2021,
	title = {{DNABERT}: pre-trained {Bidirectional} {Encoder} {Representations} from {Transformers} model for {DNA}-language in genome},
	volume = {37},
	issn = {1367-4803},
	shorttitle = {{DNABERT}},
	url = {https://doi.org/10.1093/bioinformatics/btab083},
	doi = {10.1093/bioinformatics/btab083},
	abstract = {Deciphering the language of non-coding DNA is one of the fundamental problems in genome research. Gene regulatory code is highly complex due to the existence of polysemy and distant semantic relationship, which previous informatics methods often fail to capture especially in data-scarce scenarios.To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, to capture global and transferrable understanding of genomic DNA sequences based on up and downstream nucleotide contexts. We compared DNABERT to the most widely used programs for genome-wide regulatory elements prediction and demonstrate its ease of use, accuracy and efficiency. We show that the single pre-trained transformers model can simultaneously achieve state-of-the-art performance on prediction of promoters, splice sites and transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, DNABERT enables direct visualization of nucleotide-level importance and semantic relationship within input sequences for better interpretability and accurate identification of conserved sequence motifs and functional genetic variant candidates. Finally, we demonstrate that pre-trained DNABERT with human genome can even be readily applied to other organisms with exceptional performance. We anticipate that the pre-trained DNABERT model can be fined tuned to many other sequence analyses tasks.The source code, pretrained and finetuned model for DNABERT are available at GitHub (https://github.com/jerryji1993/DNABERT).Supplementary data are available at Bioinformatics online.},
	number = {15},
	urldate = {2025-08-31},
	journal = {Bioinformatics},
	author = {Ji, Yanrong and Zhou, Zhihan and Liu, Han and Davuluri, Ramana V},
	month = aug,
	year = {2021},
	note = {802 citations (Crossref/DOI) [2025-10-22]},
	pages = {2112--2120},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/VKSTMFPQ/2021-08-09 - Ji et al. - DNABERT pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/UBBEDFL5/btab083.html:text/html},
}

@article{kelley_cross-species_2020,
	title = {[{Basenji2}] {Cross}-species regulatory sequence activity prediction},
	volume = {16},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008050},
	doi = {10.1371/journal.pcbi.1008050},
	abstract = {Machine learning algorithms trained to predict the regulatory activity of nucleic acid sequences have revealed principles of gene regulation and guided genetic variation analysis. While the human genome has been extensively annotated and studied, model organisms have been less explored. Model organism genomes offer both additional training sequences and unique annotations describing tissue and cell states unavailable in humans. Here, we develop a strategy to train deep convolutional neural networks simultaneously on multiple genomes and apply it to learn sequence predictors for large compendia of human and mouse data. Training on both genomes improves gene expression prediction accuracy on held out and variant sequences. We further demonstrate a novel and powerful approach to apply mouse regulatory models to analyze human genetic variants associated with molecular phenotypes and disease. Together these techniques unleash thousands of non-human epigenetic and transcriptional profiles toward more effective investigation of how gene regulation affects human disease.},
	language = {en},
	number = {7},
	urldate = {2025-12-15},
	journal = {PLOS Computational Biology},
	author = {Kelley, David R.},
	month = jul,
	year = {2020},
	note = {Publisher: Public Library of Science},
	pages = {e1008050},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/PRY99MKS/Jul 20, 2020 - Kelley - Cross-species regulatory sequence activity prediction.pdf:application/pdf},
}

@article{kircher_general_2014,
	title = {A general framework for estimating the relative pathogenicity of human genetic variants},
	volume = {46},
	issn = {1546-1718},
	doi = {10.1038/ng.2892},
	abstract = {Current methods for annotating and interpreting human genetic variation tend to exploit a single information type (for example, conservation) and/or are restricted in scope (for example, to missense changes). Here we describe Combined Annotation-Dependent Depletion (CADD), a method for objectively integrating many diverse annotations into a single measure (C score) for each variant. We implement CADD as a support vector machine trained to differentiate 14.7 million high-frequency human-derived alleles from 14.7 million simulated variants. We precompute C scores for all 8.6 billion possible human single-nucleotide variants and enable scoring of short insertions-deletions. C scores correlate with allelic diversity, annotations of functionality, pathogenicity, disease severity, experimentally measured regulatory effects and complex trait associations, and they highly rank known pathogenic variants within individual genomes. The ability of CADD to prioritize functional, deleterious and pathogenic variants across many functional categories, effect sizes and genetic architectures is unmatched by any current single-annotation method.},
	language = {eng},
	number = {3},
	journal = {Nature Genetics},
	author = {Kircher, Martin and Witten, Daniela M. and Jain, Preti and O'Roak, Brian J. and Cooper, Gregory M. and Shendure, Jay},
	month = feb,
	year = {2014},
	pmid = {24487276},
	pmcid = {PMC3992975},
	note = {5494 citations (Crossref/DOI) [2025-10-22]},
	pages = {310--315},
}

@misc{nguyen_hyenadna_2023,
	title = {{HyenaDNA}: {Long}-{Range} {Genomic} {Sequence} {Modeling} at {Single} {Nucleotide} {Resolution}},
	shorttitle = {{HyenaDNA}},
	url = {http://arxiv.org/abs/2306.15794},
	doi = {10.48550/arXiv.2306.15794},
	abstract = {Genomic (DNA) sequences encode an enormous amount of information for gene regulation and protein synthesis. Similar to natural language models, researchers have proposed foundation models in genomics to learn generalizable features from unlabeled genome data that can then be fine-tuned for downstream tasks such as identifying regulatory elements. Due to the quadratic scaling of attention, previous Transformer-based genomic models have used 512 to 4k tokens as context ({\textless}0.001\% of the human genome), significantly limiting the modeling of long-range interactions in DNA. In addition, these methods rely on tokenizers or fixed k-mers to aggregate meaningful DNA units, losing single nucleotide resolution where subtle genetic variations can completely alter protein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a large language model based on implicit convolutions was shown to match attention in quality while allowing longer context lengths and lower time complexity. Leveraging Hyena's new long-range capabilities, we present HyenaDNA, a genomic foundation model pretrained on the human reference genome with context lengths of up to 1 million tokens at the single nucleotide-level - an up to 500x increase over previous dense attention-based models. HyenaDNA scales sub-quadratically in sequence length (training up to 160x faster than Transformer), uses single nucleotide tokens, and has full global context at each layer. We explore what longer context enables - including the first use of in-context learning in genomics. On fine-tuned benchmarks from the Nucleotide Transformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 18 datasets using a model with orders of magnitude less parameters and pretraining data. On the GenomicBenchmarks, HyenaDNA surpasses SotA on 7 of 8 datasets on average by +10 accuracy points. Code at https://github.com/HazyResearch/hyena-dna.},
	urldate = {2025-07-23},
	publisher = {arXiv},
	author = {Nguyen, Eric and Poli, Michael and Faizi, Marjan and Thomas, Armin and Birch-Sykes, Callum and Wornow, Michael and Patel, Aman and Rabideau, Clayton and Massaroli, Stefano and Bengio, Yoshua and Ermon, Stefano and Baccus, Stephen A. and Ré, Chris},
	month = nov,
	year = {2023},
	note = {arXiv:2306.15794 [cs]},
	keywords = {Important, Podcast, Printed, Read},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/JVEF4BBI/2023-11-14 - Nguyen et al. - HyenaDNA Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/G59U5Z77/2306.html:text/html},
}

@article{rives_biological_2021,
	title = {[{ESM}-1b] {Biological} structure and function emerge from scaling unsupervised learning to 250 million protein sequences},
	volume = {118},
	issn = {1091-6490},
	doi = {10.1073/pnas.2016239118},
	abstract = {In the field of artificial intelligence, a combination of scale in data and model capacity enabled by unsupervised learning has led to major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing promises unprecedented data on natural sequence diversity. Protein language modeling at the scale of evolution is a logical step toward predictive and generative artificial intelligence for biology. To this end, we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone. The learned representation space has a multiscale organization reflecting structure from the level of biochemical properties of amino acids to remote homology of proteins. Information about secondary and tertiary structure is encoded in the representations and can be identified by linear projections. Representation learning produces features that generalize across a range of applications, enabling state-of-the-art supervised prediction of mutational effect and secondary structure and improving state-of-the-art features for long-range contact prediction.},
	language = {eng},
	number = {15},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Rives, Alexander and Meier, Joshua and Sercu, Tom and Goyal, Siddharth and Lin, Zeming and Liu, Jason and Guo, Demi and Ott, Myle and Zitnick, C. Lawrence and Ma, Jerry and Fergus, Rob},
	month = apr,
	year = {2021},
	pmid = {33876751},
	pmcid = {PMC8053943},
	note = {2208 citations (Crossref/DOI) [2025-10-22]},
	pages = {e2016239118},
	file = {Rives et al. - 2021 - Biological structure and function emerge from scal-Supplement.pdf:/Users/meehl.joshua/Zotero/storage/V5K3TREP/2021-04-13 - Rives et al. - [ESM-1b] Biological structure and function emerge from scaling unsupervised learning to 250 million.pdf:application/pdf;Rives et al. - 2021 - Biological structure and function emerge from scal.pdf:/Users/meehl.joshua/Zotero/storage/45JE6CKI/2021-04-13 - Rives et al. - [ESM-1b] Biological structure and function emerge from scaling unsupervised learning to 250 million.pdf:application/pdf},
}

@article{suzek_uniref_2007,
	title = {{UniRef}: comprehensive and non-redundant {UniProt} reference clusters},
	volume = {23},
	issn = {1367-4803},
	shorttitle = {{UniRef}},
	url = {https://doi.org/10.1093/bioinformatics/btm098},
	doi = {10.1093/bioinformatics/btm098},
	abstract = {Motivation: Redundant protein sequences in biological databases hinder sequence similarity searches and make interpretation of search results difficult. Clustering of protein sequence space based on sequence similarity helps organize all sequences into manageable datasets and reduces sampling bias and overrepresentation of sequences.Results: The UniRef (UniProt Reference Clusters) provide clustered sets of sequences from the UniProt Knowledgebase (UniProtKB) and selected UniProt Archive records to obtain complete coverage of sequence space at several resolutions while hiding redundant sequences. Currently covering \&gt;4 million source sequences, the UniRef100 database combines identical sequences and subfragments from any source organism into a single UniRef entry. UniRef90 and UniRef50 are built by clustering UniRef100 sequences at the 90 or 50\% sequence identity levels. UniRef100, UniRef90 and UniRef50 yield a database size reduction of ∼10, 40 and 70\%, respectively, from the source sequence set. The reduced redundancy increases the speed of similarity searches and improves detection of distant relationships. UniRef entries contain summary cluster and membership information, including the sequence of a representative protein, member count and common taxonomy of the cluster, the accession numbers of all the merged entries and links to rich functional annotation in UniProtKB to facilitate biological discovery. UniRef has already been applied to broad research areas ranging from genome annotation to proteomics data analysis.Availability: UniRef is updated biweekly and is available for online search and retrieval at http://www.uniprot.org, as well as for download at ftp://ftp.uniprot.org/pub/databases/uniprot/unirefContact:  bes23@georgetown.eduSupplementary information: Supplementary data are available at Bioinformatics online.},
	number = {10},
	urldate = {2025-12-16},
	journal = {Bioinformatics},
	author = {Suzek, Baris E. and Huang, Hongzhan and McGarvey, Peter and Mazumder, Raja and Wu, Cathy H.},
	month = may,
	year = {2007},
	pages = {1282--1288},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/SBM7NX8T/2007-05-15 - Suzek et al. - UniRef comprehensive and non-redundant UniProt reference clusters.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/3VW858PD/btm098.html:text/html},
}

@inproceedings{wang_characterizing_2019,
	title = {Characterizing and {Avoiding} {Negative} {Transfer}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Characterizing_and_Avoiding_Negative_Transfer_CVPR_2019_paper.html},
	urldate = {2025-12-23},
	author = {Wang, Zirui and Dai, Zihang and Poczos, Barnabas and Carbonell, Jaime},
	year = {2019},
	pages = {11293--11302},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/SBCUC6WG/2019 - Wang et al. - Characterizing and Avoiding Negative Transfer.pdf:application/pdf},
}
