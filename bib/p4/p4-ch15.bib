@misc{chen_rna-fm_2022,
	title = {[{RNA}-{FM}] {Interpretable} {RNA} {Foundation} {Model} from {Unannotated} {Data} for {Highly} {Accurate} {RNA} {Structure} and {Function} {Predictions}},
	url = {http://arxiv.org/abs/2204.00300},
	doi = {10.48550/arXiv.2204.00300},
	abstract = {Non-coding RNA structure and function are essential to understanding various biological processes, such as cell signaling, gene expression, and post-transcriptional regulations. These are all among the core problems in the RNA field. With the rapid growth of sequencing technology, we have accumulated a massive amount of unannotated RNA sequences. On the other hand, expensive experimental observatory results in only limited numbers of annotated data and 3D structures. Hence, it is still challenging to design computational methods for predicting their structures and functions. The lack of annotated data and systematic study causes inferior performance. To resolve the issue, we propose a novel RNA foundation model (RNA-FM) to take advantage of all the 23 million non-coding RNA sequences through self-supervised learning. Within this approach, we discover that the pre-trained RNA-FM could infer sequential and evolutionary information of non-coding RNAs without using any labels. Furthermore, we demonstrate RNA-FM's effectiveness by applying it to the downstream secondary/3D structure prediction, SARS-CoV-2 genome structure and evolution prediction, protein-RNA binding preference modeling, and gene expression regulation modeling. The comprehensive experiments show that the proposed method improves the RNA structural and functional modelling results significantly and consistently. Despite only being trained with unlabelled data, RNA-FM can serve as the foundational model for the field.},
	urldate = {2025-05-27},
	publisher = {arXiv},
	author = {Chen, Jiayang and Hu, Zhihang and Sun, Siqi and Tan, Qingxiong and Wang, Yixuan and Yu, Qinze and Zong, Licheng and Hong, Liang and Xiao, Jin and Shen, Tao and King, Irwin and Li, Yu},
	month = aug,
	year = {2022},
	note = {arXiv:2204.00300 [q-bio]},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/7JUCADM9/2022-08-08 - Chen et al. - [RNA-FM] Interpretable RNA Foundation Model from Unannotated Data for Highly Accurate RNA Structure.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/5455JHP5/2204.html:text/html},
}

@misc{naghipourfar_cdsfm_2024,
	title = {[{cdsFM} - {EnCodon}/{DeCodon}] {A} {Suite} of {Foundation} {Models} {Captures} the {Contextual} {Interplay} {Between} {Codons}},
	copyright = {© 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2024.10.10.617568v1},
	doi = {10.1101/2024.10.10.617568},
	abstract = {In the canonical genetic code, many amino acids are assigned more than one codon. Work by us and others has shown that the choice of these synonymous codon is not random, and carries regulatory and functional consequences. Existing protein foundation models ignore this context-dependent role of coding sequence in shaping the protein landscape of the cell. To address this gap, we introduce cdsFM, a suite of codon-resolution large language models, including both EnCodon and DeCodon models, with up to 1B parameters. Pre-trained on 60 million protein-coding sequences from more than 5,000 species, our models effectively learn the relationship between codons and amino acids, recapitualing the overall structure of the genetic code. In addition to outperforming state-of-the-art genomic foundation models in a variety of zero-shot and few-shot learning tasks, the larger pre-trained models were superior in predicting the choice of synonymous codons. To systematically assess the impact of synonymous codon choices on protein expression and our models’ ability to capture these effects, we generated a large dataset measuring overall and surface expression levels of three proteins as a function of changes in their synonymous codons. We showed that our EnCodon models could be readily fine-tuned to predict the contextual consequences of synonymous codon choices. Armed with this knowledge, we applied EnCodon to existing clinical datasets of synonymous variants, and we identified a large number of synonymous codons that are likely pathogenic, several of which we experimentally confirmed in a cellbased model. Together, our findings establish the cdsFM suite as a powerful tool for decoding the complex functional grammar underlying the choice of synonymous codons.},
	language = {en},
	urldate = {2025-05-27},
	publisher = {bioRxiv},
	author = {Naghipourfar, Mohsen and Chen, Siyu and Howard, Mathew K. and Macdonald, Christian B. and Saberi, Ali and Hagen, Timo and Mofrad, Mohammad R. K. and Coyote-Maestas, Willow and Goodarzi, Hani},
	month = oct,
	year = {2024},
	note = {6 citations (Crossref/DOI) [2025-10-22]
Pages: 2024.10.10.617568
Section: New Results},
	keywords = {Printed},
	file = {2024-10-13 - Naghipourfar et al. - [cdsFM - EnCodonDeCodon] A Suite of Foundation Models Captures the Contextual Interplay Between Cod:/Users/meehl.joshua/Zotero/storage/PMT8CUN4/2024-10-13 - Naghipourfar et al. - [cdsFM - EnCodonDeCodon] A Suite of Foundation Models Captures the Contextual Interplay Between Cod.pdf:application/pdf},
}

@misc{liu_life-code_2025,
	title = {Life-{Code}: {Central} {Dogma} {Modeling} with {Multi}-{Omics} {Sequence} {Unification}},
	shorttitle = {Life-{Code}},
	url = {http://arxiv.org/abs/2502.07299},
	doi = {10.48550/arXiv.2502.07299},
	abstract = {The interactions between DNA, RNA, and proteins are fundamental to biological processes, as illustrated by the central dogma of molecular biology. Although modern biological pre-trained models have achieved great success in analyzing these macromolecules individually, their interconnected nature remains underexplored. This paper follows the guidance of the central dogma to redesign both the data and model pipeline and offers a comprehensive framework, Life-Code, that spans different biological functions. As for data flow, we propose a unified pipeline to integrate multi-omics data by reverse-transcribing RNA and reverse-translating amino acids into nucleotide-based sequences. As for the model, we design a codon tokenizer and a hybrid long-sequence architecture to encode the interactions between coding and non-coding regions through masked modeling pre-training. To model the translation and folding process with coding sequences, Life-Code learns protein structures of the corresponding amino acids by knowledge distillation from off-the-shelf protein language models. Such designs enable Life-Code to capture complex interactions within genetic sequences, providing a more comprehensive understanding of multi-omics with the central dogma. Extensive experiments show that Life-Code achieves state-of-the-art results on various tasks across three omics, highlighting its potential for advancing multi-omics analysis and interpretation.},
	urldate = {2025-08-07},
	publisher = {arXiv},
	author = {Liu, Zicheng and Li, Siyuan and Chen, Zhiyuan and Wu, Fang and Yu, Chang and Yang, Qirong and Guo, Yucheng and Yang, Yujie and Zhang, Xiaoming and Li, Stan Z.},
	month = jun,
	year = {2025},
	note = {arXiv:2502.07299 [cs]},
	keywords = {Printed, Podcast},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/LFXSCF3K/2025-06-15 - Liu et al. - Life-Code Central Dogma Modeling with Multi-Omics Sequence Unification.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/P626N5L8/2502.html:text/html},
}

@misc{li_codonbert_2023,
	title = {{CodonBERT}: {Large} {Language} {Models} for {mRNA} design and optimization},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	shorttitle = {{CodonBERT}},
	url = {https://www.biorxiv.org/content/10.1101/2023.09.09.556981v1},
	doi = {10.1101/2023.09.09.556981},
	abstract = {mRNA based vaccines and therapeutics are gaining popularity and usage across a wide range of conditions. One of the critical issues when designing such mRNAs is sequence optimization. Even small proteins or peptides can be encoded by an enormously large number of mRNAs. The actual mRNA sequence can have a large impact on several properties including expression, stability, immunogenicity, and more. To enable the selection of an optimal sequence, we developed CodonBERT, a large language model (LLM) for mRNAs. Unlike prior models, CodonBERT uses codons as inputs which enables it to learn better representations. CodonBERT was trained using more than 10 million mRNA sequences from a diverse set of organisms. The resulting model captures important biological concepts. CodonBERT can also be extended to perform prediction tasks for various mRNA properties. CodonBERT outperforms previous mRNA prediction methods including on a new flu vaccine dataset.},
	language = {en},
	urldate = {2025-05-27},
	publisher = {bioRxiv},
	author = {Li, Sizhen and Moayedpour, Saeed and Li, Ruijiang and Bailey, Michael and Riahi, Saleh and Miladi, Milad and Miner, Jacob and Zheng, Dinghai and Wang, Jun and Balsubramani, Akshay and Tran, Khang and Zacharia, Minnie and Wu, Monica and Gu, Xiaobo and Clinton, Ryan and Asquith, Carla and Skalesk, Joseph and Boeglin, Lianne and Chivukula, Sudha and Dias, Anusha and Montoya, Fernando Ulloa and Agarwal, Vikram and Bar-Joseph, Ziv and Jager, Sven},
	month = sep,
	year = {2023},
	note = {20 citations (Crossref/DOI) [2025-10-22]
Pages: 2023.09.09.556981
Section: New Results},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/L7TR5V68/2023-09-12 - Li et al. - CodonBERT Large Language Models for mRNA design and optimization.pdf:application/pdf},
}

@misc{sample_human_2019,
	title = {MISSING BIB ENTRY: sample_human_2019},
	note = {Placeholder generated from chapter citations; replace with full BibTeX.},
	year = {2019},
}

@misc{agarwal_predicting_2022,
	title = {MISSING BIB ENTRY: agarwal_predicting_2022},
	note = {Placeholder generated from chapter citations; replace with full BibTeX.},
	year = {2022},
}

@misc{agarwal_predicting_2015,
	title = {MISSING BIB ENTRY: agarwal_predicting_2015},
	note = {Placeholder generated from chapter citations; replace with full BibTeX.},
	year = {2015},
}
