@article{abramson_alphafold3_2024,
	title = {[{AlphaFold3}] {Accurate} structure prediction of biomolecular interactions with {AlphaFold} 3},
	volume = {630},
	copyright = {2024 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-024-07487-w},
	doi = {10.1038/s41586-024-07487-w},
	abstract = {The introduction of AlphaFold 21 has spurred a revolution in modelling the structure of proteins and their interactions, enabling a huge range of applications in protein modelling and design2–6. Here we describe our AlphaFold 3 model with a substantially updated diffusion-based architecture that is capable of predicting the joint structure of complexes including proteins, nucleic acids, small molecules, ions and modified residues. The new AlphaFold model demonstrates substantially improved accuracy over many previous specialized tools: far greater accuracy for protein–ligand interactions compared with state-of-the-art docking tools, much higher accuracy for protein–nucleic acid interactions compared with nucleic-acid-specific predictors and substantially higher antibody–antigen prediction accuracy compared with AlphaFold-Multimer v.2.37,8. Together, these results show that high-accuracy modelling across biomolecular space is possible within a single unified deep-learning framework.},
	language = {en},
	number = {8016},
	urldate = {2025-07-11},
	journal = {Nature},
	author = {Abramson, Josh and Adler, Jonas and Dunger, Jack and Evans, Richard and Green, Tim and Pritzel, Alexander and Ronneberger, Olaf and Willmore, Lindsay and Ballard, Andrew J. and Bambrick, Joshua and Bodenstein, Sebastian W. and Evans, David A. and Hung, Chia-Chun and O’Neill, Michael and Reiman, David and Tunyasuvunakool, Kathryn and Wu, Zachary and Žemgulytė, Akvilė and Arvaniti, Eirini and Beattie, Charles and Bertolli, Ottavia and Bridgland, Alex and Cherepanov, Alexey and Congreve, Miles and Cowen-Rivers, Alexander I. and Cowie, Andrew and Figurnov, Michael and Fuchs, Fabian B. and Gladman, Hannah and Jain, Rishub and Khan, Yousuf A. and Low, Caroline M. R. and Perlin, Kuba and Potapenko, Anna and Savy, Pascal and Singh, Sukhdeep and Stecula, Adrian and Thillaisundaram, Ashok and Tong, Catherine and Yakneen, Sergei and Zhong, Ellen D. and Zielinski, Michal and Žídek, Augustin and Bapst, Victor and Kohli, Pushmeet and Jaderberg, Max and Hassabis, Demis and Jumper, John M.},
	month = may,
	year = {2024},
	note = {7239 citations (Crossref/DOI) [2025-10-22]
5776 citations (Semantic Scholar/DOI) [2025-10-22]
7239 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Important},
	pages = {493--500},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/IMVZAJZR/2024-05-08 - Abramson et al. - [AlphaFold3] Accurate structure prediction of biomolecular interactions with AlphaFold 3.pdf:application/pdf},
}

@article{brandes_genome-wide_2023,
	title = {Genome-wide prediction of disease variant effects with a deep protein language model},
	volume = {55},
	copyright = {2023 The Author(s)},
	issn = {1546-1718},
	url = {https://www.nature.com/articles/s41588-023-01465-0},
	doi = {10.1038/s41588-023-01465-0},
	abstract = {Predicting the effects of coding variants is a major challenge. While recent deep-learning models have improved variant effect prediction accuracy, they cannot analyze all coding variants due to dependency on close homologs or software limitations. Here we developed a workflow using ESM1b, a 650-million-parameter protein language model, to predict all {\textasciitilde}450 million possible missense variant effects in the human genome, and made all predictions available on a web portal. ESM1b outperformed existing methods in classifying {\textasciitilde}150,000 ClinVar/HGMD missense variants as pathogenic or benign and predicting measurements across 28 deep mutational scan datasets. We further annotated {\textasciitilde}2 million variants as damaging only in specific protein isoforms, demonstrating the importance of considering all isoforms when predicting variant effects. Our approach also generalizes to more complex coding variants such as in-frame indels and stop-gains. Together, these results establish protein language models as an effective, accurate and general approach to predicting variant effects.},
	language = {en},
	number = {9},
	urldate = {2025-08-07},
	journal = {Nature Genetics},
	author = {Brandes, Nadav and Goldman, Grant and Wang, Charlotte H. and Ye, Chun Jimmie and Ntranos, Vasilis},
	month = aug,
	year = {2023},
	note = {312 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Printed, Read},
	pages = {1512--1522},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/8TS597B9/2023-09 - Brandes et al. - Genome-wide prediction of disease variant effects with a deep protein language model.pdf:application/pdf},
}

@article{cheng_alphamissense_2023,
	title = {[{AlphaMissense}] {Accurate} proteome-wide missense variant effect prediction with {AlphaMissense}},
	volume = {381},
	url = {https://www.science.org/doi/full/10.1126/science.adg7492},
	doi = {10.1126/science.adg7492},
	abstract = {The vast majority of missense variants observed in the human genome are of unknown clinical significance. We present AlphaMissense, an adaptation of AlphaFold fine-tuned on human and primate variant population frequency databases to predict missense variant pathogenicity. By combining structural context and evolutionary conservation, our model achieves state-of-the-art results across a wide range of genetic and experimental benchmarks, all without explicitly training on such data. The average pathogenicity score of genes is also predictive for their cell essentiality, capable of identifying short essential genes that existing statistical approaches are underpowered to detect. As a resource to the community, we provide a database of predictions for all possible human single amino acid substitutions and classify 89\% of missense variants as either likely benign or likely pathogenic.},
	number = {6664},
	urldate = {2023-09-21},
	journal = {Science},
	author = {Cheng, Jun and Novati, Guido and Pan, Joshua and Bycroft, Clare and Žemgulytė, Akvilė and Applebaum, Taylor and Pritzel, Alexander and Wong, Lai Hong and Zielinski, Michal and Sargeant, Tobias and Schneider, Rosalia G. and Senior, Andrew W. and Jumper, John and Hassabis, Demis and Kohli, Pushmeet and Avsec, Žiga},
	month = sep,
	year = {2023},
	note = {1325 citations (Crossref/DOI) [2025-10-22]
Publisher: American Association for the Advancement of Science},
	keywords = {Printed, Read},
	pages = {eadg7492},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/MZM35E4X/2023-09-19 - Cheng et al. - [AlphaMissense] Accurate proteome-wide missense variant effect prediction with AlphaMissense.pdf:application/pdf;science.adg7492_sm:/Users/meehl.joshua/Zotero/storage/TXHUTZAK/science.adg7492_sm.pdf:application/pdf},
}

@misc{elnaggar_prottrans_2021,
	title = {{ProtTrans}: {Towards} {Cracking} the {Language} of {Life}'s {Code} {Through} {Self}-{Supervised} {Deep} {Learning} and {High} {Performance} {Computing}},
	shorttitle = {{ProtTrans}},
	url = {http://arxiv.org/abs/2007.06225},
	doi = {10.48550/arXiv.2007.06225},
	abstract = {Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models taken from NLP. These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids. The LMs were trained on the Summit supercomputer using 5616 GPUs and TPU Pod up-to 1024 cores. Dimensionality reduction revealed that the raw protein LM-embeddings from unlabeled data captured some biophysical features of protein sequences. We validated the advantage of using the embeddings as exclusive input for several subsequent tasks. The first was a per-residue prediction of protein secondary structure (3-state accuracy Q3=81\%-87\%); the second were per-protein predictions of protein sub-cellular localization (ten-state accuracy: Q10=81\%) and membrane vs. water-soluble (2-state accuracy Q2=91\%). For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without using evolutionary information thereby bypassing expensive database searches. Taken together, the results implied that protein LMs learned some of the grammar of the language of life. To facilitate future work, we released our models at https://github.com/agemagician/ProtTrans.},
	urldate = {2025-07-25},
	publisher = {arXiv},
	author = {Elnaggar, Ahmed and Heinzinger, Michael and Dallago, Christian and Rihawi, Ghalia and Wang, Yu and Jones, Llion and Gibbs, Tom and Feher, Tamas and Angerer, Christoph and Steinegger, Martin and Bhowmik, Debsindhu and Rost, Burkhard},
	month = may,
	year = {2021},
	note = {arXiv:2007.06225 [cs]},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/6EXJ5MVF/2021-05-04 - Elnaggar et al. - ProtTrans Towards Cracking the Language of Life's Code Through Self-Supervised Deep Learning and Hi.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/QDVZ9M8L/2007.html:text/html},
}

@article{jumper_alphafold2_2021,
	title = {[{AlphaFold2}] {Highly} accurate protein structure prediction with {AlphaFold}},
	volume = {596},
	issn = {1476-4687},
	doi = {10.1038/s41586-021-03819-2},
	abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1-4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence-the structure prediction component of the 'protein folding problem'8-has been an important open research problem for more than 50 years9. Despite recent progress10-14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
	language = {eng},
	number = {7873},
	journal = {Nature},
	author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
	month = jul,
	year = {2021},
	pmid = {34265844},
	pmcid = {PMC8371605},
	note = {34742 citations (Crossref/DOI) [2025-10-22]},
	pages = {583--589},
	file = {Jumper et al. - 2021 - Highly accurate protein structure prediction with .pdf:/Users/meehl.joshua/Zotero/storage/SWMRH2BJ/2021-07-15 - Jumper et al. - [AlphaFold2] Highly accurate protein structure prediction with AlphaFold.pdf:application/pdf},
}

@misc{lin_esm-2_2022,
	title = {[{ESM}-2] {Language} models of protein sequences at the scale of evolution enable accurate structure prediction},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1},
	doi = {10.1101/2022.07.20.500902},
	abstract = {Large language models have recently been shown to develop emergent capabilities with scale, going beyond simple pattern matching to perform higher level reasoning and generate lifelike images and text. While language models trained on protein sequences have been studied at a smaller scale, little is known about what they learn about biology as they are scaled up. In this work we train models up to 15 billion parameters, the largest language models of proteins to be evaluated to date. We find that as models are scaled they learn information enabling the prediction of the three-dimensional structure of a protein at the resolution of individual atoms. We present ESMFold for high accuracy end-to-end atomic level structure prediction directly from the individual sequence of a protein. ESMFold has similar accuracy to AlphaFold2 and RoseTTAFold for sequences with low perplexity that are well understood by the language model. ESMFold inference is an order of magnitude faster than AlphaFold2, enabling exploration of the structural space of metagenomic proteins in practical timescales.},
	language = {en},
	urldate = {2022-12-20},
	publisher = {bioRxiv},
	author = {Lin, Zeming and Akin, Halil and Rao, Roshan and Hie, Brian and Zhu, Zhongkai and Lu, Wenting and Costa, Allan dos Santos and Fazel-Zarandi, Maryam and Sercu, Tom and Candido, Sal and Rives, Alexander},
	month = jul,
	year = {2022},
	note = {273 citations (Crossref/DOI) [2025-10-22]
Pages: 2022.07.20.500902
Section: New Results},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/B28E6SXM/2022-07-21 - Lin et al. - [ESM-2] Language models of protein sequences at the scale of evolution enable accurate structure pre.pdf:application/pdf},
}

@misc{meier_esm-1v_2021,
	title = {[{ESM}-1v] {Language} models enable zero-shot prediction of the effects of mutations on protein function},
	copyright = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2021.07.09.450648v1},
	doi = {10.1101/2021.07.09.450648},
	abstract = {Modeling the effect of sequence variation on function is a fundamental problem for understanding and designing proteins. Since evolution encodes information about function into patterns in protein sequences, unsupervised models of variant effects can be learned from sequence data. The approach to date has been to fit a model to a family of related sequences. The conventional setting is limited, since a new model must be trained for each prediction task. We show that using only zero-shot inference, without any supervision from experimental data or additional training, protein language models capture the functional effects of sequence variation, performing at state-of-the-art.},
	language = {en},
	urldate = {2022-11-03},
	publisher = {bioRxiv},
	author = {Meier, Joshua and Rao, Roshan and Verkuil, Robert and Liu, Jason and Sercu, Tom and Rives, Alexander},
	month = jul,
	year = {2021},
	note = {316 citations (Crossref/DOI) [2025-10-22]
Pages: 2021.07.09.450648
Section: New Results},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/7I5WYWWC/2021-07-10 - Meier et al. - [ESM-1v] Language models enable zero-shot prediction of the effects of mutations on protein function.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/KGEPNITZ/2021.07.09.450648v1.html:text/html},
}

@article{morcos_dca_2011,
	title = {Direct-coupling analysis of residue coevolution captures native contacts across many protein families},
	volume = {108},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.1111471108},
	doi = {10.1073/pnas.1111471108},
	abstract = {The similarity in the three-dimensional structures of homologous proteins imposes strong constraints on their sequence variability. It has long been suggested that the resulting correlations among amino acid compositions at different sequence positions can be exploited to infer spatial contacts within the tertiary protein structure. Crucial to this inference is the ability to disentangle direct and indirect correlations, as accomplished by the recently introduced direct-coupling analysis (DCA). Here we develop a computationally efficient implementation of DCA, which allows us to evaluate the accuracy of contact prediction by DCA for a large number of protein domains, based purely on sequence information. DCA is shown to yield a large number of correctly predicted contacts, recapitulating the global structure of the contact map for the majority of the protein domains examined. Furthermore, our analysis captures clear signals beyond intradomain residue contacts, arising, e.g., from alternative protein conformations, ligand-mediated residue couplings, and interdomain interactions in protein oligomers. Our findings suggest that contacts predicted by DCA can be used as a reliable guide to facilitate computational predictions of alternative protein conformations, protein complex formation, and even the de novo prediction of protein domain structures, contingent on the existence of a large number of homologous sequences which are being rapidly made available due to advances in genome sequencing.},
	number = {49},
	urldate = {2025-12-23},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Morcos, Faruck and Pagnani, Andrea and Lunt, Bryan and Bertolino, Arianna and Marks, Debora S. and Sander, Chris and Zecchina, Riccardo and Onuchic, José N. and Hwa, Terence and Weigt, Martin},
	month = dec,
	year = {2011},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {E1293--E1301},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/3ISISRMA/2011-12-06 - Morcos et al. - Direct-coupling analysis of residue coevolution captures native contacts across many protein familie.pdf:application/pdf},
}

@misc{rao_transformer_2020,
	title = {Transformer protein language models are unsupervised structure learners},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2020.12.15.422761v1},
	doi = {10.1101/2020.12.15.422761},
	abstract = {Unsupervised contact prediction is central to uncovering physical, structural, and functional constraints for protein structure determination and design. For decades, the predominant approach has been to infer evolutionary constraints from a set of related sequences. In the past year, protein language models have emerged as a potential alternative, but performance has fallen short of state-of-the-art approaches in bioinformatics. In this paper we demonstrate that Transformer attention maps learn contacts from the unsupervised language modeling objective. We find the highest capacity models that have been trained to date already outperform a state-of-the-art unsupervised contact prediction pipeline, suggesting these pipelines can be replaced with a single forward pass of an end-to-end model.1},
	language = {en},
	urldate = {2025-07-25},
	publisher = {bioRxiv},
	author = {Rao, Roshan and Meier, Joshua and Sercu, Tom and Ovchinnikov, Sergey and Rives, Alexander},
	month = dec,
	year = {2020},
	note = {205 citations (Crossref/DOI) [2025-10-22]
Pages: 2020.12.15.422761
Section: New Results},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/45ZG7U9H/2020-12-15 - Rao et al. - Transformer protein language models are unsupervised structure learners.pdf:application/pdf},
}

@article{suzek_uniref_2007,
	title = {{UniRef}: comprehensive and non-redundant {UniProt} reference clusters},
	volume = {23},
	issn = {1367-4803},
	shorttitle = {{UniRef}},
	url = {https://doi.org/10.1093/bioinformatics/btm098},
	doi = {10.1093/bioinformatics/btm098},
	abstract = {Motivation: Redundant protein sequences in biological databases hinder sequence similarity searches and make interpretation of search results difficult. Clustering of protein sequence space based on sequence similarity helps organize all sequences into manageable datasets and reduces sampling bias and overrepresentation of sequences.Results: The UniRef (UniProt Reference Clusters) provide clustered sets of sequences from the UniProt Knowledgebase (UniProtKB) and selected UniProt Archive records to obtain complete coverage of sequence space at several resolutions while hiding redundant sequences. Currently covering \&gt;4 million source sequences, the UniRef100 database combines identical sequences and subfragments from any source organism into a single UniRef entry. UniRef90 and UniRef50 are built by clustering UniRef100 sequences at the 90 or 50\% sequence identity levels. UniRef100, UniRef90 and UniRef50 yield a database size reduction of ∼10, 40 and 70\%, respectively, from the source sequence set. The reduced redundancy increases the speed of similarity searches and improves detection of distant relationships. UniRef entries contain summary cluster and membership information, including the sequence of a representative protein, member count and common taxonomy of the cluster, the accession numbers of all the merged entries and links to rich functional annotation in UniProtKB to facilitate biological discovery. UniRef has already been applied to broad research areas ranging from genome annotation to proteomics data analysis.Availability: UniRef is updated biweekly and is available for online search and retrieval at http://www.uniprot.org, as well as for download at ftp://ftp.uniprot.org/pub/databases/uniprot/unirefContact:  bes23@georgetown.eduSupplementary information: Supplementary data are available at Bioinformatics online.},
	number = {10},
	urldate = {2025-12-16},
	journal = {Bioinformatics},
	author = {Suzek, Baris E. and Huang, Hongzhan and McGarvey, Peter and Mazumder, Raja and Wu, Cathy H.},
	month = may,
	year = {2007},
	pages = {1282--1288},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/SBM7NX8T/2007-05-15 - Suzek et al. - UniRef comprehensive and non-redundant UniProt reference clusters.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/3VW858PD/btm098.html:text/html},
}

@misc{yang_xlnet_2020,
	title = {{XLNet}: {Generalized} {Autoregressive} {Pretraining} for {Language} {Understanding}},
	shorttitle = {{XLNet}},
	url = {http://arxiv.org/abs/1906.08237},
	doi = {10.48550/arXiv.1906.08237},
	abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.},
	urldate = {2025-12-16},
	publisher = {arXiv},
	author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
	month = jan,
	year = {2020},
	note = {arXiv:1906.08237 [cs]},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/FT6NWATL/2020-01-02 - Yang et al. - XLNet Generalized Autoregressive Pretraining for Language Understanding.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/RBVUTNMX/1906.html:text/html},
}
