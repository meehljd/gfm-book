@article{avsec_enformer_2021,
	title = {[{Enformer}] {Effective} gene expression prediction from sequence by integrating long-range interactions},
	volume = {18},
	url = {https://consensus.app/papers/effective-gene-expression-prediction-from-sequence-by-avsec-agarwal/6afb944129f35bad916e6f4a889c07cb/},
	doi = {10.1038/s41592-021-01252-x},
	abstract = {The next phase of genome biology research requires understanding how DNA sequence encodes phenotypes, from the molecular to organismal levels. How noncoding DNA determines gene expression in different cell types is a major unsolved problem, and critical downstream applications in human genetics depend on improved solutions. Here, we report substantially improved gene expression prediction accuracy from DNA sequence through the use of a new deep learning architecture called Enformer that is able to integrate long-range interactions (up to 100 kb away) in the genome. This improvement yielded more accurate variant effect predictions on gene expression for both natural genetic variants and saturation mutagenesis measured by massively parallel reporter assays. Notably, Enformer outperformed the best team on the critical assessment of genome interpretation (CAGI5) challenge for noncoding variant interpretation with no additional training. Furthermore, Enformer learned to predict promoter-enhancer interactions directly from DNA sequence competitively with methods that take direct experimental data as input. We expect that these advances will enable more effective fine-mapping of growing human disease associations to cell-type-specific gene regulatory mechanisms and provide a framework to interpret cis-regulatory evolution. To foster these downstream applications, we have made the pre-trained Enformer model openly available, and provide pre-computed effect predictions for all common variants in the 1000 Genomes dataset. One-sentence summary Improved noncoding variant effect prediction and candidate enhancer prioritization from a more accurate sequence to expression model driven by extended long-range interaction modelling.},
	urldate = {2024-12-25},
	journal = {Nature Methods},
	author = {Avsec, Žiga and Agarwal, Vikram and Visentin, D. and Ledsam, J. and Grabska-Barwinska, A. and Taylor, Kyle R. and Assael, Yannis and Jumper, J. and Kohli, Pushmeet and Kelley, David R.},
	month = oct,
	year = {2021},
	note = {846 citations (Semantic Scholar/DOI) [2025-10-22]
875 citations (Crossref/DOI) [2025-10-22]},
	keywords = {Podcast, Printed, Read},
	pages = {1196--1203},
	file = {41592_2021_1252_MOESM1_ESM:/Users/meehl.joshua/Zotero/storage/7PUC6VKT/41592_2021_1252_MOESM1_ESM.pdf:application/pdf;Full Text:/Users/meehl.joshua/Zotero/storage/WTX3AUV5/2021-10-04 - Avsec et al. - [Enformer] Effective gene expression prediction from sequence by integrating long-range interactions.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/JWYN5S8L/6afb944129f35bad916e6f4a889c07cb.html:text/html},
}

@article{chen_deepsea_2022,
	title = {[{DeepSEA} {Sei}] {A} sequence-based global map of regulatory activity for deciphering human genetics},
	volume = {54},
	copyright = {2022 The Author(s)},
	issn = {1546-1718},
	url = {https://www.nature.com/articles/s41588-022-01102-2},
	doi = {10.1038/s41588-022-01102-2},
	abstract = {Epigenomic profiling has enabled large-scale identification of regulatory elements, yet we still lack a systematic mapping from any sequence or variant to regulatory activities. We address this challenge with Sei, a framework for integrating human genetics data with sequence information to discover the regulatory basis of traits and diseases. Sei learns a vocabulary of regulatory activities, called sequence classes, using a deep learning model that predicts 21,907 chromatin profiles across {\textgreater}1,300 cell lines and tissues. Sequence classes provide a global classification and quantification of sequence and variant effects based on diverse regulatory activities, such as cell type-specific enhancer functions. These predictions are supported by tissue-specific expression, expression quantitative trait loci and evolutionary constraint data. Furthermore, sequence classes enable characterization of the tissue-specific, regulatory architecture of complex traits and generate mechanistic hypotheses for individual regulatory pathogenic mutations. We provide Sei as a resource to elucidate the regulatory basis of human health and disease.},
	language = {en},
	number = {7},
	urldate = {2025-07-23},
	journal = {Nature Genetics},
	author = {Chen, Kathleen M. and Wong, Aaron K. and Troyanskaya, Olga G. and Zhou, Jian},
	month = jul,
	year = {2022},
	note = {194 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Printed, Read},
	pages = {940--949},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/Y5MUGXGK/2022-07-11 - Chen et al. - [DeepSEA Sei] A sequence-based global map of regulatory activity for deciphering human genetics.pdf:application/pdf},
}

@article{cheng_alphamissense_2023,
	title = {[{AlphaMissense}] {Accurate} proteome-wide missense variant effect prediction with {AlphaMissense}},
	volume = {381},
	url = {https://www.science.org/doi/full/10.1126/science.adg7492},
	doi = {10.1126/science.adg7492},
	abstract = {The vast majority of missense variants observed in the human genome are of unknown clinical significance. We present AlphaMissense, an adaptation of AlphaFold fine-tuned on human and primate variant population frequency databases to predict missense variant pathogenicity. By combining structural context and evolutionary conservation, our model achieves state-of-the-art results across a wide range of genetic and experimental benchmarks, all without explicitly training on such data. The average pathogenicity score of genes is also predictive for their cell essentiality, capable of identifying short essential genes that existing statistical approaches are underpowered to detect. As a resource to the community, we provide a database of predictions for all possible human single amino acid substitutions and classify 89\% of missense variants as either likely benign or likely pathogenic.},
	number = {6664},
	urldate = {2023-09-21},
	journal = {Science},
	author = {Cheng, Jun and Novati, Guido and Pan, Joshua and Bycroft, Clare and Žemgulytė, Akvilė and Applebaum, Taylor and Pritzel, Alexander and Wong, Lai Hong and Zielinski, Michal and Sargeant, Tobias and Schneider, Rosalia G. and Senior, Andrew W. and Jumper, John and Hassabis, Demis and Kohli, Pushmeet and Avsec, Žiga},
	month = sep,
	year = {2023},
	note = {1325 citations (Crossref/DOI) [2025-10-22]
Publisher: American Association for the Advancement of Science},
	keywords = {Printed, Read},
	pages = {eadg7492},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/MZM35E4X/2023-09-19 - Cheng et al. - [AlphaMissense] Accurate proteome-wide missense variant effect prediction with AlphaMissense.pdf:application/pdf;science.adg7492_sm:/Users/meehl.joshua/Zotero/storage/TXHUTZAK/science.adg7492_sm.pdf:application/pdf},
}

@article{dalla-torre_nucleotide_2023,
	title = {Nucleotide {Transformer}: building and evaluating robust foundation models for human genomics},
	volume = {22},
	copyright = {2024 The Author(s)},
	issn = {1548-7105},
	shorttitle = {Nucleotide {Transformer}},
	url = {https://www.nature.com/articles/s41592-024-02523-z},
	doi = {10.1038/s41592-024-02523-z},
	abstract = {The prediction of molecular phenotypes from DNA sequences remains a longstanding challenge in genomics, often driven by limited annotated data and the inability to transfer learnings between tasks. Here, we present an extensive study of foundation models pre-trained on DNA sequences, named Nucleotide Transformer, ranging from 50 million up to 2.5 billion parameters and integrating information from 3,202 human genomes and 850 genomes from diverse species. These transformer models yield context-specific representations of nucleotide sequences, which allow for accurate predictions even in low-data settings. We show that the developed models can be fine-tuned at low cost to solve a variety of genomics applications. Despite no supervision, the models learned to focus attention on key genomic elements and can be used to improve the prioritization of genetic variants. The training and application of foundational models in genomics provides a widely applicable approach for accurate molecular phenotype prediction from DNA sequence.},
	language = {en},
	number = {2},
	urldate = {2025-04-16},
	journal = {Nature Methods},
	author = {Dalla-Torre, Hugo and Gonzalez, Liam and Mendoza-Revilla, Javier and Lopez Carranza, Nicolas and Grzywaczewski, Adam Henryk and Oteri, Francesco and Dallago, Christian and Trop, Evan and de Almeida, Bernardo P. and Sirelkhatim, Hassan and Richard, Guillaume and Skwark, Marcin and Beguir, Karim and Lopez, Marie and Pierrot, Thomas},
	month = jan,
	year = {2023},
	note = {123 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Important, Podcast, Printed, Read},
	pages = {287--297},
	file = {[Preprint] Dalla-Torre et al. - 2023 - Nucleotide Transformer building and evaluating robust foundation models for human genomics:/Users/meehl.joshua/Zotero/storage/METWGZEF/2023-01-15 - Dalla-Torre et al. - Nucleotide Transformer building and evaluating robust foundation models for human genomics.pdf:application/pdf;Full Text PDF:/Users/meehl.joshua/Zotero/storage/K28P6JFS/2023-01-15 - Dalla-Torre et al. - Nucleotide Transformer building and evaluating robust foundation models for human genomics.pdf:application/pdf},
}

@misc{hoffmann_chinchilla_2022,
	title = {Training {Compute}-{Optimal} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2203.15556},
	doi = {10.48550/arXiv.2203.15556},
	abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\${\textbackslash}times\$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
	urldate = {2025-12-23},
	publisher = {arXiv},
	author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and Driessche, George van den and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
	month = mar,
	year = {2022},
	note = {arXiv:2203.15556 [cs]},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/AKYHIZUP/2022-03-29 - Hoffmann et al. - Training Compute-Optimal Large Language Models.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/5MYSHZ4Y/2203.html:text/html},
}

@misc{hu_lora_2021,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{LoRA}},
	url = {http://arxiv.org/abs/2106.09685},
	doi = {10.48550/arXiv.2106.09685},
	abstract = {The dominant paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, conventional fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example, deploying many independent instances of fine-tuned models, each with 175B parameters, is extremely expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. For GPT-3, LoRA can reduce the number of trainable parameters by 10,000 times and the computation hardware requirement by 3 times compared to full fine-tuning. LoRA performs on-par or better than fine-tuning in model quality on both GPT-3 and GPT-2, despite having fewer trainable parameters, a higher training throughput, and no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptations, which sheds light on the efficacy of LoRA. We release our implementation in GPT-2 at https://github.com/microsoft/LoRA .},
	urldate = {2025-12-23},
	publisher = {arXiv},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Chen, Weizhu},
	month = jun,
	year = {2021},
	note = {arXiv:2106.09685 [cs]
version: 1},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/IPQVFEH4/2021-06-17 - Hu et al. - LoRA Low-Rank Adaptation of Large Language Models.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/XIGFMVKZ/2106.html:text/html},
}

@article{jaganathan_spliceai_2019,
	title = {[{SpliceAI}] {Predicting} {Splicing} from {Primary} {Sequence} with {Deep} {Learning}},
	volume = {176},
	issn = {0092-8674},
	url = {https://www.sciencedirect.com/science/article/pii/S0092867418316295},
	doi = {10.1016/j.cell.2018.12.015},
	abstract = {The splicing of pre-mRNAs into mature transcripts is remarkable for its precision, but the mechanisms by which the cellular machinery achieves such specificity are incompletely understood. Here, we describe a deep neural network that accurately predicts splice junctions from an arbitrary pre-mRNA transcript sequence, enabling precise prediction of noncoding genetic variants that cause cryptic splicing. Synonymous and intronic mutations with predicted splice-altering consequence validate at a high rate on RNA-seq and are strongly deleterious in the human population. De novo mutations with predicted splice-altering consequence are significantly enriched in patients with autism and intellectual disability compared to healthy controls and validate against RNA-seq in 21 out of 28 of these patients. We estimate that 9\%–11\% of pathogenic mutations in patients with rare genetic disorders are caused by this previously underappreciated class of disease variation.},
	number = {3},
	urldate = {2025-07-11},
	journal = {Cell},
	author = {Jaganathan, Kishore and Kyriazopoulou Panagiotopoulou, Sofia and McRae, Jeremy F. and Darbandi, Siavash Fazel and Knowles, David and Li, Yang I. and Kosmicki, Jack A. and Arbelaez, Juan and Cui, Wenwu and Schwartz, Grace B. and Chow, Eric D. and Kanterakis, Efstathios and Gao, Hong and Kia, Amirali and Batzoglou, Serafim and Sanders, Stephan J. and Farh, Kyle Kai-How},
	month = jan,
	year = {2019},
	note = {2156 citations (Crossref/DOI) [2025-10-22]},
	keywords = {Important, Podcast, Printed, Read},
	pages = {535--548.e24},
	file = {Full Text:/Users/meehl.joshua/Zotero/storage/RNFB2GEK/2019-01-24 - Jaganathan et al. - [SpliceAI] Predicting Splicing from Primary Sequence with Deep Learning.pdf:application/pdf;ScienceDirect Snapshot:/Users/meehl.joshua/Zotero/storage/7K7VMGER/S0092867418316295.html:text/html},
}

@article{ji_dnabert_2021,
	title = {{DNABERT}: pre-trained {Bidirectional} {Encoder} {Representations} from {Transformers} model for {DNA}-language in genome},
	volume = {37},
	issn = {1367-4803},
	shorttitle = {{DNABERT}},
	url = {https://doi.org/10.1093/bioinformatics/btab083},
	doi = {10.1093/bioinformatics/btab083},
	abstract = {Deciphering the language of non-coding DNA is one of the fundamental problems in genome research. Gene regulatory code is highly complex due to the existence of polysemy and distant semantic relationship, which previous informatics methods often fail to capture especially in data-scarce scenarios.To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, to capture global and transferrable understanding of genomic DNA sequences based on up and downstream nucleotide contexts. We compared DNABERT to the most widely used programs for genome-wide regulatory elements prediction and demonstrate its ease of use, accuracy and efficiency. We show that the single pre-trained transformers model can simultaneously achieve state-of-the-art performance on prediction of promoters, splice sites and transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, DNABERT enables direct visualization of nucleotide-level importance and semantic relationship within input sequences for better interpretability and accurate identification of conserved sequence motifs and functional genetic variant candidates. Finally, we demonstrate that pre-trained DNABERT with human genome can even be readily applied to other organisms with exceptional performance. We anticipate that the pre-trained DNABERT model can be fined tuned to many other sequence analyses tasks.The source code, pretrained and finetuned model for DNABERT are available at GitHub (https://github.com/jerryji1993/DNABERT).Supplementary data are available at Bioinformatics online.},
	number = {15},
	urldate = {2025-08-31},
	journal = {Bioinformatics},
	author = {Ji, Yanrong and Zhou, Zhihan and Liu, Han and Davuluri, Ramana V},
	month = aug,
	year = {2021},
	note = {802 citations (Crossref/DOI) [2025-10-22]},
	pages = {2112--2120},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/VKSTMFPQ/2021-08-09 - Ji et al. - DNABERT pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/UBBEDFL5/btab083.html:text/html},
}

@misc{li_omnidna_2025,
	title = {Omni-{DNA}: {A} {Unified} {Genomic} {Foundation} {Model} for {Cross}-{Modal} and {Multi}-{Task} {Learning}},
	shorttitle = {Omni-{DNA}},
	url = {http://arxiv.org/abs/2502.03499},
	doi = {10.48550/arXiv.2502.03499},
	abstract = {Large Language Models (LLMs) demonstrate remarkable generalizability across diverse tasks, yet genomic foundation models (GFMs) still require separate finetuning for each downstream application, creating significant overhead as model sizes grow. Moreover, existing GFMs are constrained by rigid output formats, limiting their applicability to various genomic tasks. In this work, we revisit the transformer-based auto-regressive models and introduce Omni-DNA, a family of cross-modal multi-task models ranging from 20 million to 1 billion parameters. Our approach consists of two stages: (i) pretraining on DNA sequences with next token prediction objective, and (ii) expanding the multi-modal task-specific tokens and finetuning for multiple downstream tasks simultaneously. When evaluated on the Nucleotide Transformer and GB benchmarks, Omni-DNA achieves state-of-the-art performance on 18 out of 26 tasks. Through multi-task finetuning, Omni-DNA addresses 10 acetylation and methylation tasks at once, surpassing models trained on each task individually. Finally, we design two complex genomic tasks, DNA2Function and Needle-in-DNA, which map DNA sequences to textual functional descriptions and images, respectively, indicating Omni-DNA's cross-modal capabilities to broaden the scope of genomic applications. All the models are available through https://huggingface.co/collections/zehui127},
	urldate = {2025-12-23},
	publisher = {arXiv},
	author = {Li, Zehui and Subasri, Vallijah and Shen, Yifei and Li, Dongsheng and Zhao, Yiren and Stan, Guy-Bart and Shan, Caihua},
	month = feb,
	year = {2025},
	note = {arXiv:2502.03499 [q-bio]},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/HAUVWTCQ/2025-02-05 - Li et al. - Omni-DNA A Unified Genomic Foundation Model for Cross-Modal and Multi-Task Learning.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/6EMDZKPT/2502.html:text/html},
}

@misc{lin_esm-2_2022,
	title = {[{ESM}-2] {Language} models of protein sequences at the scale of evolution enable accurate structure prediction},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1},
	doi = {10.1101/2022.07.20.500902},
	abstract = {Large language models have recently been shown to develop emergent capabilities with scale, going beyond simple pattern matching to perform higher level reasoning and generate lifelike images and text. While language models trained on protein sequences have been studied at a smaller scale, little is known about what they learn about biology as they are scaled up. In this work we train models up to 15 billion parameters, the largest language models of proteins to be evaluated to date. We find that as models are scaled they learn information enabling the prediction of the three-dimensional structure of a protein at the resolution of individual atoms. We present ESMFold for high accuracy end-to-end atomic level structure prediction directly from the individual sequence of a protein. ESMFold has similar accuracy to AlphaFold2 and RoseTTAFold for sequences with low perplexity that are well understood by the language model. ESMFold inference is an order of magnitude faster than AlphaFold2, enabling exploration of the structural space of metagenomic proteins in practical timescales.},
	language = {en},
	urldate = {2022-12-20},
	publisher = {bioRxiv},
	author = {Lin, Zeming and Akin, Halil and Rao, Roshan and Hie, Brian and Zhu, Zhongkai and Lu, Wenting and Costa, Allan dos Santos and Fazel-Zarandi, Maryam and Sercu, Tom and Candido, Sal and Rives, Alexander},
	month = jul,
	year = {2022},
	note = {273 citations (Crossref/DOI) [2025-10-22]
Pages: 2022.07.20.500902
Section: New Results},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/B28E6SXM/2022-07-21 - Lin et al. - [ESM-2] Language models of protein sequences at the scale of evolution enable accurate structure pre.pdf:application/pdf},
}

@misc{medvedev_biotoken_2025,
	title = {{BioToken} and {BioFM} – {Biologically}-{Informed} {Tokenization} {Enables} {Accurate} and {Efficient} {Genomic} {Foundation} {Models}},
	copyright = {© 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2025.03.27.645711v1},
	doi = {10.1101/2025.03.27.645711},
	abstract = {Genomic variation underlies human phenotypic diversity, disease susceptibility, and evolutionary adaptation. Although large-scale genomic sequencing has transformed our ability to map genetic variation, accurately modeling and interpreting this data remains challenging due to fundamental limitations in existing genomic foundation models (GFMs). Current genomic models typically treat DNA simplistically as linear textual sequences, overlooking critical biological context, such as genomic structural annotations, regulatory elements, and functional contexts central to genomic interpretation. As a result, these models are prone to positional memorization of common sequences, severely limiting their generalization to biologically meaningful tasks. Here, we introduce BioToken, a modular and extendable tokenization framework designed to encode genomic variants and biologically relevant structural annotations directly into genomic representations. By utilizing intrinsic inductive biases, BioToken facilitates meaningful representation learning and generalization across diverse molecular phenotypes, such as gene expression, alternative splicing, and variant pathogenicity prediction. Built on BioToken, our genomic foundation model, BioFM, achieves competitive or superior results relative to specialized models (e.g., Enformer, SpliceTransformer) across a comprehensive suite of genomic benchmarks, including noncoding pathogenicity, expression modulation, sQTL prediction, and long-range genomic interactions. Notably, BioFM achieves state-of-the-art performance with significantly fewer parameters (265M), substantially reducing training costs and computational requirements. Our findings high-light the substantial advantages of integrating biologically-informed inductive biases into genomic foundation modeling, providing a robust and accessible path forward in genomics. We provide our code and model checkpoints to support further research in this direction.},
	language = {en},
	urldate = {2025-05-27},
	publisher = {bioRxiv},
	author = {Medvedev, Aleksandr and Viswanathan, Karthik and Kanithi, Praveenkumar and Vishniakov, Kirill and Munjal, Prateek and Christophe, Clément and Pimentel, Marco AF and Rajan, Ronnie and Khan, Shadab},
	month = apr,
	year = {2025},
	note = {1 citations (Crossref/DOI) [2025-10-22]
Pages: 2025.03.27.645711
Section: New Results},
	keywords = {Podcast, Printed},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/XZBQLGZE/2025-04-01 - Medvedev et al. - BioToken and BioFM – Biologically-Informed Tokenization Enables Accurate and Efficient Genomic Found.pdf:application/pdf},
}

@misc{nguyen_hyenadna_2023,
	title = {{HyenaDNA}: {Long}-{Range} {Genomic} {Sequence} {Modeling} at {Single} {Nucleotide} {Resolution}},
	shorttitle = {{HyenaDNA}},
	url = {http://arxiv.org/abs/2306.15794},
	doi = {10.48550/arXiv.2306.15794},
	abstract = {Genomic (DNA) sequences encode an enormous amount of information for gene regulation and protein synthesis. Similar to natural language models, researchers have proposed foundation models in genomics to learn generalizable features from unlabeled genome data that can then be fine-tuned for downstream tasks such as identifying regulatory elements. Due to the quadratic scaling of attention, previous Transformer-based genomic models have used 512 to 4k tokens as context ({\textless}0.001\% of the human genome), significantly limiting the modeling of long-range interactions in DNA. In addition, these methods rely on tokenizers or fixed k-mers to aggregate meaningful DNA units, losing single nucleotide resolution where subtle genetic variations can completely alter protein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a large language model based on implicit convolutions was shown to match attention in quality while allowing longer context lengths and lower time complexity. Leveraging Hyena's new long-range capabilities, we present HyenaDNA, a genomic foundation model pretrained on the human reference genome with context lengths of up to 1 million tokens at the single nucleotide-level - an up to 500x increase over previous dense attention-based models. HyenaDNA scales sub-quadratically in sequence length (training up to 160x faster than Transformer), uses single nucleotide tokens, and has full global context at each layer. We explore what longer context enables - including the first use of in-context learning in genomics. On fine-tuned benchmarks from the Nucleotide Transformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 18 datasets using a model with orders of magnitude less parameters and pretraining data. On the GenomicBenchmarks, HyenaDNA surpasses SotA on 7 of 8 datasets on average by +10 accuracy points. Code at https://github.com/HazyResearch/hyena-dna.},
	urldate = {2025-07-23},
	publisher = {arXiv},
	author = {Nguyen, Eric and Poli, Michael and Faizi, Marjan and Thomas, Armin and Birch-Sykes, Callum and Wornow, Michael and Patel, Aman and Rabideau, Clayton and Massaroli, Stefano and Bengio, Yoshua and Ermon, Stefano and Baccus, Stephen A. and Ré, Chris},
	month = nov,
	year = {2023},
	note = {arXiv:2306.15794 [cs]},
	keywords = {Important, Podcast, Printed, Read},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/JVEF4BBI/2023-11-14 - Nguyen et al. - HyenaDNA Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/G59U5Z77/2306.html:text/html},
}

@article{rentzsch_cadd_2019,
	title = {{CADD}: predicting the deleteriousness of variants throughout the human genome},
	volume = {47},
	issn = {0305-1048},
	shorttitle = {{CADD}},
	url = {https://doi.org/10.1093/nar/gky1016},
	doi = {10.1093/nar/gky1016},
	abstract = {Combined Annotation-Dependent Depletion (CADD) is a widely used measure of variant deleteriousness that can effectively prioritize causal variants in genetic analyses, particularly highly penetrant contributors to severe Mendelian disorders. CADD is an integrative annotation built from more than 60 genomic features, and can score human single nucleotide variants and short insertion and deletions anywhere in the reference assembly. CADD uses a machine learning model trained on a binary distinction between simulated de novo variants and variants that have arisen and become fixed in human populations since the split between humans and chimpanzees; the former are free of selective pressure and may thus include both neutral and deleterious alleles, while the latter are overwhelmingly neutral (or, at most, weakly deleterious) by virtue of having survived millions of years of purifying selection. Here we review the latest updates to CADD, including the most recent version, 1.4, which supports the human genome build GRCh38. We also present updates to our website that include simplified variant lookup, extended documentation, an Application Program Interface and improved mechanisms for integrating CADD scores into other tools or applications. CADD scores, software and documentation are available at https://cadd.gs.washington.edu.},
	number = {D1},
	urldate = {2025-12-02},
	journal = {Nucleic Acids Research},
	author = {Rentzsch, Philipp and Witten, Daniela and Cooper, Gregory M and Shendure, Jay and Kircher, Martin},
	month = jan,
	year = {2019},
	pages = {D886--D894},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/45CSF3AC/2019-01-08 - Rentzsch et al. - CADD predicting the deleteriousness of variants throughout the human genome.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/KMBY3G6E/gky1016.html:text/html},
}

@article{rives_biological_2021,
	title = {[{ESM}-1b] {Biological} structure and function emerge from scaling unsupervised learning to 250 million protein sequences},
	volume = {118},
	issn = {1091-6490},
	doi = {10.1073/pnas.2016239118},
	abstract = {In the field of artificial intelligence, a combination of scale in data and model capacity enabled by unsupervised learning has led to major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing promises unprecedented data on natural sequence diversity. Protein language modeling at the scale of evolution is a logical step toward predictive and generative artificial intelligence for biology. To this end, we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone. The learned representation space has a multiscale organization reflecting structure from the level of biochemical properties of amino acids to remote homology of proteins. Information about secondary and tertiary structure is encoded in the representations and can be identified by linear projections. Representation learning produces features that generalize across a range of applications, enabling state-of-the-art supervised prediction of mutational effect and secondary structure and improving state-of-the-art features for long-range contact prediction.},
	language = {eng},
	number = {15},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Rives, Alexander and Meier, Joshua and Sercu, Tom and Goyal, Siddharth and Lin, Zeming and Liu, Jason and Guo, Demi and Ott, Myle and Zitnick, C. Lawrence and Ma, Jerry and Fergus, Rob},
	month = apr,
	year = {2021},
	pmid = {33876751},
	pmcid = {PMC8053943},
	note = {2208 citations (Crossref/DOI) [2025-10-22]},
	pages = {e2016239118},
	file = {Rives et al. - 2021 - Biological structure and function emerge from scal-Supplement.pdf:/Users/meehl.joshua/Zotero/storage/V5K3TREP/2021-04-13 - Rives et al. - [ESM-1b] Biological structure and function emerge from scaling unsupervised learning to 250 million.pdf:application/pdf;Rives et al. - 2021 - Biological structure and function emerge from scal.pdf:/Users/meehl.joshua/Zotero/storage/45JE6CKI/2021-04-13 - Rives et al. - [ESM-1b] Biological structure and function emerge from scaling unsupervised learning to 250 million.pdf:application/pdf},
}

@article{sanabria_grover_2024,
	title = {[{GROVER}] {DNA} language model {GROVER} learns sequence context in the human genome},
	volume = {6},
	copyright = {2024 The Author(s)},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-024-00872-0},
	doi = {10.1038/s42256-024-00872-0},
	abstract = {Deep-learning models that learn a sense of language on DNA have achieved a high level of performance on genome biological tasks. Genome sequences follow rules similar to natural language but are distinct in the absence of a concept of words. We established byte-pair encoding on the human genome and trained a foundation language model called GROVER (Genome Rules Obtained Via Extracted Representations) with the vocabulary selected via a custom task, next-k-mer prediction. The defined dictionary of tokens in the human genome carries best the information content for GROVER. Analysing learned representations, we observed that trained token embeddings primarily encode information related to frequency, sequence content and length. Some tokens are primarily localized in repeats, whereas the majority widely distribute over the genome. GROVER also learns context and lexical ambiguity. Average trained embeddings of genomic regions relate to functional genomics annotation and thus indicate learning of these structures purely from the contextual relationships of tokens. This highlights the extent of information content encoded by the sequence that can be grasped by GROVER. On fine-tuning tasks addressing genome biology with questions of genome element identification and protein–DNA binding, GROVER exceeds other models’ performance. GROVER learns sequence context, a sense for structure and language rules. Extracting this knowledge can be used to compose a grammar book for the code of life.},
	language = {en},
	number = {8},
	urldate = {2025-07-25},
	journal = {Nature Machine Intelligence},
	author = {Sanabria, Melissa and Hirsch, Jonas and Joubert, Pierre M. and Poetsch, Anna R.},
	month = jul,
	year = {2024},
	note = {47 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Printed},
	pages = {911--923},
	file = {2024-07-23 - Sanabria et al. - [Preprint][GROVER] DNA language model GROVER learns sequence context in the human genome:/Users/meehl.joshua/Zotero/storage/WLAJANB6/2024-07-23 - Sanabria et al. - [Preprint][GROVER] DNA language model GROVER learns sequence context in the human genome.pdf:application/pdf;Full Text PDF:/Users/meehl.joshua/Zotero/storage/SNYYQVLT/2024-07-23 - Sanabria et al. - [GROVER] DNA language model GROVER learns sequence context in the human genome.pdf:application/pdf},
}

@article{schubach_cadd_2024,
	title = {{CADD} v1.7: using protein language models, regulatory {CNNs} and other nucleotide-level scores to improve genome-wide variant predictions},
	volume = {52},
	issn = {0305-1048},
	shorttitle = {{CADD} v1.7},
	url = {https://doi.org/10.1093/nar/gkad989},
	doi = {10.1093/nar/gkad989},
	abstract = {Machine Learning-based scoring and classification of genetic variants aids the assessment of clinical findings and is employed to prioritize variants in diverse genetic studies and analyses. Combined Annotation-Dependent Depletion (CADD) is one of the first methods for the genome-wide prioritization of variants across different molecular functions and has been continuously developed and improved since its original publication. Here, we present our most recent release, CADD v1.7. We explored and integrated new annotation features, among them state-of-the-art protein language model scores (Meta ESM-1v), regulatory variant effect predictions (from sequence-based convolutional neural networks) and sequence conservation scores (Zoonomia). We evaluated the new version on data sets derived from ClinVar, ExAC/gnomAD and 1000 Genomes variants. For coding effects, we tested CADD on 31 Deep Mutational Scanning (DMS) data sets from ProteinGym and, for regulatory effect prediction, we used saturation mutagenesis reporter assay data of promoter and enhancer sequences. The inclusion of new features further improved the overall performance of CADD. As with previous releases, all data sets, genome-wide CADD v1.7 scores, scripts for on-site scoring and an easy-to-use webserver are readily provided via https://cadd.bihealth.org/ or https://cadd.gs.washington.edu/ to the community.},
	number = {D1},
	urldate = {2025-05-23},
	journal = {Nucleic Acids Research},
	author = {Schubach, Max and Maass, Thorben and Nazaretyan, Lusiné and Röner, Sebastian and Kircher, Martin},
	month = jan,
	year = {2024},
	note = {235 citations (Crossref/DOI) [2025-10-22]},
	keywords = {Podcast, Printed, Read},
	pages = {D1143--D1154},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/55MT7UQH/2024-01-05 - Schubach et al. - CADD v1.7 using protein language models, regulatory CNNs and other nucleotide-level scores to impro.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/X6G5SM2W/7511313.html:text/html},
}

@article{zhou_deepsea_2015,
	title = {[{DeepSEA}] {Predicting} effects of noncoding variants with deep learning–based sequence model},
	volume = {12},
	copyright = {2015 Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.3547},
	doi = {10.1038/nmeth.3547},
	abstract = {DeepSEA, a deep-learning algorithm trained on large-scale chromatin-profiling data, predicts chromatin effects from sequence alone, has single-nucleotide sensitivity and can predict effects of noncoding variants.},
	language = {en},
	number = {10},
	urldate = {2025-08-12},
	journal = {Nature Methods},
	author = {Zhou, Jian and Troyanskaya, Olga G.},
	month = aug,
	year = {2015},
	note = {2031 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	pages = {931--934},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/DS72YP6N/2015-10 - Zhou and Troyanskaya - [DeepSEA] Predicting effects of noncoding variants with deep learning–based sequence model.pdf:application/pdf},
}

@misc{zhou_dnabert-2_2024,
	title = {{DNABERT}-2: {Efficient} {Foundation} {Model} and {Benchmark} {For} {Multi}-{Species} {Genome}},
	shorttitle = {{DNABERT}-2},
	url = {http://arxiv.org/abs/2306.15006},
	doi = {10.48550/arXiv.2306.15006},
	abstract = {Decoding the linguistic intricacies of the genome is a crucial problem in biology, and pre-trained foundational models such as DNABERT and Nucleotide Transformer have made significant strides in this area. Existing works have largely hinged on k-mer, fixed-length permutations of A, T, C, and G, as the token of the genome language due to its simplicity. However, we argue that the computation and sample inefficiencies introduced by k-mer tokenization are primary obstacles in developing large genome foundational models. We provide conceptual and empirical insights into genome tokenization, building on which we propose to replace k-mer tokenization with Byte Pair Encoding (BPE), a statistics-based data compression algorithm that constructs tokens by iteratively merging the most frequent co-occurring genome segment in the corpus. We demonstrate that BPE not only overcomes the limitations of k-mer tokenization but also benefits from the computational efficiency of non-overlapping tokenization. Based on these insights, we introduce DNABERT-2, a refined genome foundation model that adapts an efficient tokenizer and employs multiple strategies to overcome input length constraints, reduce time and memory expenditure, and enhance model capability. Furthermore, we identify the absence of a comprehensive and standardized benchmark for genome understanding as another significant impediment to fair comparative analysis. In response, we propose the Genome Understanding Evaluation (GUE), a comprehensive multi-species genome classification dataset that amalgamates \$36\$ distinct datasets across \$9\$ tasks, with input lengths ranging from \$70\$ to \$10000\$. Through comprehensive experiments on the GUE benchmark, we demonstrate that DNABERT-2 achieves comparable performance to the state-of-the-art model with \$21 {\textbackslash}times\$ fewer parameters and approximately \$92 {\textbackslash}times\$ less GPU time in pre-training.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Zhou, Zhihan and Ji, Yanrong and Li, Weijian and Dutta, Pratik and Davuluri, Ramana and Liu, Han},
	month = mar,
	year = {2024},
	note = {arXiv:2306.15006 [q-bio]},
	keywords = {Important, Podcast, Printed, Read},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/2KZP5KPQ/2024-03-18 - Zhou et al. - DNABERT-2 Efficient Foundation Model and Benchmark For Multi-Species Genome.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/KUYT79IL/2306.html:text/html},
}

@article{zhou_expecto_2018,
	title = {[{Expecto}] {Deep} learning sequence-based ab initio prediction of variant effects on expression and disease risk},
	volume = {50},
	copyright = {2018 The Author(s)},
	issn = {1546-1718},
	url = {https://www.nature.com/articles/s41588-018-0160-6},
	doi = {10.1038/s41588-018-0160-6},
	abstract = {Key challenges for human genetics, precision medicine and evolutionary biology include deciphering the regulatory code of gene expression and understanding the transcriptional effects of genome variation. However, this is extremely difficult because of the enormous scale of the noncoding mutation space. We developed a deep learning–based framework, ExPecto, that can accurately predict, ab initio from a DNA sequence, the tissue-specific transcriptional effects of mutations, including those that are rare or that have not been observed. We prioritized causal variants within disease- or trait-associated loci from all publicly available genome-wide association studies and experimentally validated predictions for four immune-related diseases. By exploiting the scalability of ExPecto, we characterized the regulatory mutation space for human RNA polymerase II–transcribed genes by in silico saturation mutagenesis and profiled {\textgreater} 140 million promoter-proximal mutations. This enables probing of evolutionary constraints on gene expression and ab initio prediction of mutation disease effects, making ExPecto an end-to-end computational framework for the in silico prediction of expression and disease risk.},
	language = {en},
	number = {8},
	urldate = {2025-07-25},
	journal = {Nature Genetics},
	author = {Zhou, Jian and Theesfeld, Chandra L. and Yao, Kevin and Chen, Kathleen M. and Wong, Aaron K. and Troyanskaya, Olga G.},
	month = jul,
	year = {2018},
	note = {420 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Printed, Read, Recall},
	pages = {1171--1179},
	file = {2018-07-16 - Zhou et al. - [DeepSEA Beluga] Deep learning sequence-based ab initio prediction of variant effects on expression:/Users/meehl.joshua/Zotero/storage/GDYBADGZ/2018-07-16 - Zhou et al. - [DeepSEA Beluga] Deep learning sequence-based ab initio prediction of variant effects on expression.pdf:application/pdf},
}
