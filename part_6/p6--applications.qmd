# Part VI: Clinical Translation {.unnumbered}

::: {.callout-note}
## Part VI at a Glance

**Central question:** How do we move from promising benchmark results to tools that support real decisions about patients, drug development, and biological design?

**Prerequisites:** Parts III (foundation models) and V (evaluation and trust). Part I for data context.

| Chapter | Topic | Key Applications |
|---------|-------|-----------------|
| @sec-ch27-clinical-risk | Clinical Risk Prediction | Disease risk stratification, EHR integration, fairness |
| @sec-ch28-rare-disease | Rare Disease & Cancer | Variant prioritization, diagnostic pipelines, tumor analysis |
| @sec-ch29-drug-discovery | Drug Discovery | Target identification, genetic validation, biomarker development |
| @sec-ch30-design | Biological Design | Protein engineering, regulatory design, synthetic biology |
| @sec-ch31-frontiers | Frontiers | Emerging directions, open problems, future outlook |

**After completing Part VI, you will understand:**

- What additional requirements clinical deployment imposes beyond research benchmarks
- How to integrate foundation model predictions into diagnostic workflows
- Where foundation models contribute to the drug discovery pipeline
- How generative foundation models enable biological design
- What major challenges remain for the field
:::

The question shifts from how these models work to how they are used, and from what they can predict to what they enable us to do. This transition is not merely practical but conceptual: deploying a model in a clinical or industrial setting exposes assumptions that benchmarks leave implicit and reveals failure modes that curated evaluations obscure. A model achieving impressive metrics on held-out test sets may falter when deployed on populations underrepresented in training data, when integrated into workflows designed around different assumptions, or when its outputs must be communicated to clinicians and patients who lack the technical background to interpret confidence intervals. The gap between benchmark performance and real-world utility represents one of the most consequential challenges in genomic AI.

Deployment transforms the requirements for genomic foundation models. Clinical risk prediction, rare disease diagnosis, drug discovery, and biological design each impose constraints absent from research settings: calibration requirements become stricter, fairness considerations become urgent, interpretability demands become concrete, and the consequences of failure become measured in patient outcomes rather than leaderboard rankings.

Clinical risk prediction (@sec-ch27-clinical-risk) combines foundation model features with electronic health records to stratify patients for disease, progression, and treatment response. Variant interpretation in rare disease (@sec-ch28-rare-disease) enters diagnostic pipelines alongside clinical geneticists and laboratory scientists. Drug discovery applications (@sec-ch29-drug-discovery) contribute to target identification, genetic validation, and biomarker development. Reversing the direction of inference from prediction to generation (@sec-ch30-design), foundation models guide protein engineering, regulatory element design, and programmable biology. Emerging directions and open problems (@sec-ch31-frontiers) will shape how genomic AI moves from research to practice.

The goal is not definitive protocols for each domain but a framework for reasoning about deployment: what questions to ask, what pitfalls to anticipate, and what principles should guide responsible development.

::: {.callout-warning}
## The Benchmark-Deployment Gap

**Strong benchmark performance does not guarantee clinical utility.** Every chapter in this part addresses specific ways that models successful in research settings can fail in deployment:

- **Calibration failures** where confident predictions are systematically wrong
- **Population shift** where performance degrades for underrepresented groups
- **Integration challenges** where model outputs don't fit clinical workflows
- **Communication gaps** where predictions cannot be explained to stakeholders

Part V provides the tools to anticipate and address these failure modes.
:::

::: {.callout-tip}
## Connections to Other Parts

- **Part I** data foundations determine what populations and variants models can serve
- **Part III** foundation model capabilities determine what predictions are possible
- **Part V** evaluation and trust frameworks are prerequisites for responsible deployment
:::
